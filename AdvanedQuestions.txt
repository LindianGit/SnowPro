 Question 1. A financial services company uses an internal named stage in Snowflake for daily transaction file ingestion. One day, the ingestion process fails, and the operations team must manually download the failed file and check for errors. The architect is tasked with improving this process to reduce manual intervention and speed up recovery. Which of the following Snowflake features would BEST address this requirement?

A) Implementing Snowpipe with automatic error notifications and leveraging the COPY_HISTORY function to identify failed files  
B) Switching to an external stage and using AWS Lambda to reprocess files  
C) Setting up a scheduled task to automatically delete failed files from the stage  
D) Using a Data Exchange listing for file recovery  

**Answer:** A

---

Question 2. During a business-critical data load event, an ingestion job fails and the file remains in the internal named stage. The operations team needs to quickly identify which rows in the file caused the failure in order to correct and reload only the problematic data. Which Snowflake feature can help the team isolate error details at a row level?

A) Re-running the entire COPY INTO command with ON_ERROR='CONTINUE'  
B) Reviewing the QUERY_HISTORY view for error codes  
C) Using the VALIDATION_MODE option with the COPY INTO command to return error rows  
D) Manually parsing the file outside of Snowflake using third-party tools  

**Answer:** C

---

Question 3. A retail company’s ingestion process uses an internal named stage. After a failure, the operations team downloads the file and checks for errors, which is time-consuming and error-prone. As the architect, you are asked to automate error handling and streamline recovery. Which architectural change would most effectively reduce manual effort while ensuring failed files are traceable for audit purposes?

A) Configure Snowflake tasks to automatically retry failed ingestions and move failed files to a designated error stage  
B) Set up a process to permanently delete all failed files after each attempt  
C) Require the operations team to email all failed files to the data engineering team  
D) Only allow ingestion during business hours to ensure staff are available for manual checks  

**Answer:** A

---

Question 4. A large retail company is using a Snowflake Business Critical edition account and wants to share sales data with a partner company that only has a Snowflake Enterprise edition account. Which of the following statements is correct regarding this scenario?

A) Data sharing is not possible between accounts of different editions.  
B) Data sharing is only possible if both accounts are on the same edition and region.  
C) Data sharing is possible from a Business Critical edition provider to an Enterprise edition consumer.  
D) Data sharing is restricted to Virtual Private Snowflake (VPS) accounts only.  

**Correct Answer:** C

---

Question 5. An architect at a financial institution with a Snowflake Business Critical edition account is asked about sharing sensitive data with a subsidiary operating on an Enterprise edition account. Which approach is feasible?

A) Data sharing cannot occur unless the subsidiary upgrades to Business Critical.  
B) The Business Critical account is permitted to share data with the Enterprise account, subject to Snowflake’s security and compliance controls.  
C) Only Secure Data Sharing is allowed between VPS and Enterprise accounts.  
D) Data sharing requires both accounts to be in the same organization.  

**Correct Answer:** B

---

Question 6. A multinational company intends to share its analytics data with multiple partners, some of which use Business Critical edition and others use Enterprise edition Snowflake accounts. What is the key consideration when architecting data sharing for this scenario?

A) Data sharing is only possible between accounts with the exact same edition.  
B) The data provider’s edition must be lower or equal to the data consumer’s edition.  
C) Data sharing is possible from higher to lower editions, such as Business Critical to Enterprise, with appropriate security and governance practices.  
D) Data sharing requires manual export and import of data between editions.  

**Correct Answer:** C

---

Question 7. The Business Intelligence team notices significant slowdowns in dashboard queries when multiple team members are running queries at the same time. As a Snowflake Architect, which of the following actions would best help you identify and troubleshoot the root cause?

A) Increase the warehouse size without analyzing query patterns.  
B) Review the Query History and Warehouse Load in the Snowflake UI to identify concurrency or resource bottlenecks.  
C) Immediately restrict user access to the dashboards.  
D) Ask users to schedule their queries at different times without further investigation.  

**Correct Answer:** B

---

Question 8. During periods of high activity, BI dashboard queries are experiencing slow response times. What is the most effective first step for a Snowflake Architect to take in order to diagnose the performance bottleneck?

A) Use Snowflake’s Query Profile to analyze individual queries and look for patterns such as queuing or resource contention.  
B) Archive all historical data in the Snowflake account.  
C) Contact Snowflake support without collecting any internal metrics.  
D) Migrate the workload to another cloud provider immediately.  

**Correct Answer:** A

---

Question 9. Multiple users experience delays when running queries for dashboards in parallel. Which approach should the Snowflake Architect recommend to ensure minimal impact on business operations while identifying the issue?

A) Monitor the performance of the virtual warehouse using Resource Monitors and consider enabling Multi-cluster Warehouses to handle concurrency.  
B) Disable dashboard access for all users until the issue is resolved.  
C) Reduce the size of the virtual warehouse to save costs.  
D) Drop and recreate all affected tables.  

**Correct Answer:** A

---

Question 10. The BI team at your company reports query slowdowns when many users run dashboards simultaneously. After investigating, you find that the virtual warehouse is frequently queuing queries during peak periods. As a Snowflake Architect, what is an appropriate solution to address this issue?

A) Reduce the size of the virtual warehouse to limit resource consumption.  
B) Enable auto-suspend on the warehouse to save costs during idle periods.  
C) Increase the number of clusters in a multi-cluster warehouse to handle more concurrent queries without queuing.  
D) Split the data into more databases to distribute the load.  

**Correct Answer:** C

---

Question 11. An Architect is analyzing a slow-running query using the `QUERY_HISTORY` function in Snowflake. They observe that the `COMPILATION_TIME` for the query is significantly greater than the `EXECUTION_TIME`. Which of the following best explains this observation?

A) The query is waiting for resources due to warehouse queuing.  
B) The query execution is delayed due to network latency between client and Snowflake.  
C) The query is complex, possibly involving dynamic SQL, extensive parsing, or large numbers of objects, which increases the time required for query compilation compared to execution.  
D) The data required for the query is stored in a remote region, causing high data transfer times.  

**Correct Answer:** C

**Explanation:**  
When `COMPILATION_TIME` is greater than `EXECUTION_TIME`, it typically means the query is complex to parse, plan, or optimize, often involving dynamic SQL, lots of objects/tables, or complicated logic. The actual execution (reading and processing data) is relatively fast, but most time is spent preparing the query.

---

Question 12. A Snowflake Architect is investigating a query that has a much higher COMPILATION_TIME than EXECUTION_TIME in the QUERY_HISTORY results. Which scenario is most likely causing this observation?

A) The warehouse was suspended and had to be resumed before the query could execute.  
B) The query references many objects, such as multiple tables or complex views, causing Snowflake to spend more time parsing and optimizing before execution.  
C) The underlying data files are heavily compressed, slowing down data retrieval.  
D) The network connection between the client and Snowflake was interrupted during execution.  

**Correct Answer:** B

---

Question 13. If you are using the `--mfa-passcode-in-password` flag with SnowSQL and the password prompt is forced (such as with `-P`), the password you enter should be a **concatenation of your Snowflake password and your current MFA token**.

Given your example:
- **Snowflake password:** SNOWFLAKE
- **MFA token:** 123456

**The password you should enter at the prompt will be:**
```
SNOWFLAKE123456
```

**Explanation:**  
You simply append the current 6-digit MFA code to the end of your normal password, with no spaces or separators. So in this case, enter `SNOWFLAKE123456` as the password when prompted.

---

Question 14. A Snowflake user’s password is `Winter2025`, and their current MFA token from their authenticator app is `654321`. When using SnowSQL with the `--mfa-passcode-in-password` flag and prompted for a password, which of the following should they enter?

A) 654321Winter2025  
B) Winter2025-654321  
C) Winter2025654321  
D) Winter2025 654321  

**Correct Answer:** C

---

Question 15. You are configuring SnowSQL for a user whose password is `SecurePass!` and who receives an MFA token of `987654`. When prompted for a password while using `--mfa-passcode-in-password`, what is the correct input?

A) SecurePass!987654  
B) 987654SecurePass!  
C) SecurePass!-987654  
D) SecurePass! 987654  

**Correct Answer:** A

---

Question 16. A company needs to securely share product catalog data from Snowflake with a partner that is not a Snowflake customer and uses Amazon S3 for storage. Both tables, PRODUCT_CATEGORY and PRODUCT_DETAILS, need to be joined and only the partner should have access. What is the most cost-effective and secure Snowflake solution for this requirement?

A) Create a secure view in Snowflake and share the credentials with the partner.  
B) Use Snowflake's external function to directly write data to the partner's S3 bucket, ensuring access is restricted.  
C) Export the joined and filtered data to a secure stage, then use Snowflake’s COPY INTO command to unload the data as files to the partner's Amazon S3 bucket with proper permissions.  
D) Give the partner VPN access to the Snowflake account and restrict queries at the network layer.  

**Correct Answer:** C

---

Question 17. The partner requires access only to the product catalog records, and data access should be strictly governed. Which Snowflake feature best enforces this requirement while exporting data to Amazon S3?

A) Use a masking policy on the PRODUCT_ID column.  
B) Implement a row access policy to filter data before unloading to S3.  
C) Create a public stage and let the partner download the files.  
D) Share the entire database with the partner through a data share.  

**Correct Answer:** B

---

Question 18. After exporting the required data from Snowflake to Amazon S3, what is the best way to ensure only the intended partner can access these files?

A) Store the files in a Snowflake internal stage with open access.  
B) Use S3 bucket policies to allow access only from the partner’s AWS account.  
C) Email the exported files directly to the partner.  
D) Make the S3 bucket public for easy access.  

**Correct Answer:** B

---

Question 19. A user connects to Snowflake and starts a new session. The user does not explicitly specify a warehouse in their query. In what order does Snowflake determine which warehouse to use for the session?

A) Role default warehouse → User default warehouse → Account default warehouse → No active warehouse  
B) User default warehouse → Role default warehouse → Account default warehouse → No active warehouse  
C) User default warehouse → Account default warehouse → Role default warehouse → No active warehouse  
D) Account default warehouse → User default warehouse → Role default warehouse → No active warehouse  

**Correct Answer:** B

**Explanation:**  
Snowflake determines the active warehouse for a session in the following hierarchy:  
1. The warehouse specified by the user in the session  
2. The user’s default warehouse  
3. The role’s default warehouse  
4. The account’s default warehouse  
5. If none are set, there is no active warehouse for the session

---

Question 20. A company needs to share its product catalog, stored in Snowflake tables PRODUCT_CATEGORY and PRODUCT_DETAILS, with a business partner who uses Amazon S3 for storage and is not a Snowflake customer. Which approach is the most cost-effective and secure for providing the data to the partner?

A) Create a Snowflake Data Share and grant access to the partner’s AWS account.  
B) Use the COPY INTO command to export the joined tables as files to a secure S3 bucket, and share access only with the partner’s AWS account.  
C) Email CSV exports of the tables to the partner.  
D) Create a public Snowflake stage and instruct the partner to download the files from there.  

**Correct Answer:** B

---

Question 21. To ensure that only the partner has access to the product catalog data exported to S3, which security measure should the Snowflake Architect prioritize?

A) Set the S3 bucket policy to allow access from any AWS user.  
B) Encrypt the exported files with a password and send the password by email.  
C) Apply an S3 bucket policy that grants read access exclusively to the partner’s AWS account.  
D) Store the files in an unencrypted S3 bucket for ease of access.  

**Correct Answer:** C

---

Question 22. The partner requires access only to specific records in the product catalog export. What is the most efficient way, using Snowflake features, to meet this requirement before unloading data to S3?

A) Use masking policies to hide data columns in the export.  
B) Filter and join the required records in a SQL query and use COPY INTO to export only the relevant data to S3.  
C) Export the entire tables and ask the partner to filter the records after downloading.  
D) Manually delete unwanted records from the S3 files after export.  

**Correct Answer:** B

---

Question 23. During a regional outage in AWS US East, the operations team fails over Snowflake workloads to a secondary account in Azure Europe West. After promoting the secondary account to primary, they notice a 30-minute data lag in analytics dashboards. What is the most likely reason for this data lag?

A) Network latency between AWS and Azure  
B) Replication schedule interval between primary and secondary accounts  
C) Insufficient compute resources in Azure Europe West  
D) Data corruption during failover  

**Correct Answer:** B

---

Question 24. After recovering from an outage and restoring the AWS US East region, which of the following is a valid post-outage step to ensure data consistency between the two Snowflake accounts?

A) Decrease the size of the compute warehouses in both regions  
B) Perform a manual replication or sync to reconcile data between the accounts  
C) Switch the replication schedule to every hour  
D) Delete the failover group in Azure Europe West  

**Correct Answer:** B

---

Question 25. What is one architectural benefit of having Snowflake accounts in both AWS US East and Azure Europe West for a global enterprise?

A) It increases licensing costs  
B) It enables regionally isolated compute resources  
C) It provides high availability and business continuity across cloud providers  
D) It complicates user access management  

**Correct Answer:** C

---

Question 26. A retail company uses Snowflake to store both structured transactional data and semi-structured JSON event data. What is the most appropriate method to maintain consistency across teams and enable efficient querying?

A) Store both data types in VARIANT columns without documentation  
B) Use structured tables for transactional data and VARIANT columns with standardized views for JSON event data  
C) Convert all JSON event data to CSV before loading into Snowflake  
D) Keep all data in separate databases for each team  

**Correct Answer:** B

---

Question 27. In designing a Snowflake data warehouse for mixed data types, which of the following actions best supports efficient querying of semi-structured event data?

A) Load JSON data as plain text in VARCHAR columns  
B) Store JSON data in VARIANT columns and create views that flatten the data for analytics  
C) Store all data in a single wide table with every possible attribute  
D) Use separate Snowflake accounts for event data and transactional data  

**Correct Answer:** B

---

Question 28. To ensure consistency and collaboration across multiple teams in a Snowflake-powered retail data warehouse, which strategy should the Architect prioritize?

A) Allow each team to define their own JSON key structures  
B) Document and standardize JSON key paths and table schemas, and provide shared views for common queries  
C) Restrict access to event data to only the data engineering team  
D) Disable semi-structured data support in Snowflake  

**Correct Answer:** B

---

Question 29. Which Snowflake feature allows a media company to continuously ingest JSON event data from cloud storage into Snowflake tables with low latency and automated file detection?

A) Snowflake Streams  
B) Snowpipe  
C) Time Travel  
D) External Tables  

**Correct Answer:** B

---

Question 30. A custom Java application streams real-time JSON event data. To achieve exactly-once delivery and handle failures gracefully, which Snowflake solution should the Architect recommend?

A) Use Snowpipe Streaming with the Snowflake Ingest SDK and offset tokens  
B) Batch load data daily using COPY INTO commands  
C) Store data in VARIANT columns and query periodically  
D) Load data via manual file uploads to the UI  

**Correct Answer:** A

---

Question 31. When implementing Snowpipe Streaming with the Snowflake Ingest SDK, what mechanism is used to resume ingestion after a failure without duplicating data?

A) File timestamps  
B) Offset tokens  
C) Retry counters  
D) Row-level security policies  

**Correct Answer:** B

---

Question 32. For a requirement of low-latency ingestion and real-time analytics, which configuration provides the most direct path from a custom application to Snowflake?

A) Java app writes to cloud storage, then Snowpipe loads files  
B) Java app uses Snowflake Ingest SDK to stream data directly into Snowflake tables  
C) Java app writes CSV files for batch ETL  
D) Java app sends data to a third-party data lake, then Snowflake external tables query the lake  

**Correct Answer:** B

---

Question 33. How does Snowflake ensure exactly-once ingestion when using Snowpipe Streaming and the Ingest SDK?

A) By using offset tokens that track the ingestion progress  
B) By requiring all files to be unique  
C) By running periodic deduplication jobs  
D) By preventing simultaneous connections  

**Correct Answer:** A

---

Question 34. Which architectural pattern best supports both failure handling and operational monitoring for a real-time Snowflake pipeline ingesting JSON event data from a web application?

A) Use Snowpipe Streaming with Ingest SDK, implement logging and alerting on ingestion errors and offsets  
B) Only rely on periodic batch loading and ignore failures  
C) Use Snowflake’s Time Travel to undo failed ingestions  
D) Store event data in external tables and query on demand  

**Correct Answer:** A

---

Question 35. A company with a Business Critical Snowflake account wants to enable disaster recovery for a sensitive customer database across regions and cloud providers. What is the first step the architect should take?

A) Encrypt the database with a master key  
B) Create a failover group including the customer database  
C) Configure a multi-cluster warehouse  
D) Set up time travel for the database  

**Correct Answer:** B

---

Question 36. After creating a failover group for a sensitive database, which step ensures the group is available in a secondary Snowflake account located in a different region and cloud provider?

A) Grant replication privileges and configure cross-region replication to the secondary account  
B) Export data to CSV files and upload to the secondary account  
C) Enable auto-scaling on the primary warehouse  
D) Use Snowflake Streams to synchronize data  

**Correct Answer:** A

---

Question 37. A healthcare company needs to restrict access to patient SSN data so that only users with the 'COMPLIANCE_OFFICER' role can view the unmasked information, while all other users see masked data. Which Snowflake context function should you use inside the masking policy to accomplish this requirement?

A) IS_ROLE_IN_SESSION  
B) CURRENT_ROLE  
C) USER_ROLE  
D) SESSION_USER  

**Correct Answer:** B

---

Question 38. An architect is designing a multi-tenant analytics solution where different clients should only be able to see their own data. The solution uses secure views and dynamic masking policies. Which context function should be used in the masking policy to ensure that the masking logic is enforced based on the role that runs the view, rather than the owner of the view?

A) INVOKER_ROLE  
B) CURRENT_ACCOUNT  
C) OBJECT_OWNER  
D) CURRENT_SESSION  

**Correct Answer:** A

---

Question 39. Your organization uses multiple roles with different privileges. You want a masking policy to check if a specific privileged role (e.g., 'DATA_AUDITOR') is active in the session, and only show unmasked data if that role is present. Which context function should the masking policy use to implement this condition?

A) CURRENT_USER  
B) CURRENT_ROLE  
C) IS_ROLE_IN_SESSION  
D) INVOKER_ROLE  

**Correct Answer:** C

---

Question 40. A retail company wants to provide its developers with access to a fresh copy of production sales data for testing new analytics features. The company needs to minimize storage costs and quickly create test environments in their Snowflake account. Which approach should the architect recommend?

A) Use CREATE TABLE ... AS SELECT (CTAS) statements to copy production data into new tables for each environment.  
B) Use zero-copy cloning into transient tables for each environment.  
C) Export production data to external files and reload into test tables.  
D) Use permanent tables for cloned environments to leverage fail-safe.

**Correct Answer:** B

---

Question 41. A financial services firm requires a pre-production environment to validate new features against real production data before release. The environment must be isolated, cost-effective, and easy to refresh. Which Snowflake feature addresses these requirements most efficiently?

A) Deep copy production tables into new schemas using CTAS and regularly refresh using ETL jobs.  
B) Use zero-copy cloning to create transient tables for pre-production validation, refreshing as needed.  
C) Move production data to a separate Snowflake account and reload for each environment.  
D) Grant all users access to the production database for testing.

**Correct Answer:** B

---

Question 42. An e-commerce business wants to ensure that its development and test environments are as close as possible to production, but without incurring unnecessary long-term storage costs. Which Snowflake table type should the architect use when cloning production data for these environments?

A) Permanent tables  
B) Transient tables  
C) Temporary tables  
D) External tables  
E) Materialized views  

**Correct Answer:** B

---

Question 43. An e-commerce company frequently receives product inventory files in different formats (CSV, JSON) from multiple vendors in their Azure Blob storage. They want to automate the process of making these files queryable in Snowflake with minimal manual intervention. Which strategy is most effective for the architect to implement?

A) Manually create a new external table for each file and format as they arrive.  
B) Configure multiple external stages and file formats, then use Snowflake's external table feature to query new files automatically as they appear in the external storage.  
C) Convert all files to a single format before uploading and use a single external table definition.  
D) Use Snowpipe to ingest all data into a raw staging table and query only the internal tables.

**Correct Answer:** B

---

Question 44. A financial analytics team has noticed that queries run on their Snowflake warehouse are frequently queued during certain periods, impacting report delivery timelines. As the Snowflake Architect, which data source would you use to identify the specific days and warehouses where enabling multi-cluster would provide the most benefit?

A) SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY  
B) SNOWFLAKE.ACCOUNT_USAGE.LOGIN_HISTORY  
C) SNOWFLAKE.ACCOUNT_USAGE.METERING_HISTORY  
D) SNOWFLAKE.INFORMATION_SCHEMA.OBJECTS  

**Correct Answer:** A

---

Question 45. An e-commerce company wants to improve performance for its busiest data science workloads. The architect needs to recommend scaling strategies based on real warehouse activity. Which metric should be analyzed to determine if multi-cluster warehouses are needed?

A) Number of distinct users logged in  
B) Average queued time for queries per warehouse per day  
C) Number of tables in the database  
D) Total amount of data stored in stages  




Question 46. A Snowflake Architect is tasked with optimizing cost and performance for several virtual warehouses. Management asks how to justify multi-cluster configuration for only a subset of warehouses and days. What is the best approach?

A) Enable multi-cluster on all warehouses regardless of usage  
B) Analyze query history to find periods of high concurrency and queuing, then target those warehouses and days  
C) Move all workloads to a single large warehouse  
D) Increase the auto-suspend timeout on warehouses to avoid queuing  

**Correct Answer:** B

---

Question 47. A pharmaceutical company needs to share sensitive research data with a partner who operates in a different cloud provider and region. What is the recommended Snowflake feature to enable secure, private sharing between the company’s AWS us-west-2 account and the partner’s Azure East US 2 account?

A) Use public data sharing via Snowflake Marketplace  
B) Use private data sharing with a reader account in the same cloud region  
C) Use cross-cloud private data sharing via a direct share to the customer’s Azure account  
D) Export data to S3 and have the customer ingest it manually  

**Correct Answer:** C

---

Question 48. During a cross-cloud private data sharing setup, which key step must the data provider complete to ensure the consumer in Azure East US 2 can access the shared data from their AWS us-west-2 Snowflake account?

A) Create a share and invite the consumer’s account locator in the Azure region  
B) Grant SELECT privileges directly to the consumer’s user  
C) Move the provider’s Snowflake account to Azure  
D) Use Snowpipe to stream the data to Azure Blob Storage  

**Correct Answer:** A

---

Question 49. An architect is asked to outline the correct sequence of actions for enabling cross-cloud private data sharing between two Snowflake accounts. Which sequence should they recommend?

A) Create a share → Grant table privileges → Invite consumer using their account locator → Consumer creates a database from the share  
B) Export data to CSV → Upload to Azure Blob Storage → Consumer loads data into Snowflake  
C) Grant privileges to the consumer’s Snowflake user → Consumer queries the provider’s tables  
D) Create a reader account in the provider’s region → Consumer accesses shared data via the reader account  

**Correct Answer:** A

---

Question 50. Company B maintains their sales data in on-premises databases, while company A’s Snowflake account is hosted in AWS us-east-1. What is the most efficient way for the architect to consolidate company B’s sales data into company A’s Snowflake environment for ongoing integration?

A) Use Snowflake’s data sharing feature between accounts  
B) Export data from Company B’s database as files, upload to an S3 bucket in us-east-1, and use an external stage with COPY INTO to load data into Snowflake  
C) Connect directly to company B’s database from Snowflake using native connectors  
D) Use a Snowflake reader account in company B’s environment  

**Correct Answer:** B

---

Question 51. Company B has their sales data in their own Snowflake account, but it is hosted in a different region and cloud provider than company A’s Snowflake account in AWS us-east-1. Which Snowflake capability should the architect use to consolidate sales data into company A’s account with minimal data movement and security risk?

A) Snowflake Secure Data Sharing  
B) Manual data export and import via CSV files  
C) Replicate the entire Snowflake account to AWS us-east-1  
D) Use Snowpipe to stream data across regions  

**Correct Answer:** A

---

Question 52. The architect wants to automate the consolidation process so that company B’s latest sales data is regularly ingested into company A’s Snowflake account in AWS us-east-1, with minimal manual effort. Which solution is best suited for this requirement?

A) Use an ETL tool (such as Fivetran or Informatica) to schedule incremental loads from company B’s source system to company A’s Snowflake account  
B) Require company B to email CSV files daily for manual upload  
C) Manually run COPY INTO commands each week  
D) Use a physical data courier to deliver external drives with data  

**Correct Answer:** A

---

Question 53. A multinational company operates separate Snowflake accounts for development and production in different geographic regions. The development team needs to work with production data. Why would copying the data be required in this scenario, rather than using zero-copy cloning?

A) Zero-copy cloning does not support copying data across different accounts or regions.  
B) Zero-copy cloning always creates a physical copy of the data by default.  
C) Zero-copy cloning automatically masks the data for security compliance.  
D) Zero-copy cloning encrypts the data during transfer between accounts.  

**Correct Answer:** A

---

Question 54. An architect is tasked with creating a testing environment that requires only a subset of production data, with sensitive information masked for compliance reasons. Why is zero-copy cloning not suitable for this scenario?

A) Zero-copy cloning is only available for database administrators.  
B) Zero-copy cloning does not allow for data transformation or masking during the cloning process.  
C) Zero-copy cloning automatically anonymizes all data.  
D) Zero-copy cloning is only supported for temporary tables.  

**Correct Answer:** B

---

Question 55. In which situation would copying data be preferred over zero-copy cloning when setting up a persistent test environment that should remain accessible even after the source production data is deleted or altered?

A) Zero-copy clones are independent of the source and persist even after the original table is dropped.  
B) Zero-copy cloning automatically refreshes the test environment with new production data.  
C) Copying data creates a fully independent copy that remains accessible regardless of changes to the source.  
D) Zero-copy cloning supports cross-cloud transfers without additional configuration.  

**Correct Answer:** C

---

Question 56. After cloning a production database in Snowflake to create a test environment, the architect notices that scheduled tasks in the cloned database are not executing. What is the most likely reason for this behavior?

A) Tasks in cloned databases are automatically enabled and should run without intervention.  
B) Cloned tasks are suspended by default to prevent unintended execution in the new environment.  
C) The clone does not include any tasks from the original database.  
D) Tasks require manual re-creation after cloning a database.  

**Correct Answer:** B

---

Question 57. What action must a Snowflake architect take to resume scheduled task execution after cloning a database and its objects?

A) Nothing; tasks will automatically resume once the clone is created.  
B) Use the ALTER TASK <task_name> RESUME command to enable tasks in the cloned database.  
C) Re-create all tasks in the cloned environment from scratch.  
D) Enable automatic scheduling in the Snowflake account settings.  

**Correct Answer:** B

---

Question 58. After implementing Okta SSO with MFA for Snowflake, the Data Analyst team using DBeaver reports frequent credential prompts that disrupt workflow. What is the best way for the architect to reduce these prompts and improve analyst productivity?

A) Switch DBeaver’s authentication method to OAuth and configure Okta as the identity provider for session reuse.  
B) Disable MFA in Okta for all analyst accounts.  
C) Increase the frequency of password resets for analyst users.  
D) Require analysts to use the Snowflake web UI instead of DBeaver.  

**Correct Answer:** A

---

Question 59. The data analysts are experiencing frequent sign-in prompts in DBeaver after SSO/MFA was enabled in Snowflake. Which configuration should the architect recommend to minimize these interruptions without compromising security?

A) Configure DBeaver to use OAuth authentication for Snowflake, allowing session persistence and SSO token reuse.  
B) Remove SSO and MFA requirements from Snowflake.  
C) Ask analysts to save their passwords in a spreadsheet for convenience.  
D) Set up a shared analyst account with no authentication.  

**Correct Answer:** A

---

Question 60. USER_01 has been assigned to build a materialized view in the schema EDW.STG_SCHEMA. Which privilege must the architect grant on the schema to allow USER_01 to create a materialized view?

A) CREATE VIEW  
B) CREATE MATERIALIZED VIEW  
C) OWNERSHIP  
D) USAGE  

**Correct Answer:** B

---

Question 61. After granting USER_01 the CREATE MATERIALIZED VIEW privilege on EDW.STG_SCHEMA, USER_01 still cannot create a materialized view referencing tables in the schema. What additional privilege must be granted to USER_01 to resolve this issue?

A) DELETE on the referenced tables  
B) OWNERSHIP on the referenced tables  
C) SELECT on the referenced tables  
D) INSERT on the referenced tables  

**Correct Answer:** C

---

Question 62. An architect wants to ensure USER_01 can create and query materialized views in EDW.STG_SCHEMA but does not want to grant excessive permissions. Which combination of privileges should be granted to satisfy both requirements securely?

A) USAGE on the schema and CREATE MATERIALIZED VIEW on the schema  
B) CREATE MATERIALIZED VIEW on the schema and OWNERSHIP on all tables  
C) USAGE on the schema and SELECT on all tables in the schema  
D) USAGE on the schema, CREATE MATERIALIZED VIEW on the schema, and SELECT on the referenced tables  

**Correct Answer:** D

---

Question 63. A retail analytics team complains about slow query performance when accessing external tables in Snowflake that reference data in an AWS S3 data lake. What is the most effective way for the architect to improve performance for frequently accessed queries?

A) Create materialized views on the external tables  
B) Increase the size of the Snowflake virtual warehouse  
C) Move the external data to Snowflake internal tables  
D) Enable automatic clustering on the external tables  

**Correct Answer:** C

---

Question 64. Users report slow performance when querying external tables that reference thousands of small files in cloud storage. What action should the architect take to optimize query speed?

A) Partition the files by commonly queried columns  
B) Combine smaller files into larger files to reduce metadata overhead  
C) Use uncompressed CSV format for all files  
D) Increase the number of external tables  

**Correct Answer:** B

---

Question 65. The architect observes that queries on external tables are not efficiently scanning only necessary data. Which optimization can help Snowflake skip scanning unnecessary files and improve query performance?

A) Use partitioned file structures in cloud storage and write queries using partition columns in WHERE clauses  
B) Set the file format to JSON for better compatibility  
C) Disable caching for external tables  
D) Drop and recreate the external tables daily  

**Correct Answer:** A

---

Question 66. The Operations team must manually download failed files from an internal named stage for error analysis after an ingestion failure. Which method should the Architect recommend to automate and reduce operational overhead for file retrieval?

A) Instruct the team to manually copy files using the Snowflake web UI  
B) Use the Snowflake GET command to programmatically download files from the internal stage  
C) Request files from the stage by emailing the Snowflake administrator  
D) Manually transfer files using FTP from the Snowflake stage  

**Correct Answer:** B

---

Question 67. When designing a recovery solution for file ingestion failures, which approach allows the Operations team to most efficiently and consistently retrieve failed files for inspection from an internal stage?

A) Automate file download using the GET command within a script or scheduled task  
B) Assign more staff to manually handle downloads during peak failure times  
C) Require users to access the stage via external cloud storage tools  
D) Use manual downloads from the Snowflake UI for each failed file  

**Correct Answer:** A

---

Question 68. A healthcare company operating on Snowflake Business Critical edition wants to securely share data with a partner using an Enterprise edition account. Which statement accurately describes this capability?

A) Data sharing is only supported between accounts on the same edition  
B) Secure data sharing is supported between Business Critical and Enterprise edition accounts, subject to compliance and security controls  
C) Data sharing is not possible outside the Business Critical edition  
D) Data sharing requires both accounts to be upgraded to Enterprise edition  

**Correct Answer:** B

---

Question 69. A Business Critical edition provider shares data with an Enterprise edition consumer. What important limitation should the architect consider?

A) The consumer cannot access any shared data  
B) The consumer may not have access to certain Business Critical-only features, such as Tri-Secret Secure  
C) The provider must downgrade their edition before sharing  
D) Data sharing is only possible in the same region  

**Correct Answer:** B

---

Question 70. A Business Critical data provider wants to share sensitive data with an Enterprise edition consumer but has enabled account-level restriction (override restriction) for sharing. What effect does this have?

A) The override restriction prevents the provider from sharing data with accounts on lower editions unless explicitly permitted  
B) The Enterprise consumer can always access the shared data, regardless of restrictions  
C) The override restriction only applies to metadata, not actual data  
D) Override restrictions are ignored during secure data sharing  

**Correct Answer:** A

---

Question 71. An Architect notices that the COMPILATION_TIME of a query in QUERY_HISTORY is significantly higher than its EXECUTION_TIME. What is the most likely reason for this observation?

A) The query executed against a very large dataset  
B) The query involves complex logic, joins, or subqueries that require extensive optimization during compilation  
C) The query was run during peak hours with heavy warehouse load  
D) The query was suspended during execution by the administrator  

**Correct Answer:** B

---

Question 72. When examining QUERY_HISTORY, an Architect sees several queries with higher COMPILATION_TIME than EXECUTION_TIME. Which scenario could result in quick execution but lengthy compilation?

A) The queries reference simple tables with no indexes  
B) The queries are accessing tables with very few rows  
C) The queries have complex structures but ultimately filter down to a small result set  
D) The queries use only SELECT * FROM table statements  

**Correct Answer:** C

---

Question 73. What is a possible operational cause for consistently high COMPILATION_TIME compared to EXECUTION_TIME in a Snowflake environment?

A) Frequent changes to schema objects, such as table structures or view definitions, preventing cache reuse during query compilation  
B) The virtual warehouse is undersized and cannot process data quickly  
C) Users are running queries in the Snowflake web UI instead of a BI tool  
D) Data is stored in an unsupported file format  

**Correct Answer:** A

---

Question 74. An analyst is required to use MFA when connecting to Snowflake via SnowSQL. If the password is `SNOWFLAKE` and the current MFA token is `123456`, what should the analyst enter at the password prompt when using `--mfa-passcode-in-password`?

A) SNOWFLAKE:123456  
B) SNOWFLAKE123456  
C) 123456SNOWFLAKE  
D) SNOWFLAKE 123456  

**Correct Answer:** B

---

Question 75. During a security audit, the architect notices that users are connecting to Snowflake using SnowSQL and MFA. What is the correct method for including the MFA token when prompted for a password with the `--mfa-passcode-in-password` flag?

A) Enter the MFA token only  
B) Enter the password, then the MFA token, separated by a space  
C) Concatenate the password and MFA token with no spaces or separators  
D) Use a comma to separate the password and MFA token  

**Correct Answer:** C

---

Question 76. Which of the following is a security best practice when using the `--mfa-passcode-in-password` option in SnowSQL for MFA-enabled accounts?

A) Always separate the password and token by a colon  
B) Enter the password first, then the MFA token, without any spaces or special characters  
C) Share the combined password and token with colleagues to simplify login  
D) Store the combined password and token in a plaintext file for convenience  

**Correct Answer:** B

---

Question 77. During a CI/CD deployment, an architect attempts to clone a production table into the development environment, but the operation fails. Upon investigation, the architect finds that the table’s data retention time in production was set to a very low value. Which of the following best explains why the cloning operation failed?

A) The low retention time caused some historical data to be unavailable for cloning.  
B) The development environment does not support cloning operations.  
C) The table was encrypted in production.  
D) The table’s schema was incompatible with development.  

**Correct Answer:** A

---

Question 78. In a CI/CD workflow, an architect is unable to clone a table from production to development. After checking permissions, the architect notices that users in the development environment do not have the necessary privileges to access the production table. What step should the architect take to resolve this issue?

A) Increase the table’s data retention period in production.  
B) Grant the appropriate privileges on the production table to the development environment’s users.  
C) Change the table’s storage format.  
D) Deactivate CI/CD automation.  

**Correct Answer:** B

---

Question 79. While cloning a table from production to development as part of a CI/CD process, an architect encounters a failure. The architect discovers that the production table uses features not supported in the development environment, such as certain clustering keys. What is the best solution to enable successful cloning?

A) Remove the unsupported features from the production table before cloning.  
B) Increase the development environment’s compute resources.  
C) Set a longer retention time on the production table.  
D) Use a different CI/CD tool.  

**Correct Answer:** A

---

Question 80. A company needs to share its product catalog (stored in PRODUCT_CATEGORY and PRODUCT_DETAILS tables) with a partner who is not a Snowflake customer and uses Amazon S3 for cloud storage. The company wants to ensure only the partner has access and manages data securely and cost-effectively. Which Snowflake feature should the architect recommend for this scenario?

A) Use Snowflake Data Sharing to create a secure share for the partner.  
B) Export the data to Parquet format and use Snowflake’s External Functions to upload directly to the partner’s S3 bucket.  
C) Use Snowflake’s COPY INTO command to export the data to Amazon S3 and control access via S3 permissions.  
D) Grant the partner direct read access to the Snowflake tables using a Snowflake Reader account.  

**Correct Answer:** C

---

Question 81. The company must ensure that only the intended partner can access the exported product catalog data on Amazon S3, and that access is auditable. Which of the following actions should the architect implement as part of the solution?

A) Set up S3 bucket policies restricting access to the partner’s AWS account.  
B) Share the S3 bucket credentials with the partner via email.  
C) Make the S3 bucket public for easy access.  
D) Use Snowflake’s secure views to mask sensitive data before export.  

**Correct Answer:** A

---

Question 82. In order to minimize costs and automate the data sharing process between Snowflake and the partner’s Amazon S3, which solution should the architect implement?

A) Schedule a Snowflake task to periodically run a COPY INTO statement, exporting only updated records to S3.  
B) Manually download query results and upload them to the S3 bucket.  
C) Use Snowflake Streams to replicate data directly to the partner’s AWS account.  
D) Enable continuous data sharing using Snowflake’s secure data exchange marketplace.  

**Correct Answer:** A

---

Question 83. A data engineer logs in to Snowflake and starts a session without specifying a warehouse in their connection string. The account has a default warehouse set, and the engineer’s user profile also has a default warehouse. Which warehouse will Snowflake assign to the session by default?

A) The warehouse specified in the connection string  
B) The default warehouse set for the user profile  
C) The warehouse last used in the previous session  
D) The account-level default warehouse  

**Correct Answer:** B

---

Question 84. During a troubleshooting scenario, an architect is asked why a particular session used a specific warehouse even though the user did not specify one explicitly and their user profile does not have a default warehouse. Which warehouse will be used for the session?

A) The warehouse specified in the user's previous query  
B) No warehouse will be assigned and queries will fail  
C) The default warehouse set at the account level  
D) The warehouse with the highest compute resources  

**Correct Answer:** C

---

Question 85. A BI application connects to Snowflake and specifies a warehouse in its connection string, even though the user’s profile and account both have default warehouses set. Which warehouse will be active for the session?

A) The warehouse specified in the connection string  
B) The default warehouse for the account  
C) The default warehouse for the user profile  
D) The warehouse last assigned by the administrator  

**Correct Answer:** A

---

Question 86. A multinational company is migrating its analytics platform to Snowflake. The architect wants to ensure that only specific users can access sensitive financial data, while other team members have broader access to sales data. Which feature of Role-Based Access Control (RBAC) helps the architect achieve this selective access?

A) RBAC allows roles to be assigned to users and privileges to be granted to roles, enabling fine-grained access control.  
B) RBAC automatically grants all users full access to all data.  
C) RBAC enforces access management only at the account level.  
D) RBAC requires manual permission assignment for every individual user and object.  

**Correct Answer:** A

---

Question 87. A Snowflake Architect is designing an environment where data scientists should be able to query but not modify production tables. Which characteristic of RBAC would best support this requirement?

A) RBAC allows for row-level security policies.  
B) RBAC enables the assignment of read-only roles to users, restricting their ability to modify data.  
C) RBAC prevents users from accessing any data unless they are administrators.  
D) RBAC supports automatic privilege escalation for queries.  

**Correct Answer:** B

---

Question 88. A project manager asks the Snowflake Architect how permissions can be efficiently managed across hundreds of users and objects. What is a key advantage of RBAC for this scenario?

A) RBAC enables centralized privilege management by granting permissions to roles rather than to individual users.  
B) RBAC requires privileges to be granted directly to each user for every object.  
C) RBAC only supports static privilege assignments.  
D) RBAC restricts privilege management to database administrators only.  

**Correct Answer:** A

---

Question 89. An architect is reviewing the security configuration of a Snowflake account and finds that users are sometimes assigned multiple roles. What is a result of this RBAC characteristic?

A) Users can switch between roles in a session to access different sets of privileges, based on their current role.  
B) Users can only use privileges from their default role.  
C) Users lose access to all data if multiple roles are assigned.  
D) Users are forced to use all roles simultaneously.  

**Correct Answer:** A

---

Question 90. During an audit, the compliance team asks how Snowflake ensures that only authorized users can perform sensitive operations like creating or dropping tables. Which RBAC characteristic helps address this concern?

A) RBAC restricts sensitive operations by only allowing roles with specific privileges to execute them.  
B) RBAC allows all users to perform any operation by default.  
C) RBAC requires external IAM integration for all operations.  
D) RBAC does not support object-level privileges.  

**Correct Answer:** A

---

Question 91. A retail company has created a materialized view to accelerate frequent sales summary queries. After deployment, the analytics team notices that some queries matching the materialized view’s definition are not using the materialized view for execution. What could explain this behavior?

A) Snowflake only rewrites queries to use a materialized view if the base tables have not changed since the last refresh.  
B) Snowflake does not guarantee that every matching query will be dynamically rewritten to use the materialized view due to factors such as query structure, filters, or join order.  
C) The materialized view is automatically used for all queries that match its definition, regardless of other conditions.  
D) The materialized view must be manually referenced in every query to be utilized.  

**Correct Answer:** B

---

Question 92. An architect optimizes a dashboard by creating materialized views for the most common queries. However, users observe that performance improvements are inconsistent and sometimes queries do not benefit from the materialized views. Which scenario best explains why Snowflake might not rewrite certain queries to use a materialized view?

A) The queries contain additional columns not present in the materialized view.  
B) The queries are executed by users without sufficient privileges.  
C) Snowflake only rewrites queries to use materialized views if the query matches the definition and other rewrite conditions are satisfied.  
D) The virtual warehouse size is too small.  

**Correct Answer:** C

---

Question 93. After creating a materialized view, a Snowflake Architect runs a query that is structurally identical to the materialized view’s definition. Surprisingly, the query does not use the materialized view. Which of the following is a valid reason for this outcome?

A) The materialized view is still refreshing and not yet available for query rewriting.  
B) Snowflake always rewrites the query to use the materialized view if the definitions match.  
C) The query must include a USE MATERIALIZED VIEW clause.  
D) Only queries run by account administrators can use materialized views.  

**Correct Answer:** A

---

Question 94. A financial analyst is running a series of SQL statements to update customer balances in Snowflake. The analyst wants to ensure that all statements either complete successfully together or none are applied if an error occurs. Which characteristic of transactions in Snowflake enables this business requirement?

A) Transactions in Snowflake automatically commit each individual statement.  
B) Transactions in Snowflake support atomicity, allowing multiple statements to be committed or rolled back as a single unit.  
C) Transactions in Snowflake are limited to read-only operations.  
D) Transactions require manual log file management for rollback.  

**Correct Answer:** B

---

Question 95. A retail company’s ETL process inserts and updates data in several tables during nightly loads. The architect wants to ensure that, in case of a failure, no partial changes are made to the database. Which transaction behavior in Snowflake helps achieve this?

A) Snowflake automatically saves all changes, even if an error occurs.  
B) Snowflake supports the use of explicit COMMIT and ROLLBACK statements to control transaction boundaries and undo partial changes.  
C) Snowflake only supports implicit commits with no rollback capability.  
D) Transactions in Snowflake are only available for SELECT queries.  

**Correct Answer:** B

---

Question 96. A development team needs to coordinate updates to multiple related tables to ensure data consistency. During a deployment, they encounter a deadlock when two sessions try to update the same set of rows. What does this reveal about Snowflake’s transaction management?

A) Snowflake does not support concurrent transactions.  
B) Snowflake uses optimistic concurrency control, and deadlocks are possible when transactions contend for the same data.  
C) Snowflake automatically serializes all transactions to prevent conflicts.  
D) Transactions in Snowflake do not support updates to multiple tables.  

**Correct Answer:** B

Question 97. A logistics company is experiencing delays when collecting telemetry data from its fleet. The architect proposes Kafka. Which feature of Kafka is most relevant to solving this business problem?

A) Kafka’s ability to batch data for weekly reports  
B) Kafka’s support for distributed, fault-tolerant streaming  
C) Kafka’s built-in machine learning models  
D) Kafka’s data visualization capabilities  
E) Kafka’s user authentication system  

**Correct Answer:** B

---

Question 98. A media company wants to capture, process, and distribute live event data to multiple analytics systems simultaneously. Why would an architect recommend Kafka?

A) Kafka integrates with Excel for manual analysis  
B) Kafka allows simultaneous, scalable data distribution to multiple consumers  
C) Kafka replaces relational databases for historical storage  
D) Kafka performs automatic data masking  
E) Kafka is a cloud-only service  

**Correct Answer:** B

---

Question 99. A media company stores large volumes of JSON log files from various sources and needs to efficiently ingest and analyze this data in Snowflake. What is the MOST efficient technique to support these semi-structured workloads?

A) Load data using the COPY INTO command with the VARIANT column type  
B) Convert JSON files to CSV before loading  
C) Use Snowflake’s secure views to analyze the data  
D) Store files in external tables only  
E) Use clustering keys to organize the data  

**Correct Answer:** A

---

Question 100. An e-commerce business regularly receives semi-structured data in Avro and Parquet formats from its partners. The architect must enable scalable analytics on this data in Snowflake. Which ingestion approach is best suited to this use case?

A) Use Snowflake’s automatic clustering feature  
B) Load Avro and Parquet files into a VARIANT column using COPY INTO  
C) Convert all files to XML before loading  
D) Apply masking policies to every file  
E) Upload files manually using the web UI  

**Correct Answer:** B

---

Question 101. A logistics firm wants to analyze real-time sensor data sent as semi-structured JSON to their Snowflake data lake. Which technique will best enable fast ingestion and flexible querying of this data?

A) Use Snowpipe to stream JSON files into Snowflake with VARIANT columns  
B) Insert each record manually via a worksheet  
C) Convert the JSON to flat tables before loading  
D) Schedule batch jobs with the COPY INTO command for CSV files  
E) Export the data to a third-party BI tool before ingesting  

**Correct Answer:** A

---

Question 102. A media company ingests large volumes of semi-structured data from multiple sources into their Snowflake data lake. The data structures often vary and evolve over time. What is the primary advantage of using a schema-on-read approach in this environment?

A) It enforces strict column definitions before data ingestion  
B) It allows flexible querying and interpretation of data structure at query time  
C) It requires converting all incoming data into flat tables  
D) It limits the data types that can be ingested  
E) It automatically indexes all columns  

**Correct Answer:** B

---

Question 103. An architect is considering how to handle frequent changes in JSON and Avro files from external partners. Which statement best describes the schema-on-read technique in Snowflake?

A) Schema-on-read means the data’s structure is defined before ingestion  
B) Schema-on-read requires users to manually update table definitions for each change  
C) Schema-on-read enables Snowflake to infer the structure of semi-structured data when queries are run  
D) Schema-on-read forces users to convert data to CSV format  
E) Schema-on-read disables time travel features  

**Correct Answer:** C

---

Question 104. A healthcare organization needs to analyze patient device logs that arrive in varying formats and structures. How does schema-on-read help the architect deliver a solution in Snowflake that is both scalable and adaptive?

A) It requires all logs to be standardized before loading  
B) It allows storing and querying diverse log formats in a VARIANT column without prior schema definition  
C) It restricts loading to only structured tables  
D) It automatically transforms all logs into a fixed schema  
E) It blocks ingestion of files with unexpected fields  

**Correct Answer:** B

---

Question 105. A retail data provider shares five tables with a partner using Snowflake’s data sharing feature. The consumer account’s role has been granted the imported privileges privilege. What does this enable the consumer role to do?

A) Create new tables in the provider account  
B) Grant privileges on shared objects to other roles within the consumer account  
C) Modify the definition of shared tables  
D) Delete shared tables from the provider account  
E) Access provider’s account usage views  

**Correct Answer:** B

---

Question 106. After granting the imported privileges privilege to a role in the consumer account, which scenario demonstrates how this privilege is typically used in business workflows?

A) The consumer role can update records in the shared tables  
B) The consumer role can grant SELECT access on shared tables to additional users in their organization  
C) The consumer role can change the structure of the shared tables  
D) The consumer role can revoke access from the provider account  
E) The consumer role can load new data into the provider’s tables  

**Correct Answer:** B

---

Question 107. A financial institution, acting as a Snowflake data consumer, receives shared tables from a provider account and the role has imported privileges. Which limitation still applies to the consumer’s access to these shared tables?

A) The consumer cannot grant privileges to other roles  
B) The consumer cannot create views on shared tables  
C) The consumer cannot modify the data or schema in shared tables  
D) The consumer can delete shared tables from the provider account  
E) The consumer can move shared tables to another database  

**Correct Answer:** C

---

Question 108. A SaaS provider wants to offer its analytics platform to multiple clients using a single Snowflake account, while keeping each client’s data isolated and secure. Which architectural concept does this scenario illustrate?

A) Single-tenant architecture  
B) Multi-tenant architecture  
C) Hybrid deployment  
D) On-premises installation  
E) Peer-to-peer sharing  

**Correct Answer:** B

---

Question 109. In a multi-tenant Snowflake environment, what is a key consideration for an architect when designing data models?

A) Combining all tenants’ data in one table without distinction  
B) Ensuring data isolation and security between tenants  
C) Limiting the number of tenants to one per account  
D) Disabling resource monitors for all tenants  
E) Sharing credentials among all tenants  

**Correct Answer:** B

---

Question 110. A software company supports several customers on a shared Snowflake infrastructure. What is an advantage of using a multi-tenant setup for this business model?

A) Each customer must have a separate account  
B) Increased operational efficiency and cost savings through shared resources  
C) Customers can access each other’s data directly  
D) Query performance is always identical for all tenants  
E) No need to manage roles and privileges  

**Correct Answer:** B

---

Question 111. A global retail company is experiencing slow query performance on its large sales transaction table due to frequent range scans on the "sales_date" column. What clustering strategy should the architect recommend to optimize these queries?

A) Cluster the table on the "sales_date" column  
B) Use automatic clustering on unrelated columns  
C) Partition the table by region  
D) Increase the virtual warehouse size  
E) Create materialized views on the table  

**Correct Answer:** A

---

Question 112. An architect is tasked with improving query efficiency for a table with billions of rows. The business frequently filters by "customer_id" and "region". What is an effective clustering strategy?

A) Cluster the table by both "customer_id" and "region" columns  
B) Cluster the table by the primary key only  
C) Do not use clustering for large tables  
D) Create a separate table for each region  
E) Use the COPY INTO command to reload the table daily  

**Correct Answer:** A

---

Question 113. A logistics company notices that their queries on shipment data are becoming slower over time, even after increasing compute resources. The architect suspects poor clustering. Which step should be taken to maintain optimal clustering over time?

A) Enable automatic clustering on the table  
B) Rebuild the table from scratch every week  
C) Drop and recreate all indexes  
D) Disable clustering  
E) Archive old shipments to external storage  

**Correct Answer:** A

---

Question 114. A retail analytics team notices that queries on their large transactions table are slowing down. The architect recommends using SYSTEM$CLUSTERING_INFORMATION. What is the primary purpose of this function in Snowflake?

A) To analyze the clustering depth and effectiveness of a table  
B) To create materialized views for faster queries  
C) To encrypt the table data  
D) To monitor warehouse usage  
E) To archive table partitions  

**Correct Answer:** A

---

Question 115. An architect is troubleshooting query performance issues on a heavily clustered table and wants to measure clustering quality. Which result from SYSTEM$CLUSTERING_INFORMATION would indicate the need for reclustering?

A) High clustering depth and large average values for clustering metrics  
B) Low number of partitions  
C) Frequent auto-suspend events  
D) Large warehouse size  
E) High percentage of NULL values  

**Correct Answer:** A

---

Question 116. A logistics company is using SYSTEM$CLUSTERING_INFORMATION to monitor their shipment data table. What business value does this approach provide?

A) Enables the team to proactively maintain clustering, optimizing query performance  
B) Automatically creates new tables for each month  
C) Ensures data is encrypted in transit  
D) Notifies users of schema changes  
E) Schedules backups of the database  

**Correct Answer:** A

---

Question 117. A retail company stores sales data in AWS S3 and wants to automate ingestion into their Snowflake account hosted on GCP. Which solution can trigger the ingestion process directly from AWS when new files land?

A) Use AWS Lambda to invoke the Snowpipe REST endpoint  
B) Manually upload files using Snowflake’s web interface  
C) Schedule batch jobs in GCP Dataflow  
D) Export files to local disk before loading  
E) Use Google Cloud Functions without integration  

**Correct Answer:** A

---

Question 118. An Architect is designing a cross-cloud ingestion workflow from AWS S3 to Snowflake on GCP. What is the role of the Snowpipe REST endpoint in this architecture?

A) It provides a programmatic interface for triggering file ingestion into Snowflake  
B) It processes files locally on AWS before transfer  
C) It encrypts the files for transit  
D) It generates data masking policies  
E) It manages network routing between AWS and GCP  

**Correct Answer:** A

---

Question 119. A media company wants to automate file ingestion from AWS S3 to Snowflake on GCP whenever new files are uploaded. Which AWS-native component can detect new files and trigger the ingestion?

A) AWS Lambda function  
B) AWS EC2 instance  
C) AWS Glue crawler  
D) AWS Redshift stream  
E) AWS IAM role  

**Correct Answer:** A

---

Question 120. A financial services architect must ensure immediate ingestion of compliance reports from AWS S3 into Snowflake hosted on GCP. Which approach will provide near real-time automation?

A) Use AWS Lambda to call the Snowpipe REST endpoint when new files arrive  
B) Manually monitor the S3 bucket and run COPY INTO in Snowflake  
C) Schedule daily transfer jobs in Google Cloud Composer  
D) Email files to the Snowflake admin for loading  
E) Use a third-party ETL tool with weekly loads  

**Correct Answer:** A

---

Question 121. The company’s architect wants to minimize manual steps and automate file ingestion from AWS S3 to Snowflake on GCP. What is the most scalable solution for this cross-cloud integration?

A) Configure an AWS Lambda function to invoke the Snowpipe REST endpoint for each new file  
B) Require staff to manually upload files through the Snowflake UI  
C) Use Google Cloud Storage Transfer Service for all files  
D) Download files to a local server and push to Snowflake  
E) Schedule a monthly bulk load job in Snowflake  

**Correct Answer:** A

---

Question 122. An architect is evaluating options for ingesting files from AWS S3 into Snowflake on GCP. What is a key benefit of using the Snowpipe REST endpoint in this scenario?

A) Enables automated, event-driven ingestion from external cloud storage  
B) Restricts ingestion to only CSV file formats  
C) Requires manual polling of the S3 bucket  
D) Prevents ingestion from multiple cloud sources  
E) Transfers files only during off-peak hours  

**Correct Answer:** A

---

Question 123. A retail company wants to automate the ingestion of new sales data files from AWS S3 into Snowflake. Which AWS service could be used to detect new files and trigger the Snowpipe REST endpoint for ingestion?

A) AWS Lambda function  
B) Amazon EC2 instance  
C) Amazon Redshift  
D) AWS IAM role  
E) Amazon DynamoDB  

**Correct Answer:** A

---

Question 124. During a cloud architecture review, an architect recommends using Lambda functions to process and route data between different cloud services. What is a key advantage of using Lambda functions in this scenario?

A) Lambda functions enable serverless, event-driven execution without manual server management  
B) Lambda functions require dedicated servers for each event  
C) Lambda functions only support batch processing  
D) Lambda functions must be scheduled manually  
E) Lambda functions encrypt data at rest by default  

**Correct Answer:** A

---

Question 125. A logistics company wants to perform real-time transformations on streaming data before loading it into Snowflake. How can Lambda functions assist in this business workflow?

A) By executing custom code automatically in response to data events  
B) By creating materialized views in Snowflake  
C) By storing raw data in Amazon Glacier  
D) By disabling event notifications  
E) By monitoring warehouse usage in Snowflake  

**Correct Answer:** A

---

Question 126. A data engineer needs to ensure that a Snowpark stored procedure has access to maximum memory and compute resources for a complex data transformation. Which SQL command should the architect use on the `snowpark_opt_wh` warehouse?

A) `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';`  
B) `ALTER PROCEDURE my_proc SET MEMORY = 'MAX';`  
C) `ALTER SCHEMA SET COMPUTE_OPTIMIZED = TRUE;`  
D) `ALTER WAREHOUSE snowpark_opt_wh SUSPEND;`  
E) `ALTER USER SET RESOURCE_MONITOR = 'HIGH';`  

**Correct Answer:** A

---

Question 127. What is Snowpark in the context of Snowflake?

A) A developer framework for building data pipelines and applications using familiar programming languages directly in Snowflake  
B) A built-in tool for resizing warehouses  
C) A visualization dashboard for BI users  
D) A storage management feature for external stages  
E) A Snowflake billing optimization service  

**Correct Answer:** A

---

Question 128. A financial services company wants to run complex machine learning scoring logic directly inside Snowflake. What Snowflake feature should the architect recommend for this requirement?

A) Snowpark, to run custom logic and ML models within Snowflake using Java, Scala, or Python  
B) Secure Data Sharing  
C) Materialized Views  
D) Resource Monitors  
E) File Format objects  

**Correct Answer:** A

---

Question 129. An architect wants to optimize the performance of Snowpark stored procedures that run on the `snowpark_opt_wh` warehouse. Which configuration will best support high concurrency and intensive workloads?

A) Increase both the warehouse size using `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';` and set `MAX_CONCURRENCY_LEVEL` appropriately  
B) Set `MAX_CONCURRENCY_LEVEL = 1` to maximize memory for one query  
C) Use the smallest warehouse size for cost savings  
D) Suspend the warehouse during peak times  
E) Disable automatic clustering  

**Correct Answer:** A

---

Question 130. A retail company uses Snowpark for ETL transformations. What is one major benefit of using Snowpark over traditional SQL-based pipelines?

A) Snowpark allows developers to use familiar languages (Python, Java, Scala) and apply advanced logic not easily expressed in SQL  
B) Snowpark only supports visualization tasks  
C) Snowpark disables time travel for all tables  
D) Snowpark must be run outside Snowflake  
E) Snowpark restricts access to structured data only  

**Correct Answer:** A

---

Question 131. An architect needs to run a resource-intensive Snowpark workload. Which strategy will help ensure the warehouse provides maximum compute and memory resources during execution?

A) Use `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';` before running the workload  
B) Set the warehouse to auto-suspend frequently  
C) Use a very small warehouse to limit resource consumption  
D) Run the workload outside Snowflake  
E) Limit the number of concurrent queries to zero  

**Correct Answer:** A

---

Question 132. An architect notices that queries against an external table referencing large Parquet files on cloud storage are performing slowly. What is one step that can improve query performance?

A) Partition the data files in cloud storage based on common query filters  
B) Increase the size of the virtual warehouse  
C) Run ANALYZE TABLE to gather statistics  
D) Add clustering keys to the external table  
E) Disable automatic file discovery  

**Correct Answer:** A

---

Question 133. A logistics company frequently runs queries that filter by shipment date on their external table referencing cloud storage. Which action would result in faster query responses?

A) Organize the external data files by shipment date folders  
B) Load all data into an internal Snowflake table  
C) Enable time travel for the external table  
D) Use masking policies on date columns  
E) Increase the concurrency level of the warehouse  

**Correct Answer:** A

---

Question 134. An architect wants to minimize scan costs and reduce latency when querying a large external table on S3. What is the best approach?

A) Partition the data files in S3 based on frequently queried columns  
B) Use the smallest possible warehouse size  
C) Create materialized views on the external table  
D) Store all files in a single large folder  
E) Disable external stage caching  

**Correct Answer:** A

---

Question 135. A financial services firm experiences slow queries against an external table referencing JSON files. Which Snowflake feature can help improve performance for point lookup queries?

A) Enable the Search Optimization Service on the external table  
B) Convert all files to CSV format  
C) Set the retention period to zero  
D) Use temporary tables for the data  
E) Grant imported privileges to the consumer role  

**Correct Answer:** A

---

Question 136. A media company wants to optimize query speed on an external table referencing video metadata. What design principle should be applied to the source files?

A) Partition the files in cloud storage according to metadata attributes commonly used for filtering  
B) Compress all files into a single archive  
C) Use only unstructured blobs for storage  
D) Disable schema-on-read  
E) Store all files in random folders  

**Correct Answer:** A

---

Question 137. A retail company is running aggregate queries on an external table and notices delays. What step can the architect take to ensure better query performance?

A) Organize external files by keys often used in query predicates (e.g., region, date)  
B) Use the default file organization with no partitioning  
C) Increase the retention period on the external stage  
D) Turn off file format validation  
E) Use only VARIANT columns in the table definition  

**Correct Answer:** A

---

Question 138. A financial services company wants to ensure only authorized analysts can access sensitive tables in Snowflake. What RBAC feature should the architect leverage to enforce this requirement?

A) Assign SELECT privileges to analysts using roles  
B) Enable time travel for sensitive tables  
C) Set file format restrictions  
D) Use clustering keys for access control  
E) Partition data by department  

**Correct Answer:** A

---

Question 139. During a security audit, an architect is asked how Snowflake RBAC can prevent accidental data changes by junior staff. What is a characteristic of RBAC that supports this control?

A) Roles can restrict access and actions at the object level  
B) RBAC automatically encrypts all data  
C) RBAC disables data sharing features  
D) RBAC forces multi-factor authentication for all users  
E) Roles determine query performance  

**Correct Answer:** A

---

Question 140. A global retailer wants to simplify privilege management across multiple teams in Snowflake. Which RBAC characteristic should the architect use to manage access efficiently?

A) Privileges are granted to roles, not directly to users  
B) All privileges must be granted directly to each user  
C) Every user must have the same role  
D) Roles are only available for administrators  
E) RBAC requires external identity providers  

**Correct Answer:** A

---

Question 141. An architect is designing a multi-department data warehouse in Snowflake. How does RBAC help maintain data security between departments?

A) By assigning department-specific roles and granting privileges only on relevant objects  
B) By requiring all departments to use the same tables  
C) By disabling time travel for all departments  
D) By forcing all users into a single role  
E) By encrypting data at the column level  

**Correct Answer:** A

---

Question 142. A healthcare company wants to allow only doctors to view patient records, while administrators can manage but not view the data. Which RBAC principle enables this separation of duties?

A) Roles can be tailored so that different users have different access and capabilities  
B) Roles must be assigned randomly  
C) RBAC disables external sharing  
D) Privileges must be granted to everyone  
E) RBAC requires role hierarchy to match org chart exactly  

**Correct Answer:** A

---

Question 143. A media organization is onboarding new staff with different responsibilities. How does RBAC in Snowflake simplify provisioning access?

A) Users inherit privileges from assigned roles, so onboarding only requires assigning the correct role  
B) Each privilege must be set for every user individually  
C) New users cannot be added to Snowflake  
D) RBAC only works for database objects  
E) Role assignment must be performed outside of Snowflake  

**Correct Answer:** A

---

Question 144. A financial services architect wants to ensure only schema owners can grant privileges on objects within a specific schema, even as new tables and views are added. What Snowflake feature should be used to enforce this requirement?

A) Managed access schema  
B) Time travel  
C) External stage  
D) Secure view  
E) Data masking policy  

**Correct Answer:** A

---

Question 145. In a multi-team Snowflake environment, how can an architect support future privilege grants on newly created tables so only the schema owner can grant access to other roles?

A) By creating a managed access schema  
B) By enabling automatic clustering  
C) By setting up a resource monitor  
D) By using external tables  
E) By disabling all grants  

**Correct Answer:** A

---

Question 146. A retail company wants to prevent users from directly granting privileges on objects inside a schema to other roles, unless they are the schema owner. Which approach achieves this goal in Snowflake?

A) Use managed access schema for the relevant database objects  
B) Assign imported privileges to all user roles  
C) Store all objects in external stages  
D) Use only transient tables  
E) Grant privileges to the public role  

**Correct Answer:** A

---

Question 147. In Snowflake, what is a managed access schema?

A) A schema where only the schema owner (or roles with the MANAGE GRANTS privilege) can grant privileges on objects within the schema  
B) A schema that automatically encrypts all data  
C) A schema that allows any user to grant privileges on its objects  
D) A schema used only for temporary tables  
E) A schema that enforces clustering on all tables  

**Correct Answer:** A

---

Question 148. A retail company wants to ensure only schema owners can grant access to tables and views in a specific schema, even after new objects are created. Which Snowflake feature supports this requirement?

A) Managed access schema  
B) Secure data sharing  
C) External stage  
D) Materialized views  
E) Data masking policy  

**Correct Answer:** A

---

Question 149. What is a key benefit of using a managed access schema in Snowflake for a multi-team project?

A) Centralized privilege management for all objects in the schema  
B) Automatic creation of tables and views  
C) Schema objects are automatically replicated to other accounts  
D) All users can grant access to any object  
E) Only structured data formats are supported  

**Correct Answer:** A

---

Question 150. A development team plans to use database cloning to create isolated test environments quickly. What is a key consideration to address before cloning the production database?

A) Sensitive data in the source database may need to be masked or obfuscated before cloning  
B) Cloning automatically encrypts all data  
C) Cloning disables object creation in the clone  
D) Cloning increases the retention period of all tables  
E) Cloning removes all user roles from the clone  

**Correct Answer:** A

---

Question 151. An architect uses database cloning to enable parallel development efforts. What limitation should be communicated to developers regarding cloned databases?

A) Changes made to the clone do not affect the source database  
B) All changes in the clone are automatically reflected in the source  
C) Cloned databases cannot be queried  
D) Clones are only available for 24 hours  
E) Cloning deletes all stages and file formats  

**Correct Answer:** A

---

Question 152. A software company wants to use database cloning for rapid prototyping in their dev environment. Which storage consideration is important when managing cloned databases?

A) Cloned databases initially consume little additional storage, but changes made to the clone will increase storage usage  
B) Cloned databases always double the original storage instantly  
C) Storage usage for clones is unrelated to changes made in the clone  
D) Cloning compresses all data to minimize costs  
E) Clones require manual data backups to preserve state  

**Correct Answer:** A

---

Question 153. A project manager wants to ensure compliance requirements are met when using database clones for testing. What should the architect advise?

A) Review and update access controls and masking policies on clones to match compliance requirements  
B) Compliance is automatically inherited from the source database  
C) Clones cannot be assigned any roles  
D) Cloned databases do not support masking policies  
E) Compliance reviews are only necessary for production databases  

**Correct Answer:** A

---

Question 154. An architect is using database cloning to create a development environment from production. What happens to pipes that reference internal stages in the source database?

A) Pipes referring to internal stages are not cloned and must be recreated in the target environment  
B) Pipes are automatically redirected to external stages  
C) All pipes are cloned regardless of stage type  
D) Pipes are converted to tasks during cloning  
E) Pipes are duplicated and enabled in both source and clone  

**Correct Answer:** A

---

Question 155. A retail company clones a database for testing but discovers some automated data ingestion is missing. What is the likely reason if their ingestion uses pipes?

A) Pipes that reference internal stages are not cloned and need to be manually recreated  
B) All pipes are disabled after cloning  
C) Pipes referencing external stages are deleted  
D) Cloning automatically migrates all ingestion logic  
E) Pipes are converted to scheduled jobs in the clone  

**Correct Answer:** A

---

Question 156. During a database cloning process, an architect must ensure data pipelines continue working in the cloned environment. What must be checked and possibly recreated after cloning?

A) Any pipes in the source that refer to internal stages  
B) All masking policies  
C) External tables and file formats  
D) All virtual warehouses  
E) User roles and privileges  

**Correct Answer:** A

---

Question 157. A development team clones a schema to create a test environment in Snowflake. What happens to the privileges on tables and views inside the cloned schema?

A) The clone inherits all granted privileges of tables and views from the source schema  
B) All privileges are removed in the clone  
C) Only SELECT privileges are inherited  
D) Privileges must be manually reassigned to every object in the clone  
E) The clone can only inherit privileges from the database, not child objects  

**Correct Answer:** A

---

Question 158. After cloning a schema, a user notices that the database-level privileges are not present in the clone. What explains this behavior?

A) Database-level privileges are not inherited by the clone; only child object privileges are inherited  
B) All privileges from source are always inherited, including database privileges  
C) The clone automatically receives all privileges from every parent object  
D) Database privileges are converted to schema privileges in the clone  
E) Privileges are never inherited in any cloning operation  

**Correct Answer:** A

---

Question 159. An architect clones a schema for a parallel development stream. What must they consider regarding access control in the cloned schema?

A) Privileges on child objects (tables, views, etc.) are inherited, but any database-level privileges must be granted separately  
B) All privileges must be manually granted on every child object  
C) No privileges are inherited during cloning  
D) The clone receives privileges only on temporary tables  
E) All privileges are inherited, including future grants  

**Correct Answer:** A

---

Question 160. A global financial institution wants the highest level of availability for its mission-critical Snowflake workloads. Which architecture provides the best solution for minimizing disruption during a service event?

A) Deploy Snowflake across multiple cloud regions with automatic failover  
B) Run all workloads in a single region  
C) Use manual backups and restore processes  
D) Limit access to only one virtual warehouse  
E) Use transient tables for all data  

**Correct Answer:** A

---

Question 161. An e-commerce company is designing a disaster recovery strategy for its Snowflake environment. What feature should the architect use to ensure the fastest recovery time and least disruption?

A) Cross-region replication with failover capabilities  
B) Rely solely on time travel for data recovery  
C) Schedule weekly data exports to local servers  
D) Use only the default cloud region  
E) Suspend all warehouses during business hours  

**Correct Answer:** A

---

Question 162. A healthcare provider needs to maximize redundancy for its Snowflake environment, ensuring application processes remain available during regional outages. Which approach best supports this requirement?

A) Configure Snowflake accounts with business continuity enabled across multiple regions  
B) Store all historical data in on-premises servers  
C) Rely exclusively on daily manual backups  
D) Use a single small warehouse for all workloads  
E) Implement row-level security only  

**Correct Answer:** A

---

Question 163. During a Snowflake architecture review, the CTO asks how to guarantee the highest uptime for critical applications, regardless of cost. What is the best solution?

A) Multi-region deployment with automatic failover and replication  
B) Single-region deployment with increased warehouse size  
C) Manual monitoring and intervention for service events  
D) Take periodic snapshots to cloud storage  
E) Use transient tables to speed up recovery  

**Correct Answer:** A

---

Question 164. A financial services company shares sensitive data using secure views in a Snowflake data share. What is the most effective method for the architect to validate that only authorized records are visible to consumers?

A) Log in as a consumer user and query the secure view directly  
B) Review the Snowflake billing history  
C) Run a DESCRIBE TABLE command on the secure view  
D) Check the record count in the provider account  
E) Email consumers to confirm data visibility  

**Correct Answer:** A

---

Question 165. A media company wants to ensure that secure views shared via a data share only expose specific records to consumers. Which approach should the architect use to confirm correct data exposure?

A) Create a test consumer account and perform queries on the shared secure views  
B) Enable automatic clustering on the data share  
C) Modify the secure view definition in the consumer account  
D) Compare query results from materialized views  
E) Review the underlying table privileges  

**Correct Answer:** A

---

Question 166. An architect suspects that a secure view in a data share may be exposing more data than intended. What is a recommended validation step before granting access to production consumers?

A) Simulate consumer queries in a sandbox or test consumer environment  
B) Use the SHOW SHARES command to list all shares  
C) Rely on the default view settings without checks  
D) Grant access to all users and monitor usage  
E) Disable secure views in the share  

**Correct Answer:** A

---

Question 167. A retail company needs to verify that their shared secure views enforce row-level security for each consumer of a Snowflake data share. What is the best validation practice?

A) Query the shared secure views from a consumer account with appropriate role and confirm row-level security  
B) Inspect only the provider account’s access controls  
C) Only check the definition of the secure view  
D) Run metadata queries on the provider database  
E) Use external tools to review view logic  

**Correct Answer:** A

Question 168. A company needs to confirm that only appropriate records are exposed in a data share for each consumer. How can the provider account efficiently validate secure view results for different consumers?

A) By setting SIMULATED_DATA_SHARING_CONSUMER to the consumer’s account identifier in the provider session and querying the secure view  
B) By granting SELECT privileges to every user in the provider account  
C) By exporting the data share and importing into a sandbox  
D) By disabling all secure views in the share  
E) By using masking policies on internal tables only  

**Correct Answer:** A

---

Question 169. An architect wants to review the privileges of a newly created user, user_01, in Snowflake. What does the command `SHOW GRANTS TO USER user_01;` display?

A) All privileges that have been granted to user_01, including roles assigned  
B) All privileges granted by user_01 to other users  
C) Only table-level grants for user_01  
D) Privileges on objects owned by user_01  
E) All database objects created by user_01  

**Correct Answer:** A

---

Question 170. In a troubleshooting session, a Snowflake architect runs the command `SHOW GRANTS ON USER user_01;`. What information does this command provide?

A) All privileges granted on the user object user_01 (e.g., who can manage this user)  
B) A list of all roles assigned to user_01  
C) Warehouse usage statistics for user_01  
D) Secure view access for user_01  
E) Masking policies applied to user_01  

**Correct Answer:** A

---

Question 171. A company manager asks the architect about the difference between `SHOW GRANTS TO USER user_01;` and `SHOW GRANTS ON USER user_01;`. Which statement is correct?

A) The first command lists privileges and roles assigned to user_01; the second lists who has privileges to manage the user_01 object itself  
B) Both commands list the same information  
C) The first command shows login history, and the second shows grants  
D) The first command only works for warehouse objects  
E) The second command lists all databases user_01 can access  

**Correct Answer:** A

---

Question 172. The business team wants to visualize daily data in Tableau, and old data can be discarded. Which Snowflake table type is most appropriate for this use case?

A) Transient table, since it does not retain historical data and is cost-effective for temporary storage  
B) Permanent table, as it retains all historical data  
C) External table, to keep old data accessible  
D) Materialized view, for fast query performance on all historical data  
E) Secure view, to restrict access to old data  

**Correct Answer:** A

---

Question 173. As a Snowflake architect, which strategy best supports the business requirement of discarding yesterday’s data when loading new data for Tableau dashboards?

A) Drop or truncate the table before loading new data each day  
B) Store every day’s data in a new table and keep them indefinitely  
C) Archive old data to an external stage  
D) Use time travel to retain old versions  
E) Enable automatic clustering for the table  

**Correct Answer:** A

---

Question 174. The business team requests assurance that only the latest data is available for Tableau reporting. What Snowflake feature or process should be implemented to meet this requirement?

A) Schedule a daily ETL job that replaces the table contents with fresh data  
B) Partition the table by date and retain all partitions  
C) Set up continuous data loading and never delete old data  
D) Use external tables to access old and new data  
E) Apply row-level security to hide old data  

**Correct Answer:** A

---

Question 175. An architect is tasked with integrating Snowflake with an external application. The team suggests using a REST API. What is a REST API?

A) An interface that allows systems to communicate over HTTP using standardized methods like GET, POST, PUT, and DELETE  
B) A tool for visualizing data in dashboards  
C) A protocol for real-time streaming of video data  
D) An encryption algorithm for securing network traffic  
E) A type of database management system  

**Correct Answer:** A

---

Question 176. A retail company wants to automate the loading of daily sales data into Snowflake from their web application. Which feature of REST APIs makes them suitable for this task?

A) They allow programmatic data exchange between applications over the web  
B) They require manual data entry  
C) They only work for internal network communication  
D) They do not support authentication  
E) They can only be used for file storage  

**Correct Answer:** A

---

Question 177. A Snowflake architect needs to expose business logic to a third-party analytics platform. Which solution would be most appropriate, and why?

A) Build a REST API so the analytics platform can send requests and receive responses over HTTP  
B) Use a spreadsheet and email it manually  
C) Store the logic in a local file on a server  
D) Require the third party to install Snowflake locally  
E) Use SMS messages for communication  

**Correct Answer:** A

---

Question 178. The business requires analysts to load their own data but not share it with other users. What privilege management strategy supports this in Snowflake?

A) Grant object creation privileges to ANALYST_ROLE in managed access schemas, but manage all grants through SYSADMIN  
B) Allow analysts to create and grant privileges on any object  
C) Give all users in the organization access to the analysts’ database  
D) Use external stages for all analyst data loads  
E) Enable time travel for all tables in the database  

**Correct Answer:** A

---

Question 179. A multinational company uses Snowflake on Azure in the Netherlands and receives frequently updated sales data as JSON files in an Amazon S3 bucket located in the AWS Singapore region. The analytics team needs to access this data for daily reporting. The architect is tasked to ensure access to up-to-date data while keeping egress costs low and maintaining low latency. Which approach best meets all the requirements with the least operational overhead?

A) Create a Snowflake external table pointing directly to the S3 bucket and query the data live for each analysis  
B) Schedule regular jobs to replicate data from the S3 bucket to Azure Blob Storage and analyze the replicated data using Snowflake external tables in Azure  
C) Use Snowflake’s materialized views on top of the external table pointing to the S3 bucket to cache the data locally  
D) Move the JSON files manually from S3 to Snowflake stage storage before each analysis  

**Correct Answer:** B

---

Question 180. An architect must design a data ingestion pipeline for a company using Snowflake on Azure in the Netherlands. The team needs frequent access to changing data stored in an AWS S3 bucket in Singapore, and wants to minimize both latency and egress costs with minimal maintenance. Which solution best fulfills these requirements?

A) Query the S3 bucket directly from Snowflake in Azure for every analytics task  
B) Use a transient Snowflake table and periodically load data from the external S3 table  
C) Use Snowflake’s Secure Data Sharing to share data from an AWS-based Snowflake account to Azure  
D) Set up a real-time streaming pipeline to continuously move data from S3 in Singapore to Azure Blob Storage in the Netherlands  

**Correct Answer:** B

---

Question 181. A company’s business analyst team wants to analyze product usage data stored in JSON files in an Amazon S3 bucket in Singapore from their Snowflake environment on Azure Netherlands. The architect is asked to provide a solution that gives timely access to new data, keeps egress costs low, and is simple to operate. Which option should the architect recommend?

A) Set up a scheduled job to replicate only the frequently changing JSON files from S3 Singapore to Azure Blob Storage Netherlands  
B) Use Snowflake’s cross-cloud data sharing to access S3 data without replication  
C) Query the S3 bucket directly via Snowflake’s external table for each analysis  
D) Use a third-party ETL tool to extract, transform, and load the data from S3 to Snowflake every hour  

**Correct Answer:** A

---

Question 182. A global retail company uses Snowflake in Azure Netherlands. Their analytics team requires frequent access to JSON data stored in Amazon S3 in Singapore for near real-time dashboards. Which solution best balances low egress cost, low query latency, and minimal operational overhead?

A) Query the S3 bucket directly from Snowflake in Azure for every dashboard refresh  
B) Use Snowflake materialized views on an external table pointing to the S3 bucket  
C) Periodically copy frequently changing data from S3 Singapore to Azure Blob Storage Netherlands and analyze it via Snowflake external tables  
D) Move the JSON files manually into Snowflake's internal stage before each analysis  
E) Use Snowflake's result cache on queries against the S3 external table  

**Correct Answer:** C

---

Question 183. The architect for a logistics firm wants to optimize query performance on a very large table in Snowflake. Which of the following optimization techniques will incur additional **storage costs** on the account?

A) Using result caching for queries  
B) Defining and maintaining clustering keys on the table  
C) Creating views (not materialized) on top of the table  
D) Increasing compute resources for the virtual warehouse  
E) Using query acceleration service  

**Correct Answer:** B

---

Question 184. A company is using Snowflake in Azure Netherlands and considers enabling the **Search Optimization Service** on some large tables with frequent point lookup queries. What is a key implication of this decision?

A) Queries will always run with zero latency  
B) The service will incur additional storage costs due to persistent search data structures  
C) The service is only available in AWS regions  
D) The service eliminates the need for clustering keys  
E) There is no cost for using the Search Optimization Service  

**Correct Answer:** B

---

Question 185. A media firm uses Snowflake in Azure Netherlands to analyze streaming data stored in JSON format in AWS S3 Singapore. They want to **minimize egress costs** and **maintain low latency** for business-critical dashboards. Which approach is most cost-effective?

A) Use Snowflake's automatic clustering to organize the S3 external table  
B) Create a materialized view on the external table directly against S3  
C) Copy data from S3 to a transient table in Snowflake, and query the transient table  
D) Query S3 directly for each dashboard refresh  
E) Use the Search Optimization Service on the external table  

**Correct Answer:** C

---

Question 186. Which of the following Snowflake features **does NOT directly incur storage costs** when enabled or used?

A) Time Travel  
B) Search Optimization Service  
C) Materialized Views  
D) Result Caching  
E) Automatic Clustering  

**Correct Answer:** D

---

Question 187. A logistics company stores sensor data in an AWS S3 bucket, and new files arrive every minute. They want their Snowflake environment to automatically ingest these files for near real-time analytics with minimal manual intervention. Which Snowflake feature should the architect recommend to enable seamless auto-ingestion triggered by AWS cloud messaging services?

A) Bulk COPY INTO command  
B) Streams and Tasks  
C) Snowpipe  
D) Materialized Views  

**Correct Answer:** C

---

Question 188. A financial institution needs to process transaction logs in near real-time as soon as they are uploaded to Azure Blob Storage. Which Snowflake ingestion method can use Azure Event Grid to automatically trigger loading of new files into Snowflake?

A) Snowpipe  
B) Manual file upload to internal stage  
C) Data Exchange  
D) External tables with a scheduled refresh  

**Correct Answer:** A

---

Question 189. A company wants to minimize latency for data analytics by having new files in Google Cloud Storage automatically ingested into Snowflake as soon as they arrive. Which combination of Snowflake feature and cloud provider service should the architect choose?

A) Snowpipe with Google Pub/Sub  
B) Bulk COPY INTO with Google Cloud Functions  
C) Streams and Tasks with Google Sheets  
D) Materialized Views with Google Dataflow  

**Correct Answer:** A

---

Question 190. A retail company stores customer interaction logs as JSON in a VARIANT column within Snowflake. Initially, queries filtering by specific fields in the JSON performed well, but recently, the analytics team reports significant slowdowns in their dashboards. As the Snowflake architect, which factor is most likely contributing to the poor query performance?

A) The VARIANT column was indexed incorrectly  
B) The number of micro-partitions has decreased  
C) The VARIANT column’s structure has become more complex and less uniform over time  
D) Query caching has been disabled  

**Correct Answer:** C

---

Question 191. An e-commerce business uses a Snowflake table with a VARIANT column to store product metadata in JSON format. After a recent data migration, users notice that queries retrieving specific attributes from the JSON are much slower than before. Which architectural solution would BEST address this performance issue?

A) Increase the compute warehouse size  
B) Extract frequently queried JSON attributes into dedicated columns and use clustering keys  
C) Enable Time Travel for the table  
D) Compress the VARIANT column using a stronger algorithm  

**Correct Answer:** B

---

Question 192. A financial services firm has a reporting dashboard that runs complex queries on a Snowflake table where transaction events are stored as JSON in a VARIANT column. Performance was acceptable, but after several weeks, reports became sluggish. What is a likely root cause an architect should investigate first?

A) The warehouse has reached its maximum storage quota  
B) The VARIANT column now contains highly heterogeneous JSON documents  
C) The data retention period was reduced  
D) The table was converted from transient to permanent  

**Correct Answer:** B

---

Question 193. An online travel agency stores booking details as JSON in a VARIANT column. After a recent increase in business, analysts notice that queries to retrieve specific booking information are much slower. As the Snowflake architect, which scenario is most likely causing the slowdown?

A) The VARIANT column contains increasingly diverse JSON structures  
B) The Snowflake account’s credit balance is low  
C) The queries are executed during Snowflake maintenance windows  
D) The network bandwidth between Snowflake and external sources has decreased  

**Correct Answer:** A

---

Question 194. A healthcare provider stores patient records as JSON documents in a VARIANT column. Recently, reporting queries are taking much longer to run. Which action should the architect take to improve query performance?

A) Increase the retention period on the table  
B) Create dedicated columns for frequently accessed attributes from the JSON  
C) Grant more privileges to analysts running the queries  
D) Set the table to transient to reduce storage costs  

**Correct Answer:** B

---

Question 195. A logistics company uses a VARIANT column to store shipment event data as JSON. Performance of analytics queries has dropped significantly in the past month. Which underlying issue should the Snowflake architect investigate first?

A) JSON documents in the VARIANT column have become larger and more nested  
B) The account has enabled data masking policies  
C) The compute warehouse uses a different region  
D) The table was cloned to a different database  

**Correct Answer:** A

---

Question 196. A financial institution is planning to segregate workloads in Snowflake. Some data requires strict regulatory compliance (PCI DSS), while other business units do not require this level of security. They need to share select datasets between these environments. What is the MOST cost-effective architectural approach?

A) Store all data in a single PCI DSS-compliant account  
B) Use separate Snowflake accounts for PCI DSS and non-PCI DSS workloads, sharing data securely where needed  
C) Store all data in the same database with row-level policies  
D) Store PCI DSS and non-PCI DSS data in separate schemas within one account  

**Correct Answer:** B

---

Question 197. A retail company wishes to minimize Snowflake costs while maintaining compliance. They have marketing data not subject to compliance and transactional data requiring PCI DSS certification. Which design strategy is MOST cost-effective?

A) Provision all data in a PCI DSS-certified account  
B) Use multiple Snowflake accounts, assigning PCI DSS only where needed  
C) Store all data in encrypted tables  
D) Separate data by schema within a single account  

**Correct Answer:** B

---

Question 198. A healthcare provider needs to share patient data (subject to HIPAA) and research data (not subject to compliance) across departments. The goal is to optimize cost and compliance in Snowflake. Which approach should the architect recommend?

A) Use one Snowflake account for all data, enabling highest compliance settings for the entire account  
B) Implement multiple-account strategy, applying compliance only to the account holding patient data and sharing with non-compliant accounts as needed  
C) Store all data in a single account using table masking  
D) Use a single account with distinct databases for each data type  

**Correct Answer:** B

---

Question 199. A media company uses Snowpipe to ingest streaming data into Snowflake. The architecture team wants to maintain a detailed log of every data load for audit purposes. Which approach using the Snowpipe REST API BEST fulfills this requirement?

A) Use the API to trigger data loads and rely on Snowflake’s native INFORMATION_SCHEMA views  
B) Configure the REST API to post load history events to a custom external logging service after each successful API call  
C) Use the REST API’s response payloads to capture load history metadata and write it to an internal logging table  
D) Query the data files directly after each load to reconstruct the load history  

**Correct Answer:** C

---

Question 200. A logistics company wishes to track failed and successful Snowpipe loads for compliance reporting. As the Snowflake architect, which method leverages the Snowpipe REST API to create a reliable load history log?

A) Periodically query the REST API’s load history endpoint and store the results in a separate database table  
B) Enable file-level auditing on the external stage used by Snowpipe  
C) Use the REST API to retrieve load events and then push relevant details (timestamps, status, file names) to an enterprise logging system  
D) Parse the Cloud Storage access logs for Snowpipe activity  

**Correct Answer:** C

---

Question 201. An insurance company must maintain a history of all files loaded via Snowpipe for regulatory review. What is the MOST effective way for the architect to use the Snowpipe REST API to keep an accurate log?

A) Configure the REST API to write directly to a Snowflake table after each load  
B) Extract load history from the REST API’s response whenever a load is triggered and aggregate the data in a log table  
C) Query the data warehouse for recently loaded files  
D) Rely on stage metadata in the cloud storage provider  

**Correct Answer:** B

---

Question 202. Your company ingests large volumes of data daily into Snowflake using Snowpipe. The compliance team requests a detailed log of all data loads, including file names, load times, and statuses, for auditing purposes. As a Snowflake architect, how should you use the Snowpipe REST API to meet this requirement?

A) Schedule a daily query on the Snowflake metadata tables and export results to a CSV file  
B) Use the Snowpipe REST API’s insertReport function to capture and store load history details in a dedicated logging table  
C) Enable Snowflake’s automatic load notification emails for each ingestion  
D) Configure a cloud storage lifecycle rule to archive ingested files  

**Correct Answer:** B

---

Question 203. A financial client wants to monitor failed file loads for their Snowpipe pipelines in real time and maintain a history for troubleshooting. Which approach best leverages Snowpipe REST API features to achieve this?

A) Use insertReport to record load events and statuses in a custom audit table  
B) Query the INFORMATION_SCHEMA.LOAD_HISTORY table every hour  
C) Subscribe to Snowflake’s system alert emails for each failed load  
D) Store all ingested files in a separate folder for manual review  

**Correct Answer:** A

---

Question 204. During a data migration project, your team must track which files were loaded by Snowpipe and when, and ensure this log is available for business users to query. What is an effective solution using Snowpipe REST API?

A) Configure Snowpipe to send notifications to a Slack channel  
B) Utilize the insertReport function to push load event details into a reporting database  
C) Manually record load events in an Excel spreadsheet  
D) Rely on the cloud provider’s storage logs for file access history  

**Correct Answer:** B

---

Question 205. An organization needs to maintain an immutable record of all Snowpipe data loads for regulatory reporting. Which Snowpipe REST API capability is most appropriate for creating such a log?

A) insertReport  
B) updateLog  
C) deleteReport  
D) queryLoadHistory  

**Correct Answer:** A

---

Question 206. A retail company wants to analyze patterns in their nightly data ingestion runs using Snowpipe, such as peak load times and error frequencies. What would be the best practice using Snowpipe REST API?

A) Build a dashboard using data collected by insertReport on load events  
B) Review the Snowpipe configuration files manually  
C) Use the REST API to trigger Snowpipe loads, but not log any history  
D) Query the file system for timestamps on uploaded files  

**Correct Answer:** A

---

Question 207. As part of a disaster recovery plan, your team must be able to reconstruct historical data loads in Snowflake in case of system failure. Which Snowpipe REST API feature can support this requirement?

A) insertReport can provide detailed logs of all past data loads for reconstruction  
B) The REST API can automatically reload all previously ingested files  
C) Snowpipe REST API’s deleteReport function archives old load logs  
D) System tables in Snowflake are updated automatically with every data load  

**Correct Answer:** A

---

Question 208. Your organization’s client application supports several authentication methods, including Okta, username/password, and key pair authentication. As the Snowflake architect, which authentication method should you recommend as the top priority for connecting to Snowflake, according to best practice?

A) Username and password  
B) Okta SSO  
C) OAuth token  
D) Key pair authentication  

**Correct Answer:** B

---

Question 209. A company wants to ensure secure, seamless user access to Snowflake for their client application, which supports Okta, basic authentication, and external OAuth. What is the recommended order of priority for authentication methods to maximize both user experience and security?

A) Basic authentication → Okta → External OAuth  
B) Okta → External OAuth → Basic authentication  
C) External OAuth → Basic authentication → Okta  
D) Key pair authentication → Okta → Basic authentication  

**Correct Answer:** B

---

Question 210. In a scenario where your client application integrates with Okta and also allows direct username/password authentication, what is the best practice for authentication priority when connecting to Snowflake?

A) Always use username/password for simplicity  
B) Use Okta as the primary method and fall back to username/password only if necessary  
C) Alternate between methods on each connection attempt  
D) Use OAuth as the primary method  

**Correct Answer:** B

---

Question 211. A retail company uses Snowflake tasks for daily ETL processing. The data engineering manager asks you, as the Snowflake architect, to provide a report detailing the last week’s task runs, including statuses and error messages. Which query should you use to retrieve this information?

A) SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY()) WHERE scheduled_time >= DATEADD('day', -7, CURRENT_DATE);  
B) SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE table_name = 'TASK_HISTORY';  
C) SELECT * FROM INFORMATION_SCHEMA.TASKS WHERE status = 'FAILED';  
D) SELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY()) WHERE query_type = 'TASK';  

**Correct Answer:** A

---

Question 212. A Snowflake architect needs to analyze the execution history of a specific task to troubleshoot intermittent failures reported by the operations team. What is the most effective way to obtain detailed execution logs for that task?

A) Use SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY()) WHERE NAME = '<task_name>';  
B) Check the cloud provider’s storage logs for access patterns  
C) Query INFORMATION_SCHEMA.TASKS for the last status update  
D) Use SELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY()) WHERE query_text LIKE '%TASK%';  

**Correct Answer:** A

---

Question 213. A Snowflake architect at ACCOUNTA needs to share the MARKET_DB database with a business partner whose Snowflake account, PARTNERB, is hosted in Azure East US 2. What is the first step required to enable cross-cloud, cross-region data sharing?

A) Create a database clone in PARTNERB’s account  
B) Set up a Snowflake Reader Account in PARTNERB  
C) Create a share in ACCOUNTA and configure a listing in Snowflake Marketplace or a Direct Share for cross-cloud sharing  
D) Enable VPC peering between ACCOUNTA and PARTNERB  

**Correct Answer:** C

---

Question 214. After ACCOUNTA publishes MARKET_DB as a share on a cross-cloud listing, what must PARTNERB do in their Azure East US 2 account to access the shared data?

A) Create a database from the share in their account using the Snowflake UI or SQL commands  
B) Request a physical backup of MARKET_DB from ACCOUNTA  
C) Set up external stages pointing to ACCOUNTA’s S3 bucket  
D) Use a VPN connection to access ACCOUNTA’s Snowflake account directly  

**Correct Answer:** A

---

Question 215. To successfully consume shared data from MARKET_DB, PARTNERB must meet certain prerequisites in their Snowflake account. Which of the following is required?

A) PARTNERB must have privileges to create databases from shares  
B) PARTNERB must migrate their Snowflake account to AWS us-east-1  
C) PARTNERB must have the same warehouse size as ACCOUNTA  
D) PARTNERB must use the same database name as ACCOUNTA  

**Correct Answer:** A

---

Question 216. A company is building a custom integration to automate file ingestion into Snowflake using the insertFiles API of Snowpipe. The team wants to ingest several thousand files in a single API call for efficiency. What is a limitation they should be aware of?

A) The insertFiles API does not support ingesting files from GCP storage  
B) There is a limit on the maximum number of files that can be included in a single insertFiles API call  
C) The insertFiles API cannot trigger Snowpipe automatically  
D) File sizes must be under 1 MB for each file in the request  

**Correct Answer:** B

---

Question 217. During a testing phase, your development team attempts to use the insertFiles API to ingest files, but some files are not loaded successfully. Upon investigation, you discover that the API does not provide immediate ingestion status feedback for each file. What is another limitation of the insertFiles API?

A) It only supports JSON files  
B) The API does not provide synchronous status feedback; ingestion status must be checked separately  
C) The API automatically retries failed ingestions  
D) It only works with files in the same region as the Snowflake account  

**Correct Answer:** B

---

Question 218. As a Snowflake architect, you are asked to design a system that uses Snowpipe’s insertFiles API to load files from multiple cloud providers into a single database. Which limitation should be considered when planning this architecture?

A) The insertFiles API supports only files stored in the same cloud provider as the Snowflake account  
B) The API encrypts files before ingestion  
C) The insertFiles API requires files to have a .csv extension  
D) The API can ingest files from any cloud provider without restriction  

**Correct Answer:** A

---

Question 219. As a Snowflake architect, you are designing a system to automate file ingestion using Snowpipe’s insertFiles API. The ETL team wants to speed up processing by submitting 5,000 files in a single API request. What will be the result, and what best practice should you follow?

A) The insertFiles API will process all 5,000 files successfully  
B) The insertFiles API will return an error; requests must be limited to 1,000 files per call  
C) The API will queue the extra files and process them later  
D) The API will automatically split the request into batches of 1,000 files each  

**Correct Answer:** B

---

Question 220. A Snowflake architect increases the warehouse size from L to XL to improve the performance of a long-running join query, but the query time remains unchanged. Upon further investigation, what data-related issue could be causing this lack of improvement?

A) Data skew in the join key column is causing one compute node to process much more data than others  
B) The query is using too many subqueries  
C) The XL warehouse is under-provisioned for the dataset  
D) The result set is not being cached  

**Correct Answer:** A

---

Question 221. In a large join between two tables, you notice that one particular value in the join column appears far more frequently than others. What is the likely impact of this scenario on Snowflake’s query performance?

A) The query will utilize all compute nodes evenly  
B) One compute node will be overloaded, resulting in slow performance due to data skew  
C) The query will automatically retry failed nodes  
D) The warehouse size will automatically adjust to handle the load  

**Correct Answer:** B

---

Question 222. A retail analytics team is experiencing inconsistent performance with their nightly joins in Snowflake, despite scaling up the virtual warehouse. What strategy should the architect consider to address performance issues caused by data skew?

A) Redistribute the data or rewrite the join logic to minimize skew in the join key  
B) Increase the warehouse size further  
C) Use manual clustering for all tables  
D) Disable result caching  

**Correct Answer:** A

---

Question 223. A Snowflake architect attempts to clone a schema at a timestamp when a different schema instance exists with the same name. What steps should the architect take to restore and clone the original schema as of the desired timestamp?

A) Rename the current schema to free up the name, then perform an UNDROP to restore the previous version and run the CLONE statement  
B) Increase the warehouse size and retry the clone  
C) Drop the database and recreate it  
D) Request support to retrieve the schema from backups  

**Correct Answer:** A

---

Question 224. An architect receives an error when trying to clone a schema as of a timestamp before the current instance was created. The error states: "Time travel data is not available for schema STAGING. The requested time is either beyond the allowed time travel period or before the object creation time." What is the most likely cause?

A) The requested timestamp is before the current schema instance was created  
B) The schema has never existed  
C) The clone command syntax is incorrect  
D) The user's role lacks sufficient privileges  

**Correct Answer:** A

---

Question 225. After renaming the current schema, an architect uses the UNDROP SCHEMA command to recover a previous version of a schema dropped a week ago. What must the architect do next to access the historical data as of a specific timestamp?

A) Clone the undropped schema using the AT (TIMESTAMP => ...) clause  
B) Drop the undropped schema again  
C) Increase the Time Travel retention period  
D) Restore the underlying database  

**Correct Answer:** A

---

Question 226. A data engineer writes a stored procedure in Snowflake and wants to capture the username of the person executing it for audit logging. Which context function should they use?

A) `CURRENT_USER`  
B) `CURRENT_ROLE`  
C) `CURRENT_DATABASE`  
D) `CURRENT_SESSION`  

**Correct Answer:** A

---

Question 227. A Snowflake architect needs to create a script that dynamically adapts to the current working schema and warehouse for session troubleshooting. Which two context functions should be included in the script?

A) `CURRENT_DATABASE` and `CURRENT_REGION`  
B) `CURRENT_WAREHOUSE` and `CURRENT_SCHEMA`  
C) `CURRENT_USER` and `CURRENT_ACCOUNT`  
D) `CURRENT_ROLE` and `CURRENT_SESSION`  

**Correct Answer:** B

---

Question 228. A Snowflake architect is investigating slow filter queries on a large table. Which parameter, when found to be **significantly greater than 1**, signals that the table is not well-clustered?

A) Clustering Ratio  
B) Clustering Depth  
C) Row Count  
D) Partition Size  

**Correct Answer:** A

---

Question 229. During a performance review, an architect observes that the **average clustering depth** of a table is much higher than expected. What does this imply about the table’s clustering?

A) The table is well-clustered and queries will be efficient  
B) The table is not well-clustered, resulting in less efficient queries  
C) The table has too few micro-partitions  
D) The table is over-indexed  

**Correct Answer:** B

---

Question 230. When analyzing micro-partition statistics, which parameter should an architect examine to confirm that filter queries are scanning more micro-partitions than necessary due to poor clustering?

A) Clustering Depth  
B) Micro-partition Count  
C) Clustering Key  
D) Table Size  

**Correct Answer:** A

Question 231. A business analyst reports that queries filtering by a certain column are slow, even after clustering. What parameter can indicate that the clustering is ineffective for that column?

A) High clustering depth for the column  
B) Low row count in the table  
C) High number of columns  
D) Table retention period  

**Correct Answer:** A

---

Question 232. The data engineering team runs a clustering information query and notices that many micro-partitions have a clustering depth of greater than 5. What action should be considered?

A) Re-cluster the table to reduce clustering depth  
B) Increase the table’s retention period  
C) Add more columns to the clustering key  
D) Reduce the warehouse size  

**Correct Answer:** A

---

Question 233. After performing a clustering operation, an architect finds that the clustering depth has only slightly decreased. What does this suggest about the table or the clustering key?

A) The clustering key may not be optimal for the table’s data distribution  
B) The warehouse size is too small  
C) The table is too large to be clustered  
D) The micro-partitions are too small  

**Correct Answer:** A

---

Question 234. A Snowflake architect is reviewing clustering metadata for a table and sees that the clustering depth for most micro-partitions is 1, but a few have depths of 15 or higher. What does this suggest about data distribution?

A) Most micro-partitions are well-clustered, but some have significant data overlap and poor clustering  
B) All micro-partitions are equally well-clustered  
C) The clustering key is optimal for the entire table  
D) The table does not need re-clustering  

**Correct Answer:** A

---

Question 235. An engineer wants to monitor changes in clustering efficiency after a scheduled reclustering job. Which parameter should they track over time to evaluate improvements?

A) Average clustering depth  
B) Total row count  
C) Number of columns in the table  
D) Table retention period  

**Correct Answer:** A

---

Question 236. A data scientist notices that queries with filters on the clustering key are still slow, even after reclustering. What clustering depth value would most likely explain this issue?

A) Clustering depth values remain high across many micro-partitions  
B) Clustering depth is consistently 1 across all micro-partitions  
C) The table’s row count has decreased  
D) The table has only one micro-partition  

**Correct Answer:** A

---

Question 237. A Snowflake architect is configuring a `COPY INTO <location>` command to unload data into an Amazon S3 bucket. Which of the following options is a valid configuration for specifying the file format?

A) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'NONE')  
B) FILE_FORMAT = (TYPE = 'PDF')  
C) FILE_FORMAT = (TYPE = 'DOCX')  
D) FILE_FORMAT = (TYPE = 'HTML')  

**Correct Answer:** A

---

Question 238. When unloading data from a Snowflake table to a cloud storage location using `COPY INTO <location>`, which of these access configurations is valid for authenticating to an Azure Blob Storage location?

A) STORAGE_INTEGRATION  
B) PASSWORD  
C) OAUTH_TOKEN  
D) API_KEY  

**Correct Answer:** A

---

Question 239. A data engineer wants to partition unloaded files by a column value using the `COPY INTO <location>` command. Which of the following statements correctly implements this configuration?

A) PARTITION BY (column_name)  
B) SPLIT BY column_name  
C) GROUP BY column_name  
D) CLUSTER BY column_name  

**Correct Answer:** A

---

Question 240. An architect wants to unload data from Snowflake into an S3 bucket in compressed CSV files. Which of the following configurations is valid for specifying file format and compression in the `COPY INTO <location>` command?

A) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'GZIP')  
B) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'PDF')  
C) FILE_FORMAT = (TYPE = 'CSV', ENCRYPTION = 'NONE')  
D) FILE_FORMAT = (TYPE = 'PARQUET', COMPRESSION = 'GZIP')  

**Correct Answer:** A

---

Question 241. A data engineer needs to ensure unloaded files are encrypted using a customer-managed key when using `COPY INTO <location>` with Azure Blob Storage. Which option should be included in the command?

A) ENCRYPTION = (TYPE = 'AZURE_CSE', MASTER_KEY = '<key_value>')  
B) ENCRYPTION = (TYPE = 'NONE')  
C) ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  
D) COMPRESSION = 'ENCRYPTED'  

**Correct Answer:** A

---

Question 242. When unloading data into Amazon S3, which of the following file formats support both compression and encryption options in the `COPY INTO <location>` command?

A) CSV  
B) PARQUET  
C) JSON  
D) All of the above  

**Correct Answer:** D

---

Question 243. The analytics team wants to unload data in JSON format, compressed and encrypted, into an external stage. Which configuration is valid for the file format and options in the `COPY INTO <location>` command?

A) FILE_FORMAT = (TYPE = 'JSON', COMPRESSION = 'GZIP'), ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  
B) FILE_FORMAT = (TYPE = 'XML', COMPRESSION = 'GZIP'), ENCRYPTION = (TYPE = 'NONE')  
C) FILE_FORMAT = (TYPE = 'JSON', ENCRYPTION = 'NONE')  
D) FILE_FORMAT = (TYPE = 'JSON', COMPRESSION = 'PDF'), ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  

**Correct Answer:** A

---

Question 244. A Snowflake architect is overseeing a finance department’s analytics workload. The multi-cluster warehouse is set to the economy scaling policy. When will Snowflake start a new cluster for this warehouse?

A) When queued queries cannot be processed quickly enough by the current cluster  
B) As soon as any user submits a query  
C) When the warehouse has been idle for 10 minutes  
D) Whenever a new user connects to the warehouse  

**Correct Answer:** A

---

Question 245. A logistics company wants to minimize costs during off-peak hours but still ensure timely query processing. They use the economy scaling policy for their multi-cluster warehouse. What is the main factor that triggers Snowflake to start a new cluster?

A) The number and urgency of queued queries  
B) The total number of users connected  
C) The amount of data stored in the warehouse  
D) The frequency of warehouse restarts  

**Correct Answer:** A

---

Question 246. In the economy scaling policy, when will Snowflake NOT start a new cluster?

A) When all queries can be processed quickly by the existing cluster  
B) When the cluster’s CPU usage exceeds 90%  
C) When query queue length is more than ten  
D) When there is a maintenance window scheduled  

**Correct Answer:** A

---

Question 247. During a high-traffic marketing campaign, the analytics team experiences query delays even with a multi-cluster warehouse set to the economy scaling policy. What should the architect investigate as a possible cause?

A) The queued queries are still being processed efficiently by the existing cluster, so no new cluster is started  
B) The warehouse is set to standard scaling policy  
C) The stage storage is full  
D) The number of clusters allowed is set to maximum  

**Correct Answer:** A

---

Question 248. A Snowflake administrator is asked why the multi-cluster warehouse didn’t start a new cluster during a spike in user activity. Which explanation is correct regarding the economy scaling policy?

A) New clusters are only started if queued queries cannot be processed quickly enough by the current cluster  
B) New clusters are started for every new user session  
C) New clusters are started every time the warehouse is resized  
D) New clusters are started at a fixed interval regardless of workload  

**Correct Answer:** A

---

Question 249. A Snowflake architect is asked why a multi-cluster warehouse with the economy scaling policy did not start a new cluster during a peak period. The architect notes that although there were queued queries, the system estimated that the additional cluster would not be busy for at least six minutes. What does this behavior demonstrate about Snowflake’s economy scaling policy?

A) New clusters are only started if the system estimates the additional cluster will be busy for at least six minutes  
B) New clusters are started for every query in the queue, regardless of estimated workload  
C) Clusters are started based solely on the number of connected users  
D) Clusters are automatically started every 15 minutes during peak hours  

**Correct Answer:** A

---

Question 250. A Snowflake architect is tasked with designing a data loading solution for a retail company’s daily sales data. Which of the following considerations should be taken into account when choosing the data loading method? (Choose three.)

A) The volume and frequency of incoming data  
B) The format and structure of source data files  
C) The database user’s preferred SQL editor  
D) The need for real-time or near-real-time availability of loaded data  
E) The color scheme of the Snowflake UI  

**Correct Answers:** A, B, D

---

Question 251. A logistics firm needs to load nightly batch data from multiple remote sources into Snowflake. Which three factors are most important for the architect to evaluate when selecting a loading approach?

A) Network connectivity and bandwidth between source systems and Snowflake  
B) Whether the source files are encrypted or compressed  
C) The time zone settings of the loading script  
D) Availability of automation and error handling features  
E) The vendor of the source database  

**Correct Answers:** A, B, D

---

Question 252. A finance department wants to migrate legacy data into Snowflake from various file formats. Which of the following considerations should influence the architect’s choice of loading method? (Choose three.)

A) Support for different file formats (e.g., CSV, JSON, Parquet)  
B) Ability to handle schema evolution or changes in source data  
C) Whether data transformation is required during the load  
D) The personal preference of the data analyst  
E) The marketing department’s opinion on the loading tool  

**Correct Answers:** A, B, C

---

Question 253. A healthcare company is evaluating different ELT tools to load patient records into Snowflake. Which three considerations should the architect prioritize when selecting an ELT method? (Choose three.)

A) Support for incremental data loading and change data capture  
B) Ability to transform data during or after the load process  
C) Integration with existing data sources and enterprise security policies  
D) The color scheme of the ELT tool’s interface  
E) The company’s preferred operating system  

**Correct Answers:** A, B, C

---

Question 254. An architect is planning a data pipeline for a retail chain, which requires automated nightly loads into Snowflake with complex transformations. What should the architect consider when choosing an ELT approach? (Choose three.)

A) Automation and scheduling capabilities  
B) Scalability to handle varying data volumes  
C) Flexibility to apply business logic as part of the transformation  
D) Whether the tool supports exporting to Excel  
E) The number of employees in the IT department  

**Correct Answers:** A, B, C

---

Question 255. A financial services firm is migrating legacy batch ETL jobs to a modern ELT process in Snowflake. What are the three most important factors to evaluate when designing the new data loading solution?

A) Compatibility with Snowflake’s native ingestion and transformation features  
B) Monitoring and error recovery capabilities  
C) Ability to orchestrate and chain multiple transformation steps  
D) The desktop wallpaper of the data engineer  
E) The preferred browser of end users  

**Correct Answers:** A, B, C

---

Question 256. A data architect wants to use Snowflake’s Search Optimization Service to speed up searches on semi-structured data stored in a VARIANT column. What limitation should they be aware of?

A) The Search Optimization Service does not support search optimization on semi-structured data types such as VARIANT, OBJECT, or ARRAY  
B) The service will automatically create materialized views for semi-structured columns  
C) All data types are supported equally for search optimization  
D) The service only works for data in external stages  

**Correct Answer:** A

---

Question 257. A financial analyst needs to optimize queries that use `LIKE '%pattern%'` on a text column. Can the Search Optimization Service improve performance for this type of query?

A) No, the Search Optimization Service does not optimize search performance for pattern matching queries using wildcards at the start of the pattern  
B) Yes, it fully supports all pattern matching queries  
C) Only for columns with numeric data types  
D) Only for clustered tables  

**Correct Answer:** A

---

Question 258. A Snowflake architect plans to use the Search Optimization Service to speed up searches on a small lookup table. What limitation might make this an inefficient choice?

A) The overhead and cost of the Search Optimization Service may outweigh the performance benefits for small tables  
B) The service cannot be used on any table smaller than 10GB  
C) The service is required for all tables in Snowflake  
D) The service requires manual refresh after each data load  

**Correct Answer:** A

---

Question 259. An architect tries to use the Search Optimization Service to accelerate searches on a table that is frequently updated throughout the day. What limitation should they consider regarding freshness of query results?

A) Search Optimization Service may have lag in indexing updates, so queries might not reflect the most recent changes immediately  
B) The service guarantees real-time indexing for all updates  
C) The service cannot be used on tables with frequent updates  
D) The service can only be refreshed once per day  

**Correct Answer:** A

---

Question 260. A retail company wants to apply the Search Optimization Service to a multi-cluster warehouse. What is a limitation they should understand?

A) The Search Optimization Service is applied at the table level, not at the warehouse level  
B) The service is only available for single-cluster warehouses  
C) It can only be used for external tables  
D) It is available only for tables with less than 1 million rows  

**Correct Answer:** A

---

Question 261. A Snowflake architect needs to assign SELECT privileges on a table named `SALES` in the `REPORTING` schema of the `FINANCE` database to a user. What must the architect do before granting the SELECT privilege on the table?

A) First grant USAGE privilege on the `FINANCE` database and `REPORTING` schema to the user  
B) Grant CREATE TABLE privilege on the schema  
C) Grant OWNERSHIP privilege on the database  
D) Grant MONITOR privilege on the table  

**Correct Answer:** A

---

Question 262. A financial services company uses Okta as its identity provider, but also supports username/password and key pair authentication for their client applications connecting to Snowflake. What is the best practice for authentication method priority?

A) Use federated authentication through Okta as the primary method, with key pair authentication as a fallback, and username/password as the last resort  
B) Always use username/password, then Okta, and finally key pair  
C) Use key pair authentication first for all users  
D) Use passwordless authentication only  

**Correct Answer:** A

---

Question 263. A Snowflake architect is designing a secure client application integration. The application supports Okta, key pair authentication, and username/password. How should the architect prioritize these authentication methods for best security and manageability?

A) Prioritize federated authentication (Okta), then key pair authentication, followed by username/password if others are unavailable  
B) Prioritize username/password, then Okta, and never allow key pair  
C) Use key pair authentication only  
D) Allow users to pick any method randomly  

**Correct Answer:** A

---

Question 264. A global enterprise is integrating several client applications with Snowflake, all of which support OAuth, external browser, Okta native authentication, key pair authentication, and password. According to best practice, which method should be given highest priority for application authentication?

A) Key Pair Authentication  
B) Password  
C) OAuth (Snowflake OAuth or External OAuth)  
D) Okta native authentication  
E) External browser  

**Correct Answer:** C

---

Question 265. A Snowflake architect is designing an authentication strategy for a multi-region application with multiple supported methods. What is the recommended order of priority for authentication methods according to Snowflake best practices?

A) Okta native authentication, External browser, Password, Key Pair Authentication, OAuth  
B) OAuth, External browser, Okta native authentication, Key Pair Authentication, Password  
C) Password, External browser, OAuth, Okta native authentication, Key Pair Authentication  
D) Key Pair Authentication, Password, Okta native authentication, OAuth, External browser  
E) External browser, OAuth, Okta native authentication, Password, Key Pair Authentication  

**Correct Answer:** B

---

Question 266. A Snowflake architect is implementing a monitoring solution that calls the Snowpipe `loadHistoryScan` endpoint multiple times per hour. What is the best practice to follow regarding the frequency of these calls?

A) Limit the frequency of calls to avoid exceeding rate limits and incurring unnecessary costs  
B) Call the endpoint as often as possible for real-time updates  
C) Only call the endpoint once per day  
D) Use multiple concurrent requests to maximize throughput  

**Correct Answer:** A

---

Question 267. A data engineer wants to retrieve Snowpipe load history for a large external stage. What is the recommended best practice when using the `loadHistoryScan` endpoint to ensure efficient and reliable data retrieval?

A) Specify a narrow time window for each scan to reduce response size and improve performance  
B) Request the entire history from the start of the stage’s existence in a single call  
C) Disable authentication for faster access  
D) Always use wildcard patterns for file selection  

**Correct Answer:** A

---

Question 268. A Snowflake architect is designing a monitoring solution for Snowpipe data ingestion. What is a recommended polling strategy when calling the `loadHistoryScan` endpoint to ensure robust, gap-free ingestion monitoring?

A) Read the last 10 minutes of history every 8 minutes to provide overlapping coverage  
B) Poll the endpoint every hour to reduce system load  
C) Only read the exact time window since the last poll to avoid duplicate data  
D) Read the entire history of the stage every time the endpoint is called  

**Correct Answer:** A

---

Question 269. A DevOps team sets `DATA_RETENTION_TIME_IN_DAYS = 7` at the database level for staging tables, but finds that some tables are only recoverable for 1 day. What is the most likely cause?

A) The `DATA_RETENTION_TIME_IN_DAYS` parameter was set to 1 at the table level, overriding the database setting  
B) The tables are in a different database  
C) The user does not have the RECOVER privilege  
D) The schema is set to transient  

**Correct Answer:** A

---

Question 270. Despite configuring a 7-day data retention at the database level, certain staging tables cannot be recovered after 1 day. Which scenario explains this behavior?

A) The staging tables have a lower retention period configured individually  
B) The database has not been refreshed  
C) Time Travel has been disabled for the account  
D) The warehouse is suspended  

**Correct Answer:** A

---

Question 271. The DevOps team discovers that some tables in the staging schema are unrecoverable after 1 day, even though the database parameter for data retention is set to 7 days. What could be causing this issue?

A) Table-level `DATA_RETENTION_TIME_IN_DAYS` settings override the database-level default if explicitly set  
B) The staging tables are not being updated  
C) The tables are permanent and do not support retention  
D) Only transient tables support data retention  

**Correct Answer:** A

---

Question 272. Company A wants to share data with Company B, but the data is located in two separate databases within Company A’s Snowflake account. What is the best approach to enable sharing of this data?

A) Create one or more secure views and combine them into a single share  
B) Create separate shares for each database, as a share can only reference objects from one database  
C) Grant direct table access to Company B’s account  
D) Export the data from both databases and upload it to Company B’s stage  

**Correct Answer:** B

---

Question 273. A Snowflake architect at Company A needs to share tables from two different databases with Company B, whose account is in the same region. What should the architect do?

A) Create two separate shares, one for each database, and provide access to Company B for both shares  
B) Copy all tables into a single database, then create a single share  
C) Use direct database grants to Company B  
D) Use external tables for sharing  

**Correct Answer:** A

---

Question 274. Company A’s data engineering team is trying to share data from two databases with Company B. Which statement reflects Snowflake’s best practice and technical limitation for data sharing?

A) Since a share can only reference objects from one database, Company A must create a separate share for each database  
B) Both databases can be included in a single share if they have the same owner  
C) A share can include objects from any number of databases in the account  
D) Data sharing is only possible if both companies use the same warehouse  

**Correct Answer:** A

---

Question 275. A Data Architect receives the error "Number of columns in file (15) does not match that of the corresponding table (14)" while loading CSV data with the COPY INTO statement. What is the best approach to resolve the error and ensure all fields are loaded into the table?

A) Alter the target table to add an additional column so the number of table columns matches the file  
B) Ignore the error and rerun the load  
C) Drop one column from the CSV file  
D) Use the ON_ERROR='SKIP_FILE' option to skip problematic files  

**Correct Answer:** A

---

Question 276. While attempting to load a CSV file with 15 columns into a table with 14 columns, a Data Architect encounters a column mismatch error. What should they do to successfully load all fields from the file?

A) Add a new column to the target table and reload the file  
B) Change the file format to JSON  
C) Set the FILE_FORMAT parameter to automatically drop the extra column  
D) Reduce the number of columns in the CSV file to 14  

**Correct Answer:** A

---

Question 277. A Data Architect is loading CSV data into Snowflake using COPY INTO and gets an error due to more columns in the file than in the target table. What is the recommended solution if every field in the CSV is needed for analysis?

A) Modify the table structure to include an additional column, then reload  
B) Use a WHERE clause in the COPY INTO statement  
C) Set the ON_ERROR parameter to 'CONTINUE'  
D) Ignore the extra column during loading  

**Correct Answer:** A

---

Question 278. A Snowflake architect wants to enable event-based automated data loading from an Amazon S3 bucket using Snowpipe. When is the INTEGRATION parameter required?

A) When configuring an external stage to use cloud messaging for automated Snowpipe ingestion  
B) When loading data manually using the Snowpipe REST API  
C) When using an internal Snowflake stage  
D) When loading data from a local file system  

**Correct Answer:** A

---

Question 279. A data engineer is setting up Snowpipe to automatically ingest files from Azure Blob Storage as soon as new files arrive. What must be included in the external stage definition to allow Snowpipe to receive event notifications from Azure?

A) The INTEGRATION parameter referencing a storage integration object  
B) The FILE_FORMAT parameter set to JSON  
C) A direct reference to the storage account credentials  
D) The ON_ERROR parameter set to CONTINUE  

**Correct Answer:** A

---

Question 280. Which scenario requires specifying the INTEGRATION parameter for Snowpipe?

A) When using an external stage with cloud event notifications for automatic loading  
B) When using COPY INTO to load data on demand  
C) When querying data from a permanent table  
D) When listing the files in an internal stage  

**Correct Answer:** A

---

Question 281. A data engineer uses a stream to track changes in a table but notices the stream has become stale after the table was truncated. What is the reason for this?

A) Truncating the source table invalidates the stream’s change tracking and causes it to become stale  
B) The table was not queried for a week  
C) The stream was dropped  
D) The table was renamed  

**Correct Answer:** A

---

Question 282. A Snowflake architect sets a table’s `DATA_RETENTION_TIME_IN_DAYS` to 2. After 5 days of not consuming the stream, it is marked as stale. Why did this occur?

A) The stream was not consumed within the table’s data retention window  
B) A new column was added to the table  
C) The stream’s name was changed  
D) The stream was used to track a view instead of a table  

**Correct Answer:** A

---

Question 283. During a schema migration, a table is replaced using `CREATE OR REPLACE TABLE`. What impact does this have on any existing streams on the table?

A) The stream becomes stale because the underlying table was replaced  
B) The stream automatically updates to follow the new table  
C) The stream continues tracking changes without interruption  
D) The stream is converted to a materialized view  

**Correct Answer:** A

---

Question 284. A financial services company discovers that the `Data` table in its Snowflake environment contains corrupted data due to a faulty ETL job. What is the most efficient command to recover the table to its state 5 minutes ago while preserving the original table for investigation?

A) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
C) DROP TABLE Data;  
D) SELECT * FROM Data WHERE date > current_timestamp() - interval '5 minutes';  

**Correct Answer:** B

---

Question 285. A healthcare organization needs to restore a table named `Data` to its state as of 5 minutes ago after a user mistakenly updated all rows. Which command should the architect use to overwrite the current table with its previous state?

A) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
B) UPDATE Data SET ... ;  
C) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
D) ALTER TABLE Data SET DATA_RETENTION_TIME_IN_DAYS = 1;  

**Correct Answer:** C

---

Question 286. A retail company wants to compare the current corrupted data in the `Data` table with its state 5 minutes ago. What is the best approach to achieve this using Snowflake features?

A) Use CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60); to create a copy for analysis  
B) Use DROP TABLE Data; and reload from backup  
C) Use SELECT * FROM Data; only  
D) Use GRANT SELECT ON Data TO ANALYST;  

**Correct Answer:** A

---

Question 287. During a system audit, a logistics company identifies data corruption in the `Data` table. The analyst is tasked with restoring the table to its exact state 5 minutes prior using Snowflake’s Time Travel feature. Which command should be executed?

A) SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
C) DELETE FROM Data WHERE timestamp < current_timestamp() - interval '5 minutes';  
D) CREATE TABLE Data AS CLONE Data AT (OFFSET => -5*60);  

**Correct Answer:** B

---

Question 288. A manufacturing company’s data engineering team wants to investigate a table corruption incident without impacting ongoing operations. What is the safest way to obtain a snapshot of the `Data` table as it was 5 minutes ago?

A) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
B) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data;  
C) SELECT * FROM Data WHERE timestamp > current_timestamp() - interval '5 minutes';  
D) TRUNCATE TABLE Data;  

**Correct Answer:** A

---

Question 289. A retail company wants to export a 5 GB table from Snowflake to CSV as quickly as possible for downstream analytics. What is the most performant method the architect should use?

A) Use the COPY INTO command and set a large SPLIT_SIZE to maximize parallelism  
B) Use SELECT * FROM table and download the results via the web interface  
C) Use COPY INTO with compression disabled  
D) Use INSERT INTO to copy data to a stage and export manually  

**Correct Answer:** A

---

Question 290. A Snowflake Architect is configuring an extract job to unload 5 GB of sales data as CSV. To take advantage of parallel operations and maximize performance, what should they do regarding the MAX_FILE_SIZE parameter?

A) Set MAX_FILE_SIZE to a very large value to minimize the number of output files  
B) Leave MAX_FILE_SIZE at its default value to allow Snowflake to split the output into multiple 16 MB files for parallel writing  
C) Set MAX_FILE_SIZE to 1 MB for smaller files  
D) Set MAX_FILE_SIZE to 5 GB to create a single output file  

**Correct Answer:** B

---

Question 291. A data architect at a logistics company needs to regularly unload large tables to CSV for reporting. What Snowflake feature allows the export process to be distributed and completed faster?

A) Snowflake automatically splits output into multiple files and writes them in parallel when using COPY INTO  
B) Data must be unloaded using a single thread for consistency  
C) Export jobs require manual file segmentation after unloading  
D) All output files must be merged after unloading to improve performance  

**Correct Answer:** A

---

Question 292. A Snowflake Architect is creating a read-only role for employees in the human resources department. Which set of permissions ensures users with this role can only view data in `hr_db` and cannot modify it?

A) GRANT USAGE ON DATABASE hr_db; GRANT USAGE ON SCHEMA hr_db.public; GRANT SELECT ON ALL TABLES IN SCHEMA hr_db.public;  
B) GRANT OWNERSHIP ON DATABASE hr_db;  
C) GRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA hr_db.public;  
D) GRANT ALL PRIVILEGES ON DATABASE hr_db;  

**Correct Answer:** A

---

Question 293. A company wants to prevent certain employees in the human resources department from making changes to the `hr_db` database, ensuring they can only view employee data. Which permissions should the architect grant to the HR read-only role?

A) USAGE and SELECT privileges on the relevant database, schema, and tables  
B) INSERT and UPDATE privileges on hr_db  
C) USAGE and DELETE privileges on all tables in hr_db  
D) OWNERSHIP privilege on hr_db  

**Correct Answer:** A

---

Question 294. An Architect must allow HR staff to run queries against employee data in `hr_db` while restricting them from editing or deleting records. Which combination of privileges should be granted to the HR read-only role?

A) USAGE on hr_db and its schema, plus SELECT on all tables  
B) CREATE TABLE and SELECT on hr_db  
C) USAGE on hr_db and INSERT on all tables  
D) USAGE on hr_db and UPDATE on all tables  

**Correct Answer:** A

---

Question 295. A fintech company wants to connect its AWS VPC directly to Snowflake without routing traffic over the public internet. Which security feature must be enabled to accomplish this?

A) AWS PrivateLink  
B) SSL encryption  
C) Snowflake Network Policy  
D) SAML authentication  

**Correct Answer:** A

---

Question 296. An architect is tasked with ensuring secure, private connectivity between an AWS VPC and Snowflake, preventing exposure of data to the public internet. What is the required security feature?

A) Configure AWS PrivateLink for Snowflake  
B) Set up a VPN tunnel between VPC and Snowflake  
C) Restrict traffic using AWS Security Groups  
D) Enable IP whitelisting in Snowflake  

**Correct Answer:** A

---

Question 297. A healthcare company must comply with strict data privacy regulations and needs to connect its AWS-hosted analytics environment directly to Snowflake. Which feature should be used to meet this requirement?

A) AWS PrivateLink  
B) Multi-factor authentication  
C) Snowflake role-based access control  
D) AWS IAM roles  

**Correct Answer:** A
Question 297. A department is running slow aggregation queries on a massive table, primarily filtering and grouping by a small subset of columns. The table is not currently clustered. What is the most optimal solution to improve query performance?

A) Define a cluster key on the columns used for filtering and grouping  
B) Increase the warehouse size for all queries  
C) Create a materialized view on the table  
D) Partition the table using manual sharding  

**Correct Answer:** A

---

Question 298. An analytics team is experiencing long runtimes for queries on a huge table because the data isn’t organized according to the columns used in search predicates. What should the architect do to optimize query performance for this use case?

A) Create a cluster key on the columns most frequently used in query filters  
B) Grant more permissions to the users  
C) Use row-level security policies  
D) Move the table to a different schema  

**Correct Answer:** A

---

Question 299. After noticing that queries are slow due to lack of clustering on relevant columns, what is Snowflake’s recommended solution to optimize performance for aggregation queries on a large table?

A) Implement clustering on the columns most often used in WHERE and GROUP BY clauses  
B) Drop and recreate the table  
C) Only run queries during off-peak hours  
D) Disable automatic clustering  

**Correct Answer:** A

---

Question 300. A data engineer needs to diagnose why a scheduled Snowflake task failed overnight. Which function should they use to retrieve information about past task executions, including status and errors?

A) TASK_HISTORY  
B) SYSTEM$TASK_STATUS  
C) INFORMATION_SCHEMA.QUERY_HISTORY  
D) SHOW TASKS  

**Correct Answer:** A

---

Question 301. A Snowflake Architect wants to analyze trends and durations of task runs for a specific data pipeline over the last week. Which approach allows them to obtain this information?

A) Query the TASK_HISTORY table function for the specific task and time period  
B) Use the SHOW TABLES command  
C) Check the WAREHOUSE_HISTORY function  
D) Review the stage history using STAGE_HISTORY  

**Correct Answer:** A

---

Question 302. A business analyst is tasked with auditing how frequently a certain Snowflake task has run and whether any executions failed recently. What is the most appropriate method to obtain this information?

A) Use the TASK_HISTORY function to list executions and statuses  
B) Query the INFORMATION_SCHEMA.TABLES view  
C) Check the stage for loaded files  
D) Use the COPY_HISTORY function  

**Correct Answer:** A

---

Question 303. A Snowflake architect creates a database with `DATA_RETENTION_TIME_IN_DAYS=30` and a schema within it with `DATA_RETENTION_TIME_IN_DAYS=50`. If the database is dropped, how long is the data in schema S1 available using Time Travel?

A) 50 days  
B) 30 days  
C) 1 day  
D) 0 days  

**Correct Answer:** B

---

Question 304. After dropping a database with a retention period of 30 days, but a contained schema has a retention period of 50 days, for how long can objects in the dropped schema be accessed via Time Travel?

A) The retention period set at the database level, 30 days  
B) The retention period set at the schema level, 50 days  
C) Until the end of the current day  
D) Indefinitely  

**Correct Answer:** A

---

Question 305. If a database with `DATA_RETENTION_TIME_IN_DAYS=30` and a schema with `DATA_RETENTION_TIME_IN_DAYS=50` is dropped, what is the maximum period that Time Travel can be used to recover data from the schema?

A) 7 days  
B) 30 days  
C) 50 days  
D) 24 hours  

**Correct Answer:** B

---

Question 306. A Snowflake Architect wants to simplify the management of credentials for several external stages pointing to different S3 buckets. What is the best way to accomplish this?

A) Create a storage integration and reference it in each stage  
B) Store credentials in each stage definition individually  
C) Use a VPN to connect to each bucket  
D) Use a user-defined function to manage access  

**Correct Answer:** A

---

Question 307. A company needs to securely automate Snowpipe data ingestion from multiple Azure Blob Storage containers. What key feature does a storage integration enable to support this business requirement?

A) Centralized credential management and support for multiple external stages  
B) Data masking for sensitive columns  
C) Automatic data compression during ingestion  
D) Creating clustered tables automatically  

**Correct Answer:** A

---

Question 308. An architect needs to ensure that only authorized Snowflake users can access specific files in Google Cloud Storage, and wants to easily audit such access. What is an appropriate use of a storage integration?

A) Use storage integration to set up least-privilege access and enable auditing  
B) Grant direct file access to all Snowflake users  
C) Store access keys in a local file  
D) Use row-level security on all tables  

**Correct Answer:** A

---

Question 309. A business wants to reuse a single set of cloud storage credentials across several stages for ease of management and compliance. Which Snowflake feature allows this?

A) Storage integration  
B) Materialized view  
C) Virtual warehouse  
D) Network policy  

**Correct Answer:** A

---

Question 310. A Snowflake Architect is evaluating options to improve query performance on a large table. The architect is considering either clustering the table or enabling search optimization. What is the primary difference between these approaches in terms of costs?

A) Clustering incurs ongoing maintenance costs as Snowflake reorganizes data, while search optimization incurs a fixed upfront cost only  
B) Clustering incurs ongoing maintenance costs, whereas search optimization incurs recurring storage costs for search access paths  
C) Both clustering and search optimization incur only one-time costs when enabled  
D) Clustering is free, while search optimization is a paid feature  

**Correct Answer:** B

---

Question 311. A financial institution wants to enable SSO for its Snowflake users using its corporate identity provider. Which of the following vendors is natively supported by Snowflake for federated authentication?

A) Okta  
B) Auth0  
C) OneLogin  
D) JumpCloud  

**Correct Answer:** A

---

Question 312. A Snowflake Architect is tasked with setting up federated authentication using SAML 2.0. Which identity provider can be configured natively in Snowflake for this purpose?

A) Microsoft Azure Active Directory  
B) Duo Security  
C) Centrify  
D) IBM Tivoli  

**Correct Answer:** A

---

Question 313. A healthcare company wants to use Ping Identity to provide SSO access to Snowflake for its employees. Is Ping Identity natively supported by Snowflake for federated authentication?

A) No, only Google Workspace is supported  
B) Yes, Ping Identity is natively supported  
C) No, only Okta and Azure AD are supported  
D) Yes, but only with OAuth2  

**Correct Answer:** B

---

Question 314. A company with multiple regional offices needs to ingest data into Snowflake from cloud storage locations in each region. What Snowflake feature should be used to securely connect these external sources?

A) Storage integration  
B) Materialized view  
C) Row access policy  
D) Data masking  

**Correct Answer:** A

---

Question 315. An architect is asked to set up a solution that allows data ingestion from many geographically dispersed sources into Snowflake, while minimizing credential sprawl. What is the recommended approach?

A) Create external stages in each region and reference a storage integration for secure access  
B) Grant direct access to the Snowflake account for all users in each region  
C) Use a virtual warehouse for each region  
D) Configure network policies for each region  

**Correct Answer:** A

---

Question 316. A global enterprise needs to automate data loading from different cloud storage locations representing various company sites into Snowflake. Which configuration enables this type of data ingestion?

A) Use storage integrations to securely connect external stages from each site  
B) Create a single table with region metadata  
C) Use masking policies for each region  
D) Grant ownership privileges to all site managers  

**Correct Answer:** A

---

Question 317. A data engineering team needs to load large datasets from Spark to Snowflake with minimal impact on their Spark cluster’s resources. Which Snowflake Spark connector transfer mode should they use?

A) External transfer mode  
B) Internal transfer mode  
C) Direct JDBC mode  
D) Manual CSV export  

**Correct Answer:** A

---

Question 318. While using the Snowflake Spark connector, a team wants all data transferred between Spark and Snowflake to happen over JDBC with no use of cloud storage. Which transfer mode accomplishes this?

A) Internal transfer mode  
B) External transfer mode  
C) Snowpipe mode  
D) Bulk copy mode  

**Correct Answer:** A

---

Question 319. A company is ingesting terabytes of data daily from Spark into Snowflake and needs to optimize for speed and scalability. What is a key feature of the external transfer mode in the Snowflake Spark connector?

A) Data is staged in cloud storage before being loaded to Snowflake  
B) Data is transferred directly via JDBC  
C) No intermediate storage is used  
D) Only small datasets are supported  

**Correct Answer:** A

---

Question 320. A Snowflake Architect is configuring data integration between Spark and Snowflake. What is the main difference between internal and external transfer modes in the Snowflake Spark connector?

A) Internal mode uses JDBC for direct data transfer, while external mode stages data in cloud storage for bulk loading  
B) Both modes always use cloud storage  
C) Internal mode uses Snowpipe for loading data  
D) External mode does not support parallel data loads  

**Correct Answer:** A

---

Question 321. A Snowflake architect notices that the database `shared_database` has been created in a consumer account via secure data sharing. Which operation can be performed on this database?

A) Query tables and views in the shared database  
B) Create new schemas in the shared database  
C) Drop objects from the shared database  
D) Grant privileges on the shared database to other accounts  

**Correct Answer:** A

---

Question 322. A user in a consumer account wants to modify the structure of a shared database named `shared_database` they received from a provider. Which of the following operations is permitted?

A) Query data in tables and views  
B) Create tables in the shared database  
C) Alter table structures in the shared database  
D) Delete schemas from the shared database  

**Correct Answer:** A

---

Question 323. An analyst is trying to insert data into a table within the `shared_database` that was shared with their Snowflake account. What is the result of this operation?

A) The operation will fail, as DML is not permitted on shared databases  
B) The data will be inserted successfully  
C) The analyst can alter the table structure before inserting  
D) The operation will succeed only if the analyst has the OWNERSHIP privilege  

**Correct Answer:** A

---

Question 324. An Architect has created an external stage in Snowflake pointing to an Amazon S3 bucket. Which feature requires setting up an AWS SNS topic to enable auto-refresh of staged files?

A) Snowpipe  
B) Manual COPY INTO command  
C) Table clustering  
D) External table creation  

**Correct Answer:** A

---

Question 325. A Snowflake Architect is configuring continuous, automated data loading from an S3 bucket. What role does an SNS topic play in this setup?

A) It sends notifications to Snowflake when new files are added to the bucket, triggering auto-refresh and data ingestion via Snowpipe  
B) It encrypts files before loading  
C) It manages user permissions for the S3 bucket  
D) It provides metadata about the bucket’s contents  

**Correct Answer:** A

---

Question 326. What is an AWS SNS topic in the context of Snowflake data ingestion from S3?

A) A messaging channel used to notify Snowflake when new files are available for automated loading  
B) A type of S3 bucket policy  
C) A user role for accessing the S3 bucket  
D) An encryption key for securing S3 files  

**Correct Answer:** A

---

Question 327. An architect is setting up a Snowpipe for automated data ingestion using the `CREATE PIPE ... AS COPY INTO` command. Which of the following copy options can be specified in this command?

A) FILE_FORMAT  
B) ON_ERROR  
C) VALIDATION_MODE  
D) ALL OF THE ABOVE  

**Correct Answer:** D

---

Question 328. A data engineer needs to configure a pipe in Snowflake to handle errors gracefully during file ingestion. Which COPY INTO option is supported in the `CREATE PIPE ... AS COPY INTO` statement to achieve this?

A) ON_ERROR  
B) PURGE  
C) COPY_GRANTS  
D) FORCE  

**Correct Answer:** A

---

Question 329. A company wants to perform data validation before loading files with Snowpipe. Which copy option is supported by the `CREATE PIPE ... AS COPY INTO` command to facilitate this requirement?

A) VALIDATION_MODE  
B) PATTERN  
C) ENCRYPTION  
D) TRUNCATECOLUMNS  

**Correct Answer:** A

---

Question 330. A Data Architect is setting up an API integration for external functions in Snowflake. What should be verified regarding network connectivity?

A) The integration’s allowed network policies enable Snowflake to access the remote API endpoint  
B) The API endpoint is hosted in the same cloud region as Snowflake  
C) The API endpoint uses HTTP only  
D) The API endpoint is accessible only from private IPs  

**Correct Answer:** A

---

Question 331. When configuring a Snowflake API integration for external functions, which security consideration is most important?

A) The remote API endpoint requires TLS (HTTPS) for secure communication  
B) The API endpoint supports multiple languages  
C) The API endpoint has no authentication  
D) The API integration uses hardcoded credentials  

**Correct Answer:** A

---

Question 332. A data architect needs to ensure that external functions in Snowflake can scale with high volumes of requests. What should be considered?

A) The remote service can handle concurrent requests and is highly available  
B) The remote service is hosted on a desktop machine  
C) The remote API endpoint is rate-limited for individual users  
D) The API integration is only used for batch jobs  

**Correct Answer:** A

---

Question 333. Before creating an external function using an API integration, what is an important configuration aspect to check in Snowflake?

A) The API integration has been granted the appropriate privileges and is active in the correct Snowflake account  
B) The API integration is named after the database  
C) The API integration uses a free API endpoint  
D) The integration only supports GET requests  

**Correct Answer:** A

---

Question 334. A Data Architect is preparing to create an API integration for external functions. Which role consideration is most important before starting the configuration?

A) The API integration must be created using the ACCOUNTADMIN or a role with the CREATE INTEGRATION privilege  
B) Any user can create an API integration without specific privileges  
C) Only the PUBLIC role can be used for API integrations  
D) The API integration does not require any role-based permissions  

**Correct Answer:** A

---

Question 335. When configuring an API integration for external functions, who should have the privilege to create or manage the integration object?

A) Only users with the necessary privileges, such as SECURITYADMIN or ACCOUNTADMIN  
B) All users in the organization  
C) Only users with the PUBLIC role  
D) Any user, regardless of privilege  

**Correct Answer:** A

---

Question 336. A Data Architect wants to restrict who can use an external function backed by an API integration. What should be considered regarding roles?

A) Grant usage privileges on the API integration and external function only to specific roles  
B) Always allow the PUBLIC role to use all integrations  
C) Do not assign usage privileges to any role  
D) Assign privileges to individual users only, not roles  

**Correct Answer:** A

---

Question 337. Before creating an external function, what is an important step for a Data Architect to take regarding Snowflake roles and privileges?

A) Ensure that the necessary roles have been granted the appropriate privileges to create, manage, and use the API integration and external functions  
B) Ignore role assignments when configuring integrations  
C) Assign privileges only after the function has been used  
D) Use a deprecated role for integration configuration  

**Correct Answer:** A

---

Question 338. A data warehouse contains a large fact table with billions of rows. Analysts frequently run queries filtering on a specific date column. When should the architect consider adding a clustering key to this table?

A) When queries often filter on the same column(s) and table size is large  
B) When the table is very small and rarely queried  
C) When only SELECT * queries are run  
D) When table is used only for staging data  

**Correct Answer:** A

---

Question 339. A business intelligence team is experiencing slow performance when querying a large table on specific columns. What scenario suggests adding a clustering key?

A) When repeated queries filter or join on the same column(s)  
B) When the table has fewer than 1,000 rows  
C) When the table is dropped frequently  
D) When queries only aggregate all rows  

**Correct Answer:** A

---

Question 340. A Snowflake Architect is designing a table expected to grow rapidly over time. The table will be queried using filters on a customer_id column. When should a clustering key be considered?

A) When query filters or sorting are frequently applied to specific columns  
B) When the table is write-only  
C) When there is no need for performance optimization  
D) When the table is used for temporary storage only  

**Correct Answer:** A

---

Question 341. A company receives thousands of very small JSON files from IoT devices every hour and stores them in cloud storage. What is the most cost-effective way to load this data into Snowflake?

A) Use Snowpipe to incrementally ingest files as they arrive  
B) Manually run COPY INTO commands for each file  
C) Load files individually using the web interface  
D) Use a high-capacity virtual warehouse for batch loading  

**Correct Answer:** A

---

Question 342. An architect is designing a solution to ingest 1,000 small JSON files per hour from cloud storage into Snowflake. Which approach minimizes cost and operational overhead?

A) Configure Snowpipe for automated, continuous ingestion  
B) Schedule frequent batch jobs using large warehouses  
C) Use the PUT command for each file  
D) Load data via the Snowflake UI  

**Correct Answer:** A

---

Question 343. A data engineer is tasked with moving numerous tiny JSON files from cloud storage into a Snowflake table. Which ingestion method is most cost-effective for this workload?

A) Snowpipe with auto-ingest from cloud storage  
B) Large-scale batch COPY INTO jobs  
C) Manual uploads with the Snowflake web UI  
D) Real-time ingestion using external functions  

**Correct Answer:** A

---

Question 344. A data engineer receives a successful response after submitting files to the `insertFiles` Snowpipe REST endpoint. What does this indicate?

A) The files have been accepted and queued for ingestion by Snowflake  
B) The files have already been loaded into the target table  
C) The files have been deleted from the stage  
D) The endpoint returned a summary of the loaded data  

**Correct Answer:** A

---

Question 345. After calling the `insertFiles` endpoint for Snowpipe, an architect sees a success message. What does this confirm?

A) Snowflake has received the files and will attempt to load them  
B) The files are immediately available in the destination table  
C) The files have failed to queue for ingestion  
D) The files were rejected due to format errors  

**Correct Answer:** A

---

Question 346. After enforcing network policies to restrict access to Private Link IP ranges in a Snowflake Azure account, users receive an error stating their IP is not allowed when logging in via SSO. What should the Architect check first to resolve this?

A) Ensure the SSO identity provider (ADFS) metadata service is also allowed by the network policy  
B) Disable SSO and use password authentication only  
C) Increase the allowed IP range to include public internet addresses  
D) Switch to OAuth authentication  

**Correct Answer:** A

---

Question 347. An Architect has confirmed that Private Link connectivity is working for direct logins, but SSO logins fail due to IP restrictions. What is a recommended next step?

A) Add the IP address ranges used by the SSO identity provider’s services to the Snowflake network policy  
B) Remove all network policies  
C) Add a new virtual warehouse  
D) Change DNS settings to point to public endpoints  

**Correct Answer:** A

---

Question 348. A company’s Snowflake account in Azure is set up with SAML SSO and Private Link. Users can log in with username/password, but receive “IP not allowed” errors with SSO. Which action should the Architect consider?

A) Update the network policy to include the IP addresses used by the SSO/SCIM provider and Azure AD Connect services  
B) Delete the network policy  
C) Use a different SSO provider  
D) Change the Private Link configuration to public access  

**Correct Answer:** A

---

Question 349. A company uses the Snowflake Connector for Kafka to ingest data into a Snowflake table. What will happen if a file generated by the connector cannot be loaded?

A) The connector will retry loading the file until it succeeds or reaches the maximum retry limit  
B) The file will be deleted and the data lost  
C) The connector will ignore the file and move on  
D) A manual intervention is required for each failed file  

**Correct Answer:** A

---

Question 350. While streaming data using the Snowflake Connector for Kafka, a file fails to load into Snowflake due to schema mismatch. What is the connector’s default behavior?

A) The connector retries the load operation according to its configuration  
B) The connector drops the failed file immediately  
C) The connector moves the file to a backup location  
D) No error is logged for failed loads  

**Correct Answer:** A

---

Question 351. An architect is troubleshooting ingestion issues with the Snowflake Connector for Kafka. If a file cannot be loaded into the Snowflake stage, what happens next?

A) The connector retries the operation, and if retries fail, it logs the error for further investigation  
B) The connector automatically deletes the file  
C) The connector marks the Kafka topic as completed  
D) The connector sends an alert to all users  

**Correct Answer:** A

---

Question 352. A data engineer is designing a real-time streaming solution for ingesting large volumes of data from various sources. Which technology is best described as a distributed event streaming platform for building real-time data pipelines and applications?

A) Kafka  
B) MySQL  
C) Hadoop  
D) Tableau  

**Correct Answer:** A

---

Question 353. A company wants to implement a scalable system that can publish and subscribe to streams of records in real time for analytics. Which of the following technologies fulfills this role?

A) Kafka  
B) Excel  
C) FTP  
D) SFTP  

**Correct Answer:** A

---

Question 354. A SYSADMIN creates several database objects and transfers OWNERSHIP to a custom role. The custom role is not in the SYSADMIN or SECURITYADMIN role hierarchy. What is the impact on SYSADMIN’s ability to manage those objects?

A) SYSADMIN loses the ability to manage the objects via ownership  
B) SYSADMIN can still drop and alter the objects  
C) SYSADMIN can grant itself access at any time  
D) SYSADMIN retains ownership regardless of hierarchy  

**Correct Answer:** A

---

Question 355. A custom role is granted OWNERSHIP of database objects, but is not assigned to SYSADMIN or SECURITYADMIN. What happens if SYSADMIN tries to modify privileges on these objects?

A) SYSADMIN will be unable to modify privileges or perform privileged operations  
B) SYSADMIN can always update privileges on any object  
C) SYSADMIN can forcibly take ownership at any time  
D) SYSADMIN can drop the objects without ownership  

**Correct Answer:** A

---

Question 356. A Snowflake Architect wants to understand the effect of transferring object ownership to a custom role outside the SYSADMIN/SECURITYADMIN hierarchy. What is the main consequence for those administrative roles?

A) They lose ownership and cannot manage the objects unless the custom role is added to their hierarchy  
B) They retain full management capabilities  
C) They can grant themselves privileges on the objects  
D) They can view but not modify the objects  

**Correct Answer:** A

---

Question 357. A data engineer uses a stream to track changes in a table but notices the stream has become stale after the table was truncated. What is the reason for this?

A) Truncating the source table invalidates the stream’s change tracking and causes it to become stale  
B) The table was not queried for a week  
C) The stream was dropped  
D) The table was renamed  

**Correct Answer:** A

---

Question 358. A financial services company discovers that the Data table in its Snowflake environment contains corrupted data due to a faulty ETL job. What is the most efficient command to recover the table to its state 5 minutes ago while preserving the original table for investigation?

A) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
C) DROP TABLE Data;  
D) SELECT * FROM Data WHERE date > current_timestamp() - interval '5 minutes';  

**Correct Answer:** B

---

Question 359. A data architect is building a monitoring solution for a high-volume Snowpipe implementation. The business wants to minimize API calls and ensure scalable tracking of file loading status. Which endpoint should be used, and what is the main benefit?

A) loadHistoryScan, because it allows for real-time data manipulation  
B) insertReport, because it provides efficient, scalable tracking of file load status with fewer API calls  
C) insertFiles, because it automatically triggers file ingestion  
D) insertHistory, because it gives historical data for all loads  

**Correct Answer:** B

---

Question 360. A company’s ETL solution checks whether files have been loaded into Snowflake using Snowpipe. The architect is concerned about API rate limits and performance. What is the recommended approach to optimize system load?

A) Periodically calling loadHistoryScan for each individual file  
B) Using insertFiles to batch file uploads  
C) Using insertReport, because it reduces the number of API calls required for load monitoring  
D) Implementing a custom notification system outside Snowflake  

**Correct Answer:** C

---

Question 361. During a Snowflake implementation, a business wants to optimize their Snowpipe monitoring strategy for cost and performance. What is a key benefit of using the insertReport endpoint over loadHistoryScan?

A) loadHistoryScan is required for external table refresh  
B) loadHistoryScan allows direct data manipulation in the stage  
C) insertReport provides batch status for multiple files in a single call, improving efficiency  
D) insertReport increases operational costs due to excessive API usage  

**Correct Answer:** C

---

Question 362. An Architect needs to grant a group of ORDER_ADMIN users the ability to clean old data in an ORDERS table (deleting all records older than 5 years), without granting any privileges on the table. The group’s manager (ORDER_MANAGER) has full DELETE privileges on the table.  
How can the ORDER_ADMIN role be enabled to perform this data cleanup, without needing the DELETE privilege held by the ORDER_MANAGER role?

A) Grant the ORDER_ADMIN role the DELETE privilege directly on the ORDERS table  
B) Create a stored procedure owned by ORDER_MANAGER that performs the cleanup, and grant EXECUTE on the procedure to ORDER_ADMIN  
C) Assign the ORDER_MANAGER role to ORDER_ADMIN users during the cleanup  
D) Share the table with ORDER_ADMIN using a secure view and allow deletes through the view  

**Correct Answer:** B

---

Question 363. A retailer schedules a daily Snowflake task to run at 2:00 AM local time. After daylight savings time ends, the business notices a shift in execution timing. How does Snowflake handle task scheduling with respect to daylight savings changes?

A) The task automatically adjusts to UTC and ignores local time  
B) The task continues to run at the defined local time, adapting to daylight savings changes  
C) The task skips execution on the day the time changes  
D) The task fails and requires manual rescheduling after time changes  

**Correct Answer:** B

---

Question 364. A global company operates Snowflake tasks scheduled for local time. When daylight savings time begins, what is the expected behavior for these tasks?

A) Tasks are paused until the administrator updates the schedule  
B) Tasks run at the same designated local time, regardless of daylight savings adjustments  
C) Tasks may run twice on the day of the change  
D) Tasks switch to a fixed UTC time and no longer represent local time  

**Correct Answer:** B

---

Question 365. An architect is asked to ensure Snowflake tasks always run at 1:30 AM local time, even during daylight savings transitions. What does Snowflake do when daylight savings time shifts?

A) The task execution time becomes inconsistent and may drift  
B) The task runs at the same local time, automatically adjusting for daylight savings changes  
C) The task executes based on server time, not user-configured time  
D) The task needs to be manually adjusted each time daylight savings starts or ends  

**Correct Answer:** B

---

Question 366. A retail company schedules a daily Snowflake task to run at 3:00 AM local time. After daylight savings ends, the business notices a change in execution time. How does Snowflake handle this shift in local time for scheduled tasks?

A) The task keeps running at the same UTC time, ignoring local time changes  
B) The task automatically adjusts and runs at the scheduled local time, accounting for daylight savings changes  
C) The task fails and requires manual rescheduling after the time change  
D) The task skips execution on the day of the change  

**Correct Answer:** B

---

Question 367. A multinational organization operates Snowflake tasks scheduled for local time. When daylight savings begins, what is the expected behavior for these tasks?

A) Tasks switch to a fixed UTC schedule and no longer represent local time  
B) Tasks are paused until the administrator updates the schedule  
C) Tasks continue to run at the same designated local time, adapting automatically for daylight savings  
D) Tasks may run twice on the day of the change  

**Correct Answer:** C

---

Question 368. An architect is asked to ensure Snowflake tasks always run at 1:30 AM local time, even during daylight savings transitions. What will Snowflake do when daylight savings time shifts?

A) The task execution time becomes inconsistent and may drift  
B) The task executes based on server time, not the user-configured local time  
C) The task needs to be manually adjusted each time daylight savings starts or ends  
D) The task runs at the same local time, automatically adjusting for daylight savings changes  

**Correct Answer:** D

---

Question 369. A media company wants to continuously ingest customer review data into Snowflake, perform sentiment analysis using Amazon Comprehend, de-identify the data, and share it publicly for advertising companies on different cloud providers. Which design best minimizes operational complexity and infrastructure maintenance?

A) Build a custom ETL pipeline using self-managed EC2 instances and schedule data loads  
B) Use Snowpipe for continuous ingestion, external functions to call Amazon Comprehend, and secure views for public data sharing  
C) Use third-party ETL tools hosted on-premises for ingestion and transformation  
D) Set up a dedicated Kubernetes cluster to manage ingestion, transformation, and sharing  

**Correct Answer:** B

---

Question 370. A Snowflake architect must design a solution for ingesting data triggered by object storage event notifications, with minimal platform management and easy integration with external sentiment analysis. Which approach should be recommended?

A) Build and manage a serverless Lambda pipeline with custom code for data movement  
B) Leverage Snowpipe for event-driven ingestion, Snowflake external functions for Amazon Comprehend, and secure data sharing for cross-cloud access  
C) Use cron jobs to batch load and process reviews  
D) Create manual upload processes for each advertising client  

**Correct Answer:** B

---

Question 371. To ensure efficiency and scalability as new customer reviews arrive, while minimizing development and upgrade effort, what Snowflake feature should be used for ingesting data from object storage?

A) Manually run ETL scripts periodically  
B) Snowpipe, which supports continuous and automated data ingestion triggered by event notifications  
C) Streamlit app for direct data upload  
D) ODBC connection with polling  

**Correct Answer:** B

---

Question 372. How many files can the COPY INTO operation load as the maximum when providing a discrete list of files?

A) 500  
B) 1000  
C) 2000  
D) 250  

**Correct Answer:** B

---

Question 373. When data is transferred from a Snowflake primary account to another target account using database replication, which account is billed for the data transfer and compute charges?

A) Both the primary and target accounts equally  
B) The target account  
C) The primary account  
D) Neither account; charges are covered by Snowflake  

**Correct Answer:** C

---

Question 374. A Data Architect is importing customer data in JSON format from an external stage into a Snowflake table with a VARIANT column. Occasionally, malformed JSON causes parsing errors, leading to failed imports. What function should the Architect use in the COPY INTO command to set the column to NULL when a parsing error occurs?

A) TRY_TO_DATE  
B) TRY_PARSE_JSON  
C) TO_VARIANT  
D) PARSE_JSON  

**Correct Answer:** B

Question 375. During a Snowflake migration project, an Architect needs to ensure that any VARIANT column in a target table is set to NULL if the source JSON data is malformed during a bulk COPY INTO operation. Which function provides this capability?

A) TO_JSON  
B) TO_OBJECT  
C) TRY_PARSE_JSON  
D) TO_VARIANT  

**Correct Answer:** C

---

Question 376. A media company’s Data Architect wants to avoid import failures caused by malformed JSON when loading review data from cloud storage. The solution should automatically set the VARIANT column to NULL for any rows with parsing errors. Which function should be used in the data pipeline?

A) PARSE_JSON  
B) TO_VARIANT  
C) TO_OBJECT  
D) TRY_PARSE_JSON  

**Correct Answer:** D

---

Question 377. A Data Architect is importing JSON data from an external stage into a table with a VARIANT column using the COPY INTO command. During testing, the Architect discovers that the import sometimes fails, with parsing errors, due to malformed JSON values. The Architect decides to set the VARIANT column to NULL when a parsing error is encountered.

A) TO_VARIANT  
B) TRY_PARSE_JSON  
C) TO_OBJECT  
D) PARSE_JSON  

**Correct Answer:** B

---

Question 378. Which pipes are cloned when cloning a database or schema?

A) Only pipes with active data streams  
B) Pipes whose stage references are also cloned in the same operation  
C) All pipes, regardless of stage references  
D) No pipes are cloned during database or schema cloning  

**Correct Answer:** B

---

Question 379. WA company needs to have the following features available in its Snowflake account:

1. Support for Multi-Factor Authentication (MFA)  
2. A minimum of 2 months of Time Travel availability  
3. Database replication in between different regions  
4. Native support for JDBC and ODBC  
5. Customer-managed encryption keys using Tri-Secret Secure  
6. Support for Payment Card Industry Data Security Standards (PCI DSS)  

In order to provide all the listed services, what is the MINIMUM Snowflake edition that should be selected during account creation?

A) Standard Edition  
B) Enterprise Edition  
C) Business Critical Edition  
D) Premier Edition  

**Correct Answer:** C

---

Question 380. A retail company uses Snowflake to load sales transaction files daily. The architect discovers that after a file is loaded and its metadata has expired, the same file needs to be reloaded for a reconciliation process. Which method should be used to reload the file?

A) Change the file name to a new one and reload  
B) Copy the file to a different location or rename it, then execute the COPY command again  
C) Re-enable metadata tracking for the file  
D) Use the REFRESH keyword in the COPY command  

**Correct Answer:** B

---

Question 381. A data engineering team needs to reload a file into Snowflake, but the file’s load metadata has expired. What is a practical way to ensure the file can be loaded again?

A) Use the OVERWRITE option in the COPY command  
B) Rename or move the file in the stage so Snowflake treats it as a new file  
C) Run COPY INTO with FORCE=true  
D) Restore the metadata from backup  

**Correct Answer:** B

---

Question 382. During a data audit, an architect is asked to reload a previously loaded file, but the metadata that tracks the load has expired. What action should be taken to successfully reload the file into Snowflake?

A) Set the file retention policy to unlimited  
B) Rename the file or move it to a different location in the stage before reloading  
C) Use the RECOVER FILE command  
D) Request manual metadata reset from Snowflake support  

**Correct Answer:** B

---

Question 383. A manufacturing company’s Data Engineering team needs to provide raw data for the Data Science team and secure, curated data for the Sales team, while also supporting reporting and visualization for Finance and Vendor Management. Which data modeling approach best supports all these requirements?

A) Store all data in a single, flat table accessible to every team  
B) Implement a multi-layered architecture with raw, curated, and presentation layers  
C) Use only secure views for all users, regardless of their needs  
D) Build a star schema with only aggregated data  

**Correct Answer:** B

---

Question 384. The Sales team at a manufacturing company requires access to engineered and protected data for monetization, while the Data Science team needs raw data for model development. How should the Data Engineering team structure their Snowflake data models to support both use cases?

A) Only provide access to aggregated reporting tables  
B) Use a multi-zone architecture, separating raw, transformed, and shared data zones  
C) Grant full access to all tables for all teams  
D) Build one large fact table with all attributes exposed  

**Correct Answer:** B

---

Question 385. A Snowflake architect is tasked with supporting diverse analytics requirements, including detailed reporting, raw data exploration, and secure data sharing for monetization. What approach should be used to organize the data in Snowflake?

A) Create a flat wide table for all reporting and analytics  
B) Use a layered modeling approach, including staging, core, and presentation layers  
C) Only store data in the stage and allow users to query directly  
D) Partition data by team and restrict cross-team sharing  

**Correct Answer:** B

---

Question 386. A Snowflake Architect notices that the destination table has significantly more data than expected after setting up a COPY INTO command to continuously load data from an external stage. What is a likely reason for this?

A) The COPY INTO command is ignoring duplicate files  
B) The same files are being loaded multiple times because file load metadata is not being tracked correctly  
C) The data in the external stage is changing format frequently  
D) The COPY INTO command is only partially loading each file  

**Correct Answer:** B

---

Question 387. After configuring a continuous data load from an external stage with COPY INTO, an Architect realizes that the target table size is unexpectedly large. What could be causing this issue?

A) The COPY INTO command is skipping some files  
B) The external stage is storing compressed files  
C) The same files are being reloaded repeatedly due to expired or missing file load metadata  
D) The destination table has incorrect clustering  

**Correct Answer:** C

---

Question 388. A Snowflake Architect implements a COPY INTO command for continuous loading, but the table accumulates far more rows than anticipated. What is a possible root cause?

A) The COPY INTO command is filtering out too much data  
B) The external stage has files with duplicate names but different contents  
C) Files are repeatedly loaded because the metadata tracking which files have been loaded has expired or is not functioning properly  
D) The table is set to auto-scale  

**Correct Answer:** C

---

Question 389. A data engineering team is organizing files into logical paths in a Snowflake stage to improve data management and query efficiency. What additional parameter does Snowflake recommend adding to the COPY INTO command to optimize file selection?

A) OVERWRITE  
B) PATTERN  
C) FILE_FORMAT  
D) FORCE  

**Correct Answer:** B

---

Question 390. A Snowflake Architect is tasked with optimizing the ingestion of files that are organized into logical directories within a stage. Which parameter should be included in the COPY INTO command to target specific files based on naming conventions?

A) VALIDATE  
B) PATTERN  
C) ON_ERROR  
D) SIZE_LIMIT  

**Correct Answer:** B

---

Question 391. A retail company wants to efficiently load only sales data files from a large stage containing various types of data. When organizing files into logical paths, which parameter is most helpful for specifying which files to include during the load process?

A) HEADER  
B) PATTERN  
C) MAX_FILE_SIZE  
D) PARSE_JSON  

**Correct Answer:** B

---

Question 392. A Snowflake architect is designing a solution that uses external tables for data sourced from cloud storage. The architect needs to implement row-level security for different business units. Which statement correctly describes how row access policies can be applied to external tables?

A) Row access policies cannot be applied to external tables  
B) Row access policies can be applied directly to external tables to control access based on user attributes  
C) External tables require column masking policies instead of row access policies  
D) Row access policies are only available for views, not tables  

**Correct Answer:** B

---

Question 393. A Data Engineering team has created several external tables to support cross-departmental analytics. They want to restrict access to specific rows based on user roles. What must the architect know about applying row access policies to external tables?

A) Row access policies automatically apply to all referenced objects  
B) Row access policies can be applied directly to external tables, enabling granular access control  
C) Row access policies must be applied at the stage level, not table level  
D) External tables must be converted to regular tables before applying row access policies  

**Correct Answer:** B

---

Question 394. A financial institution is leveraging external tables in Snowflake to ingest large amounts of transactional data. How can row access policies be used to manage sensitive data exposure in these tables?

A) Row access policies are not compatible with external tables  
B) Row access policies can be applied to external tables, allowing dynamic filtering of rows based on user context  
C) Row access policies can only be used on internal Snowflake tables  
D) Row access policies require manual enforcement outside of Snowflake  

**Correct Answer:** B

---

Question 395. A Snowflake Architect wants to use external functions to invoke a third-party REST API for data enrichment. What is one limitation they should be aware of when designing this solution?

A) External functions can only call internal Snowflake services  
B) External functions have latency and timeout limits when interacting with remote endpoints  
C) External functions automatically retry failed requests indefinitely  
D) External functions support unlimited concurrent executions  

**Correct Answer:** B

---

Question 396. A Data Engineering team is considering using external functions for data processing. What is a constraint related to permissions and network access that they must keep in mind?

A) External functions can be executed without any network configuration  
B) External functions require an integration object that manages network connectivity and security  
C) External functions are only available for users with ACCOUNTADMIN privileges  
D) External functions are exempt from Snowflake’s role-based access control  

**Correct Answer:** B

---

Question 397. A financial services company plans to use external functions to call APIs hosted outside Snowflake. Which of the following is a technical limitation they must consider?

A) External functions can return any data type, including complex nested JSON  
B) External functions cannot return large result sets due to payload size restrictions  
C) External functions can only be triggered via manual user commands  
D) External functions can access on-premises resources directly without a secure tunnel  

**Correct Answer:** B

---

Question 398. Which of the following is a limitation of Snowflake external functions?

A) They can be used to write both functions and stored procedures  
B) They can only be used to write functions, not stored procedures  
C) They can directly access on-premises databases  
D) They can execute in parallel without any restrictions  

**Correct Answer:** B

---

Question 399. Person1 is currently using the SECURITYADMIN role in Snowflake. After creating a new role named DBA_ROLE to manage warehouses, what command should Person1 execute to switch the worksheet context to DBA_ROLE?

A) ALTER ROLE DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) SET ROLE DBA_ROLE;  
D) SWITCH ROLE DBA_ROLE;  

**Correct Answer:** B

---

Question 400. A Snowflake administrator creates a role called DBA_ROLE for warehouse management. What is the correct SQL command Person1 should use to activate DBA_ROLE for their current session?

A) GRANT ROLE DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) ACTIVATE ROLE DBA_ROLE;  
D) CHOOSE ROLE DBA_ROLE;  

**Correct Answer:** B

---

Question 401. After creating a new custom role in Snowflake, which command allows a user to change their worksheet context to the newly created role?

A) SELECT DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) SWITCH TO DBA_ROLE;  
D) EXEC DBA_ROLE;  

**Correct Answer:** B

---

Question 402. Which command can we use to convert JSON NULL values to SQL NULL values?

A) TO_VARIANT  
B) NULLIF  
C) TRY_PARSE_JSON  
D) TO_JSON  

**Correct Answer:** B

---

Question 403. Which ALTER commands will impact a column's availability in Time Travel?

A) ALTER TABLE ... RENAME COLUMN  
B) ALTER TABLE ... DROP COLUMN  
C) ALTER TABLE ... MODIFY COLUMN COMMENT  
D) ALTER TABLE ... SET DEFAULT  

**Correct Answer:** B

---

Question 404. A streaming data engineering team has set up the Snowflake Kafka Connector and subscribed it to multiple Kafka topics. However, the topics have not yet been mapped to any Snowflake tables. What behavior should the team expect from the Kafka Connector?

A) The Kafka Connector will automatically create tables for each topic  
B) The Kafka Connector will not ingest data from the Kafka topics until they are mapped to Snowflake tables  
C) The Kafka Connector will send an error message to the Kafka cluster  
D) The Kafka Connector will load data into a default Snowflake table  

**Correct Answer:** B

---

Question 405. An organization’s Kafka Connector is subscribed to several Kafka topics, but those topics have not been mapped to destination tables in Snowflake. What will happen to the data streamed to those topics?

A) The data will be ingested into Snowflake using inferred schemas  
B) The data will not be loaded into Snowflake until the topics are mapped to tables  
C) The connector will store the data locally until mapping is provided  
D) The connector will merge all data into a single staging table  

**Correct Answer:** B

---

Question 406. A Snowflake Architect configures the Kafka Connector for real-time ingestion but omits mapping between Kafka topics and Snowflake tables. What is the connector’s response to incoming messages on these topics?

A) The connector creates temporary tables for unmapped topics  
B) The connector ignores incoming messages from topics that are not mapped to Snowflake tables  
C) The connector logs a warning and continues running  
D) The connector attempts to auto-map topics to tables using topic names  

**Correct Answer:** B

---

Question 407. Which system functions does Snowflake provide to monitor clustering information within a table? (Choose two.)

A) SYSTEM$CLUSTERING_DEPTH  
B) SYSTEM$CLUSTERING_INFORMATION  
C) SYSTEM$CLUSTERING_RATIO  
D) SYSTEM$CLUSTERING_METADATA  

**Correct Answers:** B, C

---

Question 408. After how many days does the load activity of the COPY INTO command and Snowpipe in the Information Schema expire?

A) 1 day  
B) 14 days  
C) 7 days  
D) 30 days  

**Correct Answer:** B

---

Question 409. What two requirements are necessary for the remote service to be called by the Snowflake external function? (Choose two.)

A) The remote service must be hosted on-premises  
B) The remote service must be accessible via a network integration  
C) The remote service must accept and respond to HTTPS requests  
D) The remote service must use the Snowflake-provided API gateway  

**Correct Answers:** B, C

---

Question 410. A Data Engineer is setting up Snowpipe for auto-ingest of event logs from Amazon S3. The Snowflake Architect wants to ensure the minimum required privileges are granted to the Snowpipe user. Which privileges must be assigned for the user to execute Snowpipe?

A) OWNERSHIP on the stage and table  
B) USAGE on the stage and INSERT on the target table  
C) SELECT on the target table and USAGE on the stage  
D) MONITOR on the Snowpipe and INSERT on the target table  

**Correct Answer:** B

---

Question 411. An organization is configuring Snowpipe for automated data ingestion from S3. What are the minimum object privileges the Snowpipe user must have to successfully execute Snowpipe?

A) CREATE PIPE on the database  
B) USAGE on the external stage and INSERT on the destination table  
C) USAGE on the database and SELECT on the table  
D) OWNERSHIP on the pipeline and MONITOR on the stage  

**Correct Answer:** B

---

Question 412. In a near real-time ingestion scenario using Snowpipe, a Data Engineer is configuring access control. Which combination of privileges is the minimum required to load data with Snowpipe?

A) USAGE on the schema and INSERT on the database  
B) USAGE on the stage and INSERT on the target table  
C) SELECT on the stage and OWNERSHIP on the table  
D) MONITOR on the Snowpipe and USAGE on the stage  

**Correct Answer:** B

---

Question 413. A global healthcare organization is migrating sensitive patient data to Snowflake. The company must comply with strong legal isolation requirements due to regional privacy laws, but also wants to support multiple tenants in a single Snowflake account for cost efficiency. Which tenancy strategy should the Snowflake Architect recommend if RBAC is an acceptable method for isolation?

A. Use a single schema for all tenants and control access with masking policies  
B. Create a separate schema and dedicated role for each tenant, using RBAC to restrict access  
C. Deploy separate Snowflake accounts for each tenant  
D. Store all tenant data in a single table and filter with row access policies  

**Answer:** B

---

Question 414. A financial services provider is onboarding multiple clients (tenants) to a Snowflake-powered analytics platform. Each client’s data must be strictly isolated for legal compliance, but the company wishes to minimize operational overhead. Which solution best balances strong legal isolation with efficient multi-tenancy, assuming RBAC is sufficient?

A. Use a separate Snowflake account for each client  
B. Use a single account with one schema for all clients and apply data masking  
C. Use a single account, with separate schemas and unique roles for each client, enforced by RBAC  
D. Use one centralized role with row access policies across all schemas  

**Answer:** C

---

Question 415. A SaaS provider is building a multi-tenant application on Snowflake. Legal regulations require that no tenant can access another tenant’s data. The architect wants to keep costs low and management simple, and RBAC is confirmed to meet regulatory needs. What is the best design pattern to satisfy these requirements?

A. Deploy a dedicated Snowflake account for each tenant  
B. Use a single Snowflake account with a dedicated schema and role for each tenant  
C. Store all tenant data in a single schema and use masking policies  
D. Use a single account with all tenant data in one table and control access via views  

**Answer:** B

---

Question 416. A retail company receives transaction files from stores every 10 seconds into an external stage on Snowflake. The files are between 500 K and 3 MB. Store managers need to see the most recent sales data immediately on their dashboards. What is the best solution with the least coding effort?

A. Develop a custom ETL pipeline in Python to load each file as it arrives  
B. Create an external table on the stage and allow dashboards to query it directly  
C. Use Snowpipe to automatically ingest the files into an internal table  
D. Schedule a batch job to load files every hour  

**Answer:** B, C

---

Question 417. A logistics company’s proprietary system drops shipment files every 10 seconds to an S3 bucket, which is configured as an external stage in Snowflake. Dashboards must reflect new shipments immediately. What two Snowflake features will provide the needed data accessibility with minimal development?

A. Manual bulk loading using COPY INTO statements  
B. Snowpipe for continuous, automated ingestion  
C. External tables for direct querying of staged data  
D. Stream and task for micro-batch processing every 5 minutes  

**Answer:** B, C

---

Question 418. A financial institution receives small data files in an external stage every 10 seconds from a proprietary system. Compliance dashboards require instant access to the newest data. As a Snowflake Architect, which approaches allow for rapid data availability with the least coding? Choose 2

A. Build a custom lambda function to ingest files  
B. Set up an external table on the stage for direct queries  
C. Configure Snowpipe for near real-time ingestion  
D. Periodically aggregate the data with a scheduled task  

**Answer:** B, C

---

Question 419. A Snowflake Architect notices that a query takes 30 minutes to run, with 24 minutes spent on compilation. The business requires faster turnaround, but increasing the virtual warehouse size is not an option. What can the Architect do to improve query performance?

A. Increase the size of the virtual warehouse  
B. Refactor the query to reduce complexity and enable better pruning  
C. Break the query into smaller, modular subqueries or use intermediate tables  
D. Run the query more frequently to leverage result caching  

**Answer:** B

---

Question 420. A data analytics team is frustrated because their long-running queries spend most of the time in the compilation phase, not execution. The team cannot upgrade to a larger warehouse due to cost constraints. Which strategy should the Snowflake Architect recommend to reduce compilation times?

A. Add more compute clusters to the warehouse  
B. Simplify the query by reducing the number of joins and subqueries  
C. Increase the cache retention period  
D. Use clustering keys on the underlying tables  

**Answer:** B

---

Question 421. A retail company’s reporting query spends 24 of its 30 minutes runtime in compilation. The Architect is asked to reduce query time but must keep the warehouse size unchanged. What is the most effective action?

A. Use a larger virtual warehouse for the query  
B. Rewrite the query to simplify nested logic and break up large statements  
C. Add more partitions to the tables  
D. Schedule the query during off-peak hours  

**Answer:** B

---

Question 422. An e-commerce company uses Snowpipe for continuous ingestion from an external stage. After several days, the Data Architect needs to change the external stage definition to point to a new S3 bucket. What is the recommended approach?

A. Delete the pipe and create a new one with the updated stage definition  
B. Suspend the pipe, modify the external stage definition, and then resume the pipe  
C. Modify the external stage directly and notify downstream users of the change  
D. Drop the stage, recreate it with the new definition, and ensure the pipe references the updated stage  

**Answer:** B

---

Question 423. A healthcare analytics team relies on Snowpipe for real-time file ingestion from an external stage. The team needs to update the external stage to use a different cloud storage location. What steps should the Data Architect follow to ensure minimal disruption?

A. Leave the pipe running while modifying the stage  
B. Suspend the pipe, update the external stage definition, then resume the pipe  
C. Create a new pipe for the new stage and deprecate the old one  
D. Run a manual COPY INTO command after updating the stage  

**Answer:** B

---

Question 424. A financial services firm has configured Snowpipe for daily ingestion from a referenced external stage. The Data Architect needs to update the pipe definition due to changes in the external stage. What is the best practice to follow?

A. Remove all files from the stage before making changes  
B. Suspend the pipe, update the stage, resume the pipe, and validate ingestion  
C. Make changes to the stage while the pipe is active  
D. Recreate the pipe from scratch after stage changes  

**Answer:** B

---

Question 425. A financial services Architect needs to retain quarter-end financial results for the previous six years in Snowflake. Which feature should the Architect use to accomplish this requirement?

A. Configure Time Travel for all financial tables  
B. Create zero-copy clones of the tables at each quarter-end and retain them  
C. Rely on the Fail-safe feature to access historical data  
D. Set up automatic data retention policies for six years  

**Answer:** B

---

Question 426. A user needs to change object parameters in Snowflake. Which roles allow a user to make these changes? (Choose 2)

A. SYSADMIN  
B. PUBLIC  
C. ACCOUNTADMIN  
D. SECURITYADMIN  

**Answers:**  
A. SYSADMIN (Correct)  
C. ACCOUNTADMIN (Correct)

---

Question 427. A retail analytics team has created two views on their sales data in the same schema: one is a Secure View, and the other is a standard View. When running queries against both, they notice that the query profiler displays different information for each. What best explains this behavior?

A. The Secure View automatically filters sensitive data columns  
B. The Secure View does not expose underlying query details in the profiler for security reasons  
C. The standard View is using cached results  
D. There is a difference in the data contained in each view  

**Answer:** B

---

Question 428. A financial institution has two views on transaction data: one is a Secure View, and the other is a regular View. Upon analysis, the architect observes that the query profiler for the Secure View discloses less information about the underlying query execution compared to the standard View. Why does this occur?

A. Secure Views are always materialized, standard Views are not  
B. Secure Views mask query details in the profiler for enhanced security  
C. Standard Views require more permissions to access profiler details  
D. Secure Views display performance metrics in a separate dashboard  

**Answer:** B

---

Question 429. A healthcare architect notices that when users query both a Secure View and a standard View with identical data, the query profiler shows less detail for one of the views. What is the reason for this difference?

A. Secure Views are faster and skip profiler tracking  
B. Secure Views intentionally limit profiler information to protect logic and sensitive data  
C. Standard Views are limited by user roles  
D. Secure Views run queries in a different warehouse  

**Answer:** B

---

Question 430. A Snowflake Architect is reviewing the output of `SYSTEM$CLUSTERING_INFORMATION` for a large table and notices the metric `average_overlaps`. What does `average_overlaps` refer to?

A. The average number of clusters in the table  
B. The average number of times a database user has queried the table  
C. The average number of micro-partitions that contain overlapping values for the clustering key columns  
D. The average number of rows in each micro-partition  

**Answer:** C

---

Question 431. A Snowflake Architect is designing a large table and wants to optimize cluster key selection. Which steps are recommended best practices for prioritizing cluster keys in Snowflake? (Choose two.)

A. Select columns that are frequently used in filtering and join conditions  
B. Choose columns with high cardinality and minimal null values  
C. Pick columns that are rarely queried to minimize overhead  
D. Use columns with complex data types such as VARIANT or OBJECT  

**Answers:**  
B. Choose columns with high cardinality and minimal null values (Correct)  
A. Select columns that are frequently used in filtering and join conditions (Correct)

---

Question 432. A Snowflake Architect accidentally drops a data share. Which command can be used to restore the dropped share?

A. ALTER SHARE ... UNDROP  
B. RECOVER SHARE ...  
C. UNDROP SHARE ...  
D. RESTORE SHARE ...  

**Answer:** C

---

Question 433. A company’s daily Snowflake workload consists of a huge number of concurrent queries triggered between 9pm and 11pm. At the individual level, these queries are smaller statements that get completed within a short time period.

What configuration can the company’s Architect implement to enhance the performance of this workload? (Choose two.)

A. Increase the size of the virtual warehouse to the largest available  
B. Enable multi-cluster warehouses with auto-scaling to handle concurrency  
C. Use a multi-cluster warehouse and set the minimum cluster count greater than one  

---

Question 434. Before declaring the disaster recovery solution operational, what should the architect do to validate business continuity?

A) Promote the failover group in the secondary account and test data access and integrity  
B) Archive the database in cold storage  
C) Set up daily backups using external tables  
D) Increase the replication frequency to every minute  

**Correct Answer:** A

---

Question 435. An analytics team migrated a legacy reporting system to Snowflake. Their queries now execute directly against large fact tables rather than precomputed summary tables, and they want to speed up performance and reduce compute costs without changing the query SQL or the reporting tool connections. Which Snowflake feature is best suited to address this requirement?

A) Clustering keys  
B) Materialized Views  
C) Query Result Caching  
D) Data Masking  

**Answer: B) Materialized Views**

---

Question 436. The analytics team wants to reduce compute costs and improve query speed for their migrated reporting system in Snowflake, but they do not want to alter the queries or require users to connect to different tables. Which feature can transparently accelerate repeated queries with identical results without any changes on the client side?

A) Zero-Copy Cloning  
B) Query Result Caching  
C) External Tables  
D) Time Travel  

**Answer: B) Query Result Caching**

---

Question 437. After migrating to Snowflake, the analytics team notices that some queries against large fact tables could be further optimized. They are not allowed to modify the original SQL or change the reporting tool’s data sources. Which approach should the architect recommend to optimize query performance in this scenario?

A) Repartitioning the data  
B) Creating materialized views  
C) Increasing the warehouse size  
D) Optimizing clustering keys  

**Answer: D) Optimizing clustering keys**

---

Question 438. A global retailer wants to analyze daily sales reports stored as CSV files in their cloud storage (AWS S3). They need to query these files directly in Snowflake, enriching the data with the source file name and row number for auditing purposes. What is the most architecturally sound solution to enable this capability?

A) Load the files into an internal Snowflake table using the COPY INTO command and add custom columns for file name and row number during ETL.  
B) Create an external stage pointing to the S3 bucket, and then define an external table in Snowflake that includes metadata columns for filename and file row number.  
C) Use Snowpipe to continuously load the data into a permanent table and include file metadata in a separate mapping table.  
D) Manually parse files outside Snowflake, append metadata, and upload them to a Snowflake internal stage.

**Answer:** B) Create an external stage pointing to the S3 bucket, and then define an external table in Snowflake that includes metadata columns for filename and file row number.

---

Question 439. A financial services company needs to ensure that sensitive customer data stored in external files is not exposed to unauthorized users when queried via Snowflake external tables. As a Snowflake Architect, which approach should you recommend to satisfy both security and business requirements?

A) Grant SELECT privileges on the external table to all users so they can access the data as needed.  
B) Use row access policies in Snowflake to restrict data visibility based on user attributes, combined with secure external stages.  
C) Allow unrestricted access to the cloud storage bucket and enforce security only at the Snowflake role level.  
D) Copy the data into transient tables and delete them after use to limit exposure.

**Answer:** B) Use row access policies in Snowflake to restrict data visibility based on user attributes, combined with secure external stages.

from here re-number starting at 440
 

Question 524
### 6. Which Snowflake sharing option allows you to manage a group of accounts and offer a share to that group?  (1) 
A) Data Exchange  
B) Direct Share  
C) Data Listing  
D) Clean Room  
E) Secure View  
Ans A

Question: 42
True or false: Snowflake enforces unique, primary key, and foreign key constraints during DML
operations.
A. True
B. False
Answer: B
Explanation:
Reference: https://docs.snowflake.com/en/sql-reference/constraints-overview.html
Question: 43
Which of the following items does the Cloud services Layer manage?
Choose 4 answers
A. user authentication
B. Metadata
C. Query compilation and optimization
D. external blob storage
E. Data security
Answer: A, B, C, E
Explanation:

Question: 44
Which statement best describes '' clustering''?
A. Clustering represents the way data is grouped together and stored within snowflake's micropartitions
B. The database administrator must define the clustering methodology for each Snowflake table.
C. The clustering key must be included on the COPV command when loading data into Snowflake.
D. Clustering can be disabled within a Snowflake account.
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html
Question: 45
Which of the following commands sets the Virtual Warehouse for a session?
A. COPT WAREHOUSE FROM <<Config file> ;
B. SET warehouse = <<warehouse name>>;
C. USE WAREHOUSE <<warehouse name>>;
D. USE VIRTUAL_WAREHOUSE <<warehouse name>>;
Answer: C
"Scale out by adding warehouses to a multi-cluster warehouse"
Question: 51
Which are true of Snowflake roles?
A. All grants to objects are given to roles, and never to users
B. In order to do DML/DOL, a user needs to have selected a single role that has that specific access to the object and operation
C. The public role controls at other roles
D. Roles are a subset of users and users own objects In Snowflake
Answer: A, B
Explanation:
Question: 56
True or False: You can define multiple columns within a clustering key on a table.
A. True
B. False
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/tables-clustering-keys.html
Question: 57
True or false: it is best practice to define a clustering key on every table.
A. True
B. False
Answer: B
Explanation:
Reference: https://dwgeek.com/how-to-create-snowflake-clustered-tables-examples.html/
Question: 58
True or False: Each worksheet in the Snowflake Web Interface (UI) can be associated with different roles, databases, schemas, and Virtual Warehouses.
A. True
B. False
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/ui-worksheet.html
 
Question: 60
What is the lowest Snowflake edition that offers Time Travel up to 90 days?
A. standard Edition
B. Premier Edition
C. Enterprise Edition
D. Business Critical Edition
Answer: C
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-availability.html
Question: 61
True or False: Once created, a micro-partition will never be changed.
A. True
B. False
Answer: A
Explanation:
Reference: https://interworks.com/blog/kbridges/2019/03/12/time-travel-with-snowflake/
Question: 62
True or False: A third-party tool that supports standard JDBC or ODBC but has no Snowflake-specific driver will be unable to connect to Snowflake.
A. True
B. False
Answer: B
Explanation:
https://docs.snowflake.com/en/user-guide/jdbc.html
Snowflake provides a JDBC type 4 driver that supports core JDBC functionality. The JDBC driver must be installed in a 64-bit environment and requires Java 1.8 (or higher). The driver can be used with
most client tools/applications that support JDBC for connecting to a database server.
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/tables-clustering-keys.html
Question: 64
Which of the following are options when creating a virtual Warehouse?
Choose 2 answers
A. Auto-drop
B. Auto resize
C. Auto-resume
D. Auto-suspend
Answer: C, D
Explanation:
Reference: https://help.pentaho.com/Documentation/9.1/Products/Create_Snowflake_warehouse
Reference: https://www.analytics.today/blog/what-is-a-snowflake-virtual-warehouse
Question: 67
Increasing the size of a Virtual Warehouse from an X-Small to an X-Large is an example of:
A. Scaling rhythmically
B. Scaling max
C. Scaling out
D. Scaling up
Answer: D
Question: 70
Which of the following commands are not blocking operations?
Choose 2 answers
A. UPDATE
B. INSERT
C. MERGE
D. COPY
Answer: B, D
Explanation:
Question: 71
What parameter controls if the Virtual warehouse starts immediately after the CREATE WAREHOUSE
statement?
Select one.
A. INITTIALLY_SUSPENDED = TRUE/FALSE
B. START_AFTCR_CREATE = TRUE/FALSE
C. START_TTIME = 60 // (seconds from now)
D. START.TIME = CURRENT.DATE()
Answer: A
Explanation:
Question: 72
When should you consider disabling auto-suspend for a Virtual Warehouse?
Choose 2 answers
A. When users will be using compute at different times throughout a 24/7 period
B. When managing a steady workload
C. When the compute must be available with no delay or lag time
D. When you don’t want to have to manually turn on the Warehouse each time a user needs it
Answer: B, C
Question: 75
Which formats are supported for unloading data from Snowflake?
Choose 2 answers
A. Delimited (CSV, TSV, etc.)
B. Avro
C. JSON
D. ORC
Answer: A, C
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-unload-prepare.html
Question: 76
True or False: When data share is established between a Data Provider and a data Consumer, the Data Consumer can extend that data share to other Data Consumers.
A. True
B. False
Answer: B
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-sharing-intro.html

Question: 53
which of the following are valid approaches to loading data into a snowflake table? select all the below that apply.
A. Bulk copy from an External Stage
B. Continuous load using Snowpipe REST API
C. The Snowflake Web Interface (UT) data loading wizard
D. Bulk copy from an Internal Stage
Answer: A, B, C

-------------------------------------------------- Start 
Explanation:
Question: 78
What is the most granular object that the Time Travel retention period can be defined on?
Select one.
A. Account
B. Database
C. Schema
D. Table
Answer: D
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-time-travel.html#data-retention-period
Question: 79
Which object allows you to limit the number of credits consumed within a Snowflake account?
Select one.
A. Account usage Tracking
B. Resource Monitor
C. Warehouse Limit Parameter
D. Credit Consumption Tracker
Answer: B
Explanation:
The time travel data retention can be overwritten at the table level "When creating a table, schema, or database, the account default can be overridden using the DATA_RETENTION_TIME_IN_DAYS parameter in the command."
Question: 80
When scaling up Virtual Warehouse by increasing Virtual Warehouse t-shirt size, you are primarily
scaling for improved:
Select one.
A. Concurrency
B. Performance
Answer: B
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-considerations.html#warehouseresizing-improves-performance

Question: 83
True or False: The user has to specify which cluster a query will run on in multi-clustering Warehouse.
A. True
B. False
Answer: B
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html
Question: 84
Each incremental increase in Virtual Warehouse size (e,g. Medium to Large) generally results in what?
Select one.
A. More micro-partitions
B. Better query scheduling
C. Double the numbers of servers In the compute duster
D. Higher storage costs
Answer: C
Explanation:
Question: 85
Which of the following statement is true of Snowflake?
Select one.
A. It was built specifically for the cloud
B. it was built as an on-premises solution and then potted to the cloud
C. It was designed as a hybrid database to allow customers to store data either on premises or in the cloud
D. It was built for Hadoop architecture
E. It's based on an Oracle Architecture
Answer: A
Explanation:
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-overview.html
Question: 87
Which of the following statements would be used to export/unload data from Snowflake?
A. COPY INTO @stage
B. EXPORT TO @stage
C. INSERT INTO @stage
D. EXPORT_TO_STAGE(stage = > @Wage, select = > 'select * from t1);
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-unload-considerations.html
Question: 88
True or False: It is possible to unload structured data to semi-structured formats such as JSON and parquet.
A. True
B. False
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-unload-prepare.html
Question: 89
What are the three layers that make up Snowflake’s architecture?
Choose 3 answer
A. Compute
B. Tri-Secret Secure
C. Storage
D. Cloud Services
Answer: A, C, D
Question: 91
True or False: It is possible to query data from an Internal or named External stage without loading the data into Snowflake.
A. True
B. False
Answer: A
Explanation:
Explanation:
Question: 93
True or False: Micro-partition metadata enables some operations to be completed without requiring Compute.
A. True
B. False
Answer: A
Explanation:
Reference: https://blog.ippon.tech/innovative-snowflake-features-caching/
Question: 94
Which of the following are options when creating a Virtual Warehouse? Choose 2
A. Auto-suspend
B. Auto-resume
C. Local SSD size
D. User count
Answer: A, B
Explanation:
Question: 95
Which of the following DML commands isn’t supported by Snowflake?
A. UPSERT
B. MERGE
C. UPDATE
D. TRUNCATE TABLE
Answer: A
Question: 97
Which of the following are true of multi-cluster Warehouses? Select all that apply below.
A. A multi-cluster Warehouse can add clusters automatically based on query activity
B. A multi-cluster Warehouse can automatically turn itself off after a period of inactivity
C. A multi-cluster Warehouse can scale down when query activity slows
D. A multi-cluster Warehouse can automatically turn itself on when a query is executed against it
Answer: A, B, C, D
Answer: B, D, E
 
Question: 99
True or False: Snowflake charges a premium for storing semi-structured data.
A. True
B. False
Answer: B
Explanation:
Reference: https://snowflakecommunity.force.com/s/Question/0D50Z00008ckwNuSAI/doessnowflakechargespremium-for-storing-semi-structured-data
Question: 100
Which of the following are valid Snowflake Virtual Warehouse Scaling Policies? (Choose two.)
A. Custom
B. Economy
C. Optimized
D. Standard
Answer: BD
Explanation:
Reference: https://community.snowflake.com/s/article/Snowflake-Visualizing-Warehouse-Performance





Question: 101
True or False: A single database can exist in more than one Snowflake account.
A. True
B. False
Answer: B
Answer: B
Explanation:
https://docs.snowflake.com/en/user-guide/security-access-control-overview.html
"Security admin: Role that can manage any object grant globally, as well as create, monitor, and manage users and roles"
Question: 103
True or False: Bulk unloading of data from Snowflake supports the use of a SELECT statement.
A. True
B. False
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide-data-unload.html
Question: 109Why would a customer size a Virtual Warehouse from an X-Small to a Medium?
A. To accommodate more queries
B. To accommodate more users
C. To accommodate fluctuations in workload
D. To accommodate a more complex workload
Answer: D
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-considerations.html
Question: 110
True or False: Reader Accounts incur no additional Compute costs to the Data Provider since they are simply reading the shared data without making changes.
A. True
B. False
Answer: B
Question: 112
Which of the following statements describes a benefit of Snowflake’s separation of compute and storage?
(Choose all that apply.)
A. Growth of storage and compute are tightly coupled together
B. Storage expands without the requirement to add more compute
C. Compute can be scaled up or down without the requirement to add more storage
D. Multiple compute clusters can access stored data without contention
Answer: B,C,D
Question: 119
True or False: It is possible to load data into Snowflake without creating a named File Format object.
A. True
B. False
Answer: A
Question: 120
True or False: A table in Snowflake can only be queried using the Virtual Warehouse that was used to load the data.
A. True
B. False
Answer: B
Explanation:
Question: 121
True or False: Query ID’s are unique across all Snowflake deployments and can be used in communication with Snowflake Support to help troubleshoot issues.
A. True
B. False
Answer: A
Question: 124
Which is true of Snowflake network policies? A Snowflake network policy: (Choose two.)
A. Is available to all Snowflake Editions
B. Is only available to customers with Business Critical Edition
C. Restricts or enables access to specific IP addresses
D. Is activated using an “ALTER DATABASE” command
Answer: AC
Explanation:
Reference: https://docs.snowflake.com/en/sql-reference/sql/create-network-policy.html
Question: 126
The Query History in the Snowflake Web Interface (UI) is kept for approximately:
A. 60 minutes
B. 24 hours
C. 14 days
D. 30 days
E. 1 year
Answer: C
Question: 127
To run a Multi-Cluster Warehouse in auto-scale mode, a user would:
A. Configure the Maximum Clusters setting to “Auto-Scale”
B. Set the Warehouse type to “Auto”
C. Set the Minimum Clusters and Maximum Clusters settings to the same value
D. Set the Minimum Clusters and Maximum Clusters settings to the different values
Answer: D
Question: 129
Which of the following objects is not covered by Time Travel?
A. Tables
B. Schemas
C. Databases
D. Stages
Answer: D
Question: 130
When can a Virtual Warehouse start running queries?
A. 12am-5am
B. Only during administrator defined time slots
C. When its provisioning is complete
D. After replication
Answer: C
Question: 131
True or False: Pipes can be suspended and resumed.
A. True
B. False
Answer: A
Question: 134
True or False: Some queries can be answered through the metadata cache and do not require an active Virtual Warehouse.
A. True
B. False
Answer: A
When scaling out by adding clusters to a multi-cluster warehouse, you are primarily scaling for improved:
A. Concurrency
B. Performance
Answer: A
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-multicluster.html
Question: 136
True or False: You can query the files in an External Stage directly without having to load the data into
a table.
A. True
B. False
Answer: A
Question: 137
True or False: An active warehouse is required to run a COPY INTO statement.
A. True
B. False
Answer: A
Question: 139
True or False: Snowflake’s Global Services Layer gathers and maintains statistics on all columns in all micro-partitions.
A. True
B. False
Answer: A
Explanation:
Snowflake is a single, integrated platform delivered as-a-service. It features storage, compute, and global services layers that are physically separated but logically integrated. 
uestion: 140
Which of the following statements is true of Snowflake?
A. It was built specifically for the cloud
B. It was built as an on-premises solution and then ported to the cloud
C. It was designed as a hybrid database to allow customers to store data either on premises or in the cloud
D. It was built for Hadoop architecture
E. It's based on an Oracle Architecture
Answer: A
Question: 142
Which of the following statements are true about Schemas in Snowflake? (Choose two.)
A. A Schema may contain one or more Databases
B. A Database may contain one or more Schemas
C. A Schema is a logical grouping of Database Objects
D. Each Schema is contained within a Warehouse
Topic 2, Exam pool B
Question: 150
The fail-safe retention period is how many days? This depends on the edition. Standard is only 1 day!
A. 1 day
B. 7 days
C. 45 days
D. 90 days
Answer: B
Question: 151
True or False: A 4X-Large Warehouse may, at times, take longer to provision than a X-Small Warehouse.
A. True
B. False
Answer: A
Question: 153
The Information Schema and Account Usage Share provide storage information for which of the following objects? (Choose three.)
A. Users
B. Tables
C. Databases
D. Internal Stages
Answer: A,B,C
Explanation:
Question: 154
What is the default File Format used in the COPY command if one is not specified?
A. CSV
B. JSON
C. Parquet
D. XML
Answer: A
Question: 155
True or False: Reader Accounts are able to extract data from shared data objects for use outside of Snowflake.
A. True
B. False
Answer: A
Explanation:
Question: 156
True or False: Loading data into Snowflake requires that source data files be no larger than 16MB.
A. True
B. False
Answer: B
A. True
B. False
Answer: A
B
Question: 159
What are two ways to create and manage Data Shares in Snowflake? (Choose two.)
A. Via the Snowflake Web Interface (Ul)
B. Via the data_share=true parameter
C. Via SQL commands
D. Via Virtual Warehouses
Answer: A,C
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-sharing-provider.html
Question: 160
True or False: Fail-safe can be disabled within a Snowflake account.
A. True
B. False
Answer: B
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/data-failsafe.html
Separate and distinct from Time Travel, Fail-safe ensures historical data is protected in the event of a system failure or other catastrophic event, e.g. a hardware failure or security breach. 
Fail safe feature cannot be enabled or disabled from the user end .
Question: 161
True or False: It is possible for a user to run a query against the query result cache without requiring an active Warehouse.
A. True
B. False
Answer: A
Question: 162
A virtual warehouse's auto-suspend and auto-resume settings apply to which of the following?
A. The primary cluster in the virtual warehouse
B. The entire virtual warehouse
C. The database in which the virtual warehouse resides
D. The Queries currently being run on the virtual warehouse
Answer: B
Question: 163
Which of the following Snowflake features provide continuous data protection automatically? (Select TWO).
A. Internal stages
B. Incremental backups
C. Time Travel
D. Zero-copy clones
E. Fail-safe
Question: 166
What is the minimum Snowflake edition required to create a materialized view?
A. Standard Edition
B. Enterprise Edition
C. Business Critical Edition
D. Virtual Private Snowflake Edition
Answer: B
Question: 167
What happens to the underlying table data when a CLUSTER BY clause is added to a Snowflake table?
A. Data is hashed by the cluster key to facilitate fast searches for common data values
B. Larger micro-partitions are created for common data values to reduce the number of partitions that must be scanned
C. Smaller micro-partitions are created for common data values to allow for more parallelism
D. Data may be colocated by the cluster key within the micro-partitions to improve pruning
performance
Answer: D
Explanation:
Question: 168
Which feature is only available in the Enterprise or higher editions of Snowflake?
A. Column-level security
B. SOC 2 type II certification
C. Multi-factor Authentication (MFA)
D. Object-level access control
Answer: A
Question: 173
A user needs to create a materialized view in the schema MYDB.MYSCHEMA.
Which statements will provide this access?
A. GRANT ROLE MYROLE TO USER USER1;
	CREATE MATERIALIZED VIEW ON SCHEMA MYDB.MYSCHEMA TO ROLE MYROLE;
B. GRANT ROLE MYROLE TO USER USER1;
	CREATE MATERIALIZED VIEW ON SCHEMA MYDB.MYSCHEMA TO USER USER1;
C. GRANT ROLE MYROLE TO USER USER1;
	CREATE MATERIALIZED VIEW ON SCHEMA MYDB.MYSCHEMA TO USER1;
D. GRANT ROLE MYROLE TO USER USER1;
	CREATE MATERIALIZED VIEW ON SCHEMA MYDB.MYSCHEMA TO MYROLE;
Answer: A
Explanation:

Question: 180
Which statement about billing applies to Snowflake credits?
A. Credits are billed per-minute with a 60-minute minimum
B. Credits are used to pay for cloud data storage usage
C. Credits are consumed based on the number of credits billed for each hour that a warehouse runs
D. Credits are consumed based on the warehouse size and the time the warehouse is running
Answer: D
Explanation:
Snowflake credits are used to pay for the consumption of resources on Snowflake. A Snowflake credit is a unit of measure, and it is consumed only when a customer is using resources, such as when a
virtual warehouse is running, the cloud services layer is performing work, or serverless features are used.
https://docs.snowflake.com/en/user-guide/what-are-credits.html
Explanation:
Question: 182
When reviewing the load for a warehouse using the load monitoring chart, the chart indicates that a high volume of Queries are always queuing in the warehouse 
According to recommended best practice, what should be done to reduce the Queue volume? (Select TWO).
A. Use multi-clustered warehousing to scale out warehouse capacity.
B. Scale up the warehouse size to allow Queries to execute faster.
C. Stop and start the warehouse to clear the queued queries
D. Migrate some queries to a new warehouse to reduce load
E. Limit user access to the warehouse so fewer queries are run against it.
Answer: A, D

Explanation:
Secure Data Sharing enables sharing selected objects in a database in your account with other Snowflake accounts. 
The following Snowflake database objects can be shared: 
Tables
External tables
Secure views
Secure materialized views
Secure UDFs
Snowflake enables the sharing of databases through shares, which are created by data providers and “imported” by data consumers.
https://docs.snowflake.com/en/user-guide/data-sharingintro.html#:~:text=Secure%20Data%20Sharing%20enables%20sharing,External%20tables
Question: 184
Which of the following commands cannot be used within a reader account?
A. CREATE SHARE
B. ALTER WAREHOUSE
C. DROP ROLE
D. SHOW SCHEMAS
E. DESCRBE TABLE
Answer: A
Explanation:
A reader account is intended primarily for querying data shared by the provider of the account.
Adding new data to the account and/or updating shared data in the account is not supported.
Changing the configuration of virtual warehouses is also not permitted as those resources are owned and managed by the provider of the account which is sharing the data.
Explanation:
Question: 186
Which of the following Snowflake capabilities are available in all Snowflake editions? (Select TWO)
A. Customer-managed encryption keys through Tri-Secret Secure
B. Automatic encryption of all data
C. Up to 90 days of data recovery through Time Travel
D. Object-level access control
E. Column-level security to apply data masking policies to tables and views
Answer: B, D
Explanation:
https://docs.snowflake.com/en/user-guide/intro-editions.html
Question: 187
Which command is used to unload data from a Snowflake table into a file in a stage?
A. COPY INTO
B. GET
C. WRITE
D. EXTRACT INTO
Answer: A

Question: 191
Which of the following are best practice recommendations that should be considered when loading data into Snowflake? (Select TWO).
A. Load files that are approximately 25 MB or smaller.
B. Remove all dates and timestamps.
C. Load files that are approximately 100-250 MB (or larger)
D. Avoid using embedded characters such as commas for numeric data types
E. Remove semi-structured data types
Answer: C, D
Explanation:
https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare.html
uestion: 192
A user has 10 files in a stage containing new customer dat
a. The ingest operation completes with no errors, using the following command:
COPY INTO my__table FROM @my__stage;
The next day the user adds 10 files to the stage so that now the stage contains a mixture of new customer data and updates to the previous data. The user did not remove the 10 original files.
If the user runs the same copy into command what will happen?
A. All data from all of the files on the stage will be appended to the table
B. Only data about new customers from the new files will be appended to the table
C. The operation will fail with the error uncertain files in stage.
D. All data from only the newly-added files will be appended to the table.
Answer: D
Explanation:
Question: 193
A user has unloaded data from Snowflake to a stage
Which SQL command should be used to validate which data was loaded into the stage?
A. list @file__stage
B. show @file__stage
C. view @file__stage
D. verify @file__stage
Answer: A
Question: 196
Which of the following describes how multiple Snowflake accounts in a single organization relate to various cloud providers?
A. Each Snowflake account can be hosted in a different cloud vendor and region.
B. Each Snowflake account must be hosted in a different cloud vendor and region
C. All Snowflake accounts must be hosted in the same cloud vendor and region
D. Each Snowflake account can be hosted in a different cloud vendor, but must be in the same region.
Answer: A
Question: 198
What are the default Time Travel and Fail-safe retention periods for transient tables?
A. Time Travel - 1 day. Fail-safe - 1 day
B. Time Travel - 0 days. Fail-safe - 1 day
C. Time Travel - 1 day. Fail-safe - 0 days
D. Transient tables are retained in neither Fail-safe nor Time Travel
Answer: C
Question: 199
What is a best practice after creating a custom role?
A. Create the custom role using the SYSADMIN role.
B. Assign the custom role to the SYSADMIN role
C. Assign the custom role to the PUBLIC role
D. Add__CUSTOM to all custom role names
Question: 201
Will data cached in a warehouse be lost when the warehouse is resized?
A. Possibly, if the warehouse is resized to a smaller size and the cache no longer fits.
B. Yes. because the compute resource is replaced in its entirety with a new compute resource.
C. No. because the size of the cache is independent from the warehouse size
D. Yes. became the new compute resource will no longer have access to the cache encryption key
Answer: A
Explanation:
Question: 203
What is the MOST performant file format for loading data in Snowflake?
A. CSV (Unzipped)
B. Parquet
C. CSV (Gzipped)
D. ORC
Answer: A
Explanation:
uestion: 204
Which copy INTO command outputs the data into one file?
A. SINGLE=TRUE
B. MAX_FILE_NUMBER=1
C. FILE_NUMBER=1
D. MULTIPLE=FAISE
Answer: A
Explanation:
Question: 205
Where would a Snowflake user find information about query activity from 90 days ago?
A. account__usage.query history view
B. account__usage.query__history__archive View
C. information__schema.cruery_history view
D. information__schema - query history_by_session view
Answer: A
Question: 206
Which Snowflake technique can be used to improve the performance of a query?
A. Clustering
B. Indexing
C. Fragmenting
D. Using INDEX__HINTS
Answer: A
Question: 208
Which command can be used to load data into an internal stage?
A. LOAD
B. copy
C. GET
D. PUT
Answer: D
Question: 209
What happens when an external or an internal stage is dropped? (Select TWO).
A. When dropping an external stage, the files are not removed and only the stage is dropped
B. When dropping an external stage, both the stage and the files within the stage are removed
C. When dropping an internal stage, the files are deleted with the stage and the files are recoverable
D. When dropping an internal stage, the files are deleted with the stage and the files are not recoverable
E. When dropping an internal stage, only selected files are deleted with the stage and are not recoverable
Answer: A, C
Explanation:
Question: 210
How long is Snowpipe data load history retained?
A. As configured in the create pipe settings
B. Until the pipe is dropped
C. 64 days
D. 14 days
Ans C 
Question: 213
Query compilation occurs in which architecture layer of the Snowflake Cloud Data Platform?
A. Compute layer
B. Storage layer
C. Cloud infrastructure layer
D. Cloud services layer
Answer: D
Explanation:
Question: 215
In the query profiler view for a query, which components represent areas that can be used to help optimize query performance? (Select TWO)
A. Bytes scanned
B. Bytes sent over the network
C. Number of partitions scanned
D. Percentage scanned from cache
E. External bytes scanned
Answer: A, C
Explanation:
Question: 217
When reviewing a query profile, what is a symptom that a query is too large to fit into the memory?
A. A single join node uses more than 50% of the query time
B. Partitions scanned is equal to partitions total
C. An AggregateOperacor node is present
D. The query is spilling to remote storage
Answer: D
Explanation:
Question: 218
Which stage type can be altered and dropped?
A. Database stage
B. External stage
C. Table stage
D. User stage
Answer: B
Question: 219
Which command can be used to stage local files from which Snowflake interface?
A. SnowSQL
B. Snowflake classic web interface (Ul)
C. Snowsight
D. .NET driver
Answer: A
Question: 220
What is the recommended file sizing for data loading using Snowpipe?
A. A compressed file size greater than 100 MB, and up to 250 MB
B. A compressed file size greater than 100 GB, and up to 250 GB
C. A compressed file size greater than 10 MB, and up to 100 MB
D. A compressed file size greater than 1 GB, and up to 2 GB
Answer: A
Question: 221
Which services does the Snowflake Cloud Services layer manage? (Select TWO).
A. Compute resources
B. Query execution
C. Authentication
D. Data storage
E. Metadata
Answer: C, E
Question: 222
What data is stored in the Snowflake storage layer? (Select TWO).
A. Snowflake parameters
B. Micro-partitions
C. Query history
D. Persisted query results
E. Standard and secure view results
Answer: B, D
Explanation:
Answer: A, E
Explanation:
Explanation:
Question: 225
What is a responsibility of Snowflake's virtual warehouses?
A. Infrastructure management
B. Metadata management
C. Query execution
D. Query parsing and optimization
E. Management of the storage layer
Answer: C
Explanation:
Explanation:
uestion: 227
What happens when a virtual warehouse is resized?
A. When increasing the size of an active warehouse the compute resource for all running and queued queries on the warehouse are affected
B. When reducing the size of a warehouse the compute resources are removed only when they are no longer being used to execute any current statements.
C. The warehouse will be suspended while the new compute resource is provisioned and will resume automatically once provisioning is complete.
D. Users who are trying to use the warehouse will receive an error message until the resizing is complete
Answer: B
Explanation:
Question: 228
A developer is granted ownership of a table that has a masking policy. The developer's role is not able 
to see the masked data. Will the developer be able to modify the table to read the masked data?
A. Yes, because a table owner has full control and can unset masking policies.
B. Yes, because masking policies only apply to cloned tables.
C. No, because masking policies must always reference specific access roles.
D. No, because ownership of a table does not include the ability to change masking policies
Answer: A
Explanation:
Question: 229
Which of the following describes how clustering keys work in Snowflake?
A. Clustering keys update the micro-partitions in place with a full sort, and impact the DML operations.
B. Clustering keys sort the designated columns over time, without blocking DML operations
C. Clustering keys create a distributed, parallel data structure of pointers to a table's rows and columns
D. Clustering keys establish a hashed key on each node of a virtual warehouse to optimize joins at run-time
Answer: B
Explanation:

Explanation:
https://docs.snowflake.com/en/user-guide/ecosystem-analytics.html
Question: 231
Which of the following is a valid source for an external stage when the Snowflake account is located on Microsoft Azure?
A. An FTP server with TLS encryption
B. An HTTPS server with WebDAV
C. A Google Cloud storage bucket
D. A Windows server file share on Azure
Answer: C
Question: 233
What can be used to view warehouse usage over time? (Select Two).
A. The load HISTORY view
B. The Query history view
C. The show warehouses command
D. The WAREHOUSE_METERING__HISTORY View
E. The billing and usage tab in the Snowflake web Ul
Answer: D, E
Explanation:
Question: 234
Which Snowflake feature is used for both querying and restoring data?
A. Cluster keys
B. Time Travel
C. Fail-safe
D. Cloning
Answer: B
Explanation:

Explanation:
https://docs.snowflake.com/en/user-guide/security-mfa.html
Question: 236
Which Snowflake objects track DML changes made to tables, like inserts, updates, and deletes?
A. Pipes
B. Streams
C. Tasks
D. Procedures
Answer: B
Explanation:
https://dataterrain.com/how-to-change-tracking-using-table-streams-insnowflake/#:~:text=A%20stream%20is%20a%20Snowflake,as%20metadata%20about%20each%20change.
Question: 237
What tasks can be completed using the copy command? (Select TWO)
A. Columns can be aggregated
B. Columns can be joined with an existing table
C. Columns can be reordered
D. Columns can be omitted
E. Data can be loaded without the need to spin up a virtual warehouse
Answer: C, D
Explanation:
Question: 238
What feature can be used to reorganize a very large table on one or more columns?
A. Micro-partitions
B. Clustering keys
C. Key partitions
D. Clustered partitions
Answer: B
Explanation:
https://docs.snowflake.com/en/user-guide/tables-clustering-keys.html
uestion: 239
What SQL command would be used to view all roles that were granted to user.1?
A. show grants to user USER1;
B. show grants of user USER1;
C. describe user USER1;
D. show grants on user USER1;
Answer: A
Explanation:
Question: 240
Which of the following can be executed/called with Snowpipe?
A. A User Defined Function (UDF)
B. A stored procedure
C. A single copy_into statement
D. A single insert__into statement
Answer: C
Explanation:
Question: 241
What Snowflake role must be granted for a user to create and manage accounts?
A. ACCOUNTADMIN
B. ORGADMIN
C. SECURITYADMIN
D. SYSADMIN
Ans A
Explanation:

Explanation:
Question: 243
When is the result set cache no longer available? (Select TWO)
A. When another warehouse is used to execute the query
B. When another user executes the query
C. When the underlying data has changed
D. When the warehouse used to execute the query is suspended
E. When it has been 24 hours since the last query
Answer: C, E
Question: 245
What are ways to create and manage data shares in Snowflake? (Select TWO)
A. Through the Snowflake web interface (Ul)
B. Through the DATA_SHARE=TRUE parameter
C. Through SQL commands
D. Through the enable__share=true parameter
E. Using the CREATE SHARE AS SELECT * TABLE command
Answer: A, C
Explanation:
Question: 246
A company's security audit requires generating a report listing all Snowflake logins (e.g.. date and user) within the last 90 days. Which of the following statements will return the required information?
A. SELECT LAST_SUCCESS_LOGIN, LOGIN_NAME FROM ACCOUNT_USAGE.USERS;
B. SELECT EVENT_TIMESTAMP, USER_NAME FROM table(information_schema.login_history_by_user())
C. SELECT EVENT_TIMESTAMP, USER_NAME FROM ACCOUNT_USAGE.ACCESS_HISTORY;
D. SELECT EVENT_TIMESTAMP, USER_NAME FROM ACCOUNT_USAGE.LOGIN_HISTORY;
Answer: D
Explanation:
Question: 248
What is the purpose of an External Function?
A. To call code that executes outside of Snowflake
B. To run a function in another Snowflake database
C. To share data in Snowflake with external parties
D. To ingest data from on-premises data sources
Answer: A
Question: 249
A user created a new worksheet within the Snowsight Ul and wants to share this with teammates
How can this worksheet be shared?
A. Create a zero-copy clone of the worksheet and grant permissions to teammates
B. Create a private Data Exchange so that any teammate can use the worksheet
C. Share the worksheet with teammates within Snowsight
D. Create a database and grant all permissions to teammates
Answer: C
Explanation:

Explanation:
https://docs.snowflake.com/en/user-guide/warehousesmulticluster.html#:~:text=Multi%2Dcluster%20warehouses%20enable%20you,during%20peak%20and%20off%20hours.
Question: 251
Which statements are true concerning Snowflake's underlying cloud infrastructure? (Select THREE),
A. Snowflake data and services are deployed in a single availability zone within a cloud provider's region.
B. Snowflake data and services are available in a single cloud provider and a single region, the use of multiple cloud providers is not supported.
C. Snowflake can be deployed in a customer's private cloud using the customer's own compute and storage resources for Snowflake compute and storage
D. Snowflake uses the core compute and storage services of each cloud provider for its own compute and storage
E. All three layers of Snowflake's architecture (storage, compute, and cloud services) are deployed and managed entirely on a selected cloud platform
F. Snowflake data and services are deployed in at least three availability zones within a cloud provider's region
Answer: D, E, F
Explanation:
Question: 252
Which snowflake objects will incur both storage and cloud compute charges? (Select TWO)
A. Materialized view
B. Sequence
C. Secure view
D. Transient table
E. Clustered table
Answer: A, E
Explanation:
Question: 253
A user is preparing to load data from an external stage Which practice will provide the MOST efficient loading performance?
A. Organize files into logical paths
B. Store the files on the external stage to ensure caching is maintained
C. Use pattern matching for regular expression execution
D. Load the data in one large file
Answer: A
Explanation:

https://docs.snowflake.com/en/user-guide/warehouses-overview.html
Question: 255
Which command sets the Virtual Warehouse for a session?
A. COPY WAREHOUSE FROM <<config file>>;
B. SET WAREHOUSE = <<warehouse name>>;
C. USE WAREHOUSE <<warehouse name>>;
D. USE VIRTUAL_WAREHOUSE <<warehouse name>>;
Answer: C
Explanation:
Reference: https://docs.snowflake.com/en/user-guide/warehouses-tasks.html
Question: 256
What occurs when a pipe is recreated using the CREATE OR REPLACE PIPE command?
A. The Pipe load history is reset to empty.
B. The REFRESH command is executed.
C. The stage will be purged.
D. The destination table is truncated.
Answer: A
Explanation:
Question: 258
Which of the following are best practices for loading data into Snowflake? (Choose three.)
A. Aim to produce data files that are between 100 MB and 250 MB in size, compressed.
B. Load data from files in a cloud storage service in a different region or cloud platform from the service or region containing the Snowflake account, to save on cost.
C. Enclose fields that contain delimiter characters in single or double quotes.
D. Split large files into a greater number of smaller files to distribute the load among the compute resources in an active warehouse.
E. When planning which warehouse(s) to use for data loading, start with the largest warehouse possible.
F. Partition the staged data into large folders with random paths, allowing Snowflake to determine the best way to load each file.
Answer: A, C, D
Explanation:
Question: 259
What do the terms scale up and scale out refer to in Snowflake? (Choose two.)
A. Scaling out adds clusters of the same size to a virtual warehouse to handle more concurrent queries.
B. Scaling out adds clusters of varying sizes to a virtual warehouse.
C. Scaling out adds additional database servers to an existing running cluster to handle more concurrent queries.
D. Snowflake recommends using both scaling up and scaling out to handle more concurrent queries.
E. Scaling up resizes a virtual warehouse so it can handle more complex workloads.
F. Scaling up adds additional database servers to an existing running cluster to handle larger workloads.
Answer: A, E
Explanation:
Question: 260
What is the minimum Snowflake edition that has column-level security enabled?
A. Standard
B. Enterprise
C. Business Critical
D. Virtual Private Snowflake
Answer: B
Question: 263
What is the maximum total Continuous Data Protection (CDP) charges incurred for a temporary
table?
A. 30 days
B. 7 days
C. 48 hours
D. 24 hours
Answer: D
Question: 265
Which of the following are characteristics of Snowflake virtual warehouses? (Choose two.)
A. Auto-resume applies only to the last warehouse that was started in a multi-cluster warehouse.
B. The ability to auto-suspend a warehouse is only available in the Enterprise edition or above.
C. SnowSQL supports both a configuration file and a command line option for specifying a default warehouse.
D. A user cannot specify a default warehouse when using the ODBC driver.
E. The default virtual warehouse size can be changed at any time.
Answer: C,E
Explanation:
Explanation:
Question: 267
The Snowflake Cloud Data Platform is described as having which of the following architectures?
A. Shared-disk
B. Shared-nothing
C. Multi-cluster shared data
D. Serverless query engine
Answer: C
Explanation:
Question: 269
What versions of Snowflake should be used to manage compliance with Personal Identifiable Information (PII) requirements? (Choose two.)
A. Custom Edition
B. Virtual Private Snowflake
C. Business Critical Edition
D. Standard Edition
E. Enterprise Edition
Answer: B, C
Question: 273
When loading data into Snowflake via Snowpipe what is the compressed file size recommendation?
A. 10-50 MB
B. 100-250 MB
C. 300-500 MB
D. 1000-1500 MB
Answer: B
Explanation:
Question: 274
Which Snowflake feature allows a user to substitute a randomly generated identifier for sensitive data, in order to prevent unauthorized users access to the data, before loading it into Snowflake?
A. External Tokenization
B. External Tables
C. Materialized Views
D. User-Defined Table Functions (UDTF)
Answer: A
Question: 276
A running virtual warehouse is suspended.
What is the MINIMUM amount of time that the warehouse will incur charges for when it is restarted?
A. 1 second
B. 60 seconds
C. 5 minutes
D. 60 minutes
Answer: B
Question: 280
How does Snowflake Fail-safe protect data in a permanent table?
A. Fail-safe makes data available up to 1 day, recoverable by user operations.
B. Fail-safe makes data available for 7 days, recoverable by user operations.
C. Fail-safe makes data available for 7 days, recoverable only by Snowflake Support.
D. Fail-safe makes data available up to 1 day, recoverable only by Snowflake Support.
Answer: C
Answer: C
Question: 282
Which minimum Snowflake edition allows for a dedicated metadata store?
A. Standard
B. Enterprise
C. Business Critical
D. Virtual Private Snowflake
Ans B
Question: 286
Which of the following statements describe features of Snowflake data caching? (Choose two.)
A. When a virtual warehouse is suspended, the data cache is saved on the remote storage layer.
B. When the data cache is full, the least-recently used data will be cleared to make room.
C. A user can only access their own queries from the query result cache.
D. A user must set USE_METADATA_CACHE to TRUE to use the metadata cache in queries.
E. The RESULT_SCAN table function can access and filter the contents of the query result cache.
Answer: B, E
Question: 287
Explanation:
Question: 288
Which of the following describes a Snowflake stored procedure?
A. They can be created as secure and hide the underlying metadata from the user.
B. They can only access tables from a single database.
C. They can contain only a single SQL statement.
D. They can be created to run with a caller's rights or an owner's rights.
Answer: D
Question: 290
Which Snowflake function will interpret an input string as a JSON document, and produce a VARIANT value?
A. parse_json()
B. json_extract_path_text()
C. object_construct()
D. flatten
Answer: A
Explanation:
Question: 292
Which Snowflake architectural layer is responsible for a query execution plan?
A. Compute
B. Data storage
C. Cloud services
D. Cloud provider
Answer: C
Explanation:
Question: 293
Which SQL commands, when committed, will consume a stream and advance the stream offset?
(Choose two.)
A. UPDATE TABLE FROM STREAM
B. SELECT FROM STREAM
C. INSERT INTO TABLE SELECT FROM STREAM
D. ALTER TABLE AS SELECT FROM STREAM
E. BEGIN COMMIT
Answer: A, C
Explanation:
Question: 294
Which methods can be used to delete staged files from a Snowflake stage? (Choose two.)
A. Use the DROP <file> command after the load completes.
B. Specify the TEMPORARY option when creating the file format.
C. Specify the PURGE copy option in the COPY INTO <table> command.
D. Use the REMOVE command after the load completes.
E. Use the DELETE LOAD HISTORY command after the load completes.
Answer: C, D
Question: 296
What is an advantage of using an explain plan instead of the query profiler to evaluate the performance of a query?
A. The explain plan output is available graphically.
B. An explain plan can be used to conduct performance analysis without executing a query.
C. An explain plan will handle queries with temporary tables and the query profiler will not.
D. An explain plan's output will display automatic data skew optimization information.
Answer: B
Question: 297
Which data types are supported by Snowflake when using semi-structured data? (Choose two.)
A. VARIANT
B. VARRAY
C. STRUCT
D. ARRAY
E. QUEUE
Answer: A, D
Question: 298
Why does Snowflake recommend file sizes of 100-250 MB compressed when loading data?
A. Optimizes the virtual warehouse size and multi-cluster setting to economy mode
B. Allows a user to import the files in a sequential order
C. Increases the latency staging and accuracy when loading the data
D. Allows optimization of parallel operations
Answer: D
Question: 299
Which of the following features are available with the Snowflake Enterprise edition? (Choose two.)
A. Database replication and failover
B. Automated index management
C. Customer managed keys (Tri-secret secure)
D. Extended time travel
E. Native support for geospatial data
Answer: A, D
uestion: 300
What is the default file size when unloading data from Snowflake using the COPY command?
A. 5 MB
B. 8 GB
C. 16 MB
D. 32 MB
Answer: C
Explanation:
Question: 301
What features that are part of the Continuous Data Protection (CDP) feature set in Snowflake do not require additional configuration? (Choose two.)
A. Row level access policies
B. Data masking policies
C. Data encryption
D. Time Travel
E. External tokenization
Answer: C, D
Question: 302
Which Snowflake layer is always leveraged when accessing a query from the result cache?
A. Metadata
B. Data Storage
C. Compute
D. Cloud Services
Answer: D
Explanation:
Question: 303
A Snowflake Administrator needs to ensure that sensitive corporate data in Snowflake tables is not visible to end users, but is partially visible to functional managers.
How can this requirement be met?
A. Use data encryption.
B. Use dynamic data masking.
C. Use secure materialized views.
D. Revoke all roles for functional managers and end users.
Answer: B
Question: 305  --------------------------------------------------------------------------------------here
What affects whether the query results cache can be used?
A. If the query contains a deterministic function
B. If the virtual warehouse has been suspended
C. If the referenced data in the table has changed
D. If multiple users are using the same virtual warehouse
Answer: C
 Question: 262
Which of the following describes the Snowflake Cloud Services layer?
A. Coordinates activities in the Snowflake account
B. Executes queries submitted by the Snowflake account users
C. Manages quotas on the Snowflake account storage
D. Manages the virtual warehouse cache to speed up queries
Answer: A

Question: 306
Which of the following is an example of an operation that can be completed without requiring compute, assuming no queries have been executed previously?
A. SELECT SUM (ORDER_AMT) FROM SALES;
B. SELECT AVG(ORDER_QTY) FROM SALES;
C. SELECT MIN(ORDER_AMT) FROM SALES;
D. SELECT ORDER_AMT * ORDER_QTY FROM SALES;
Answer: C
Question: 307
How many days is load history for Snowpipe retained?
A. 1 day
B. 7 days
C. 14 days
D. 64 days
Answer: C
Explanation:
Question: 311
What is the maximum Time Travel retention period for a temporary Snowflake table?
A. 90 days
B. 1 day
C. 7 days
D. 45 days
Answer: B
Question: 312
When should a multi-cluster warehouse be used in auto-scaling mode?
A. When it is unknown how much compute power is needed
B. If the select statement contains a large number of temporary tables or Common Table Expressions (CTEs)
C. If the runtime of the executed query is very slow
D. When a large number of concurrent queries are run on the same warehouse
Answer: D
Question: 313
Snowflake supports the use of external stages with which cloud platforms? (Choose three.)
A. Amazon Web Services
B. Docker
C. IBM Cloud
D. Microsoft Azure Cloud
E. Google Cloud Platform
F. Oracle Cloud
Answer: A, D, E
Question: 317
A company needs to allow some users to see Personally Identifiable Information (PII) while limiting other users from seeing the full value of the PII.
Which Snowflake feature will support this?
A. Row access policies
B. Data masking policies
C. Data encryption
D. Role based access control
Answer: B
Explanation:
Question: 318  from                                                     ----                Thursday
A user has unloaded data from a Snowflake table to an external stage.
Which command can be used to verify if data has been uploaded to the external stage named my_stage?
A. view @my_stage
B. list @my_stage
C. show @my_stage
D. display @my_stage
Answer: B
Question: 319
Which tasks are performed in the Snowflake Cloud Services layer? (Choose two.)
A. Management of metadata
B. Computing the data
C. Maintaining Availability Zones
D. Infrastructure security
E. Parsing and optimizing queries
Answer: A, E
Question: 320
What is true about sharing data in Snowflake? (Choose two.)
A. The Data Consumer pays for data storage as well as for data computing.
B. The shared data is copied into the Data Consumer account, so the Consumer can modify it without impacting the base data of the Provider.
C. A Snowflake account can both provide and consume shared data.
D. The Provider is charged for compute resources used by the Data Consumer to query the shared data.
E. The Data Consumer pays only for compute resources to query the shared data.
Answer: C, E
Question: 321
The following JSON is stored in a VARIANT column called src of the CAR_SALES table:
A user needs to extract the dealership information from the JSON.
How can this be accomplished?
A. select src:dealership from car_sales;
B. select src.dealership from car_sales;
C. select src:Dealership from car_sales;
D. select dealership from car_sales;
Answer: A
Explanation:
Question: 323
Which of the following accurately describes shares?
A. Tables, secure views, and secure UDFs can be shared
B. Shares can be shared
C. Data consumers can clone a new table from a share
D. Access to a share cannot be revoked once granted
Answer: A
Explanation:
Question: 324
What are best practice recommendations for using the ACCOUNTADMIN system-defined role in Snowflake? (Choose two.)
A. Ensure all ACCOUNTADMIN roles use Multi-factor Authentication (MFA).
B. All users granted ACCOUNTADMIN role must be owned by the ACCOUNTADMIN role.
C. The ACCOUNTADMIN role must be granted to only one user.
D. Assign the ACCOUNTADMIN role to at least two users, but as few as possible.
E. All users granted ACCOUNTADMIN role must also be granted SECURITYADMIN role.
Answer: A, D
Question: 326
The is the minimum Fail-safe retention time period for transient tables?
A. 1 day
B. 7 days
C. 12 hours
D. 0 days
Answer: D
Explanation:
Question: 327
Which statements are correct concerning the leveraging of third-party data from the Snowflake Data Marketplace? (Choose two.)
A. Data is live, ready-to-query, and can be personalized.
B. Data needs to be loaded into a cloud provider as a consumer account.
C. Data is not available for copying or moving to an individual Snowflake account.
D. Data is available without copying or moving.
E. Data transformations are required when combining Data Marketplace datasets with existing data in Snowflake.
Answer: A, D
Explanation:
Question: 330
When cloning a database containing stored procedures and regular views, that have fully qualified table references, which of the following will occur?
A. The cloned views and the stored procedures will reference the cloned tables in the cloned database.
B. An error will occur, as views with qualified references cannot be cloned.
C. An error will occur, as stored objects cannot be cloned.
D. The stored procedures and views will refer to tables in the source database.
Answer: D
Explanation:
Question: 331
When loading data into Snowflake, how should the data be organized?
A. Into single files with 100-250 MB of compressed data per file
B. Into single files with 1-100 MB of compressed data per file
C. Into files of maximum size of 1 GB of compressed data per file
D. Into files of maximum size of 4 GB of compressed data per file
Answer: A
uestion: 333
Which Snowflake SQL statement would be used to determine which users and roles have access to a
role called MY_ROLE?
A. SHOW GRANTS OF ROLE MY_ROLE
B. SHOW GRANTS TO ROLE MY_ROLE
C. SHOW GRANTS FOR ROLE MY_ROLE
D. SHOW GRANTS ON ROLE MY_ROLE
Answer: B
Explanation:
Question: 334
What is the MINIMUM edition of Snowflake that is required to use a SCIM security integration?
A. Business Critical Edition
B. Standard Edition
C. Virtual Private Snowflake (VPS)
D. Enterprise Edition
Answer: B
Explanation:
Question: 335
A user created a transient table and made several changes to it over the course of several days.
Three days after the table was created, the user would like to go back to the first version of the table.
How can this be accomplished?
A. Use Time Travel, as long as DATA_RETENTION_TIME_IN_DAYS was set to at least 3 days.
B. The transient table version cannot be retrieved after 24 hours.
C. Contact Snowflake Support to have the data retrieved from Fail-safe storage.
D. Use the FAIL_SAFE parameter for Time Travel to retrieve the data from Fail-safe storage.
Answer: B
Question: 337
Where can a user find and review the failed logins of a specific user for the past 30 days?
A. The USERS view in ACCOUNT_USAGE
B. The LOGIN_HISTORY view in ACCOUNT_USAGE
C. The ACCESS_HISTORY view in ACCOUNT_USAGE
D. The SESSIONS view in ACCOUNT_USAGE
Answer: B
Question: 338
Which of the following statements apply to Snowflake in terms of security? (Choose two.)
A. Snowflake leverages a Role-Based Access Control (RBAC) model.
B. Snowflake requires a user to configure an IAM user to connect to the database.
C. All data in Snowflake is encrypted.
D. Snowflake can run within a user's own Virtual Private Cloud (VPC).
E. All data in Snowflake is compressed.
Answer: A, C
Explanation:
Question: 341
Which statement is true about running tasks in Snowflake?
A. A task can be called using a CALL statement to run a set of predefined SQL commands.
B. A task allows a user to execute a single SQL statement/command using a predefined schedule.
C. A task allows a user to execute a set of SQL commands on a predefined schedule.
D. A task can be executed using a SELECT statement to run a predefined SQL command.
Answer: B
Question: 342
In an auto-scaling multi-cluster virtual warehouse with the setting SCALING_POLICY = ECONOMY enabled, when is another cluster started?
A. When the system has enough load for 2 minutes
B. When the system has enough load for 6 minutes
C. When the system has enough load for 8 minutes
D. When the system has enough load for 10 minutes
Answer: B
Explanation:
Question: 347
What happens to historical data when the retention period for an object ends?
A. The data is cloned into a historical object.
B. The data moves to Fail-safe
C. Time Travel on the historical data is dropped.
D. The object containing the historical data is dropped.
Answer: B
Question: 348
By default, which Snowflake role is required to create a share?
A. ORGADMIN
B. SECURITYADMIN
C. SHAREADMIN
D. ACCOUNTADMIN
Answer: D
Question: 350
Files have been uploaded to a Snowflake internal stage. The files now need to be deleted.
Which SQL command should be used to delete the files?
A. PURGE
B. MODIFY
C. REMOVE
D. DELETE
Answer: C
Question: 351
How should a virtual warehouse be configured if a user wants to ensure that additional multi-clusters are resumed with no delay?
A. Configure the warehouse to a size larger than generally required
B. Set the minimum and maximum clusters to autoscale
C. Use the standard warehouse scaling policy
D. Use the economy warehouse scaling policy
Answer: C
Question: 352
Where is Snowflake metadata stored?
A. Within the data files
B. In the virtual warehouse layer
C. In the cloud services layer
D. In the remote storage layer
Answer: C
Question: 353
Network policies can be applied to which of the following Snowflake objects? (Choose two.)
A. Roles
B. Databases
C. Warehouses
D. Users
E. Accounts
Answer: D, E
Question: 354 
Which of the following practices are recommended when creating a user in Snowflake? (Choose two.)
A. Configure the user to be initially disabled.
B. Force an immediate password change.
C. Set a default role for the user.
D. Set the number of minutes to unlock to 15 minutes.
E. Set the user's access to expire within a specified timeframe.
Answer: B, C
Question: 355
Which statement MOST accurately describes clustering in Snowflake?
A. The database ACCOUNTADMIN must define the clustering methodology for each Snowflake table.
B. Clustering is the way data is grouped together and stored within Snowflake micro-partitions.
C. The clustering key must be included in the COPY command when loading data into Snowflake.
D. Clustering can be disabled within a Snowflake account.
Answer: B
Question: 356
Which privilege is required for a role to be able to resume a suspended warehouse if auto-resume is not enabled?
A. USAGE
B. OPERATE
C. MONITOR
D. MODIFY
Answer: B
uestion: 357
How often are the Account and Table master keys automatically rotated by Snowflake?
A. 30 Days
B. 60 Days
C. 90 Days
D. 365 Days.
Answer: A
uestion: 358
Explanation:
Question: 359
Which query profile statistics help determine if efficient pruning is occurring? (Choose two.)
A. Bytes sent over network
B. Percentage scanned from cache
C. Partitions total
D. Bytes spilled to local storage
E. Partitions scanned
Answer: C, E
Explanation:
Question: 360
Which TABLE function helps to convert semi-structured data to a relational representation?
A. CHECK_JSON
B. TO_JSON
C. FLATTEN
D. PARSE_JSON
Answer: C
Question: 361
Which URL type allows users to access unstructured data without authenticating into Snowflake or passing an authorization token?
A. Pre-signed URL
B. Scoped URL
C. Signed URL
D. File URL
Answer: A
Question: 362
What is the recommended compressed file size range for continuous data loads using Snowpipe?
A. 8-16 MB
B. 16-24 MB
C. 10-99 MB
D. 100-250 MB
Answer: D
Question: 363
Which of the following statements describes a schema in Snowflake?
A. A logical grouping of objects that belongs to a single database
B. A logical grouping of objects that belongs to multiple databases
C. A named Snowflake object that includes all the information required to share a database
D. A uniquely identified Snowflake account within a business entity
Answer: A
uestion: 366
Credit charges for Snowflake virtual warehouses are calculated based on which of the following considerations? (Choose two.)
A. The number of queries executed
B. The number of active users assigned to the warehouse
C. The size of the virtual warehouse
D. The length of time the warehouse is running
E. The duration of the queries that are executed
Answer: C, D
Question: 370
How does Snowflake allow a data provider with an Azure account in central Canada to share data with a data consumer on AWS in Australia?
A. The data provider in Azure Central Canada can create a direct share to AWS Asia Pacific, if they are both in the same organization.
B. The data consumer and data provider can form a Data Exchange within the same organization to create a share from Azure Central Canada to AWS Asia Pacific.
C. The data provider uses the GET DATA workflow in the Snowflake Data Marketplace to create a share between Azure Central Canada and AWS Asia Pacific.
D. The data provider must replicate the database to a secondary account in AWS Asia Pacific within the same organization then create a share to the data consumer's account.
Answer:D
Explanation:
Question: 372
The first user assigned to a new account, ACCOUNTADMIN, should create at least one additional user with which administrative privilege?
A. USERADMIN
B. PUBLIC
C. ORGADMIN
D. SYSADMIN
Answer: D
uestion: 374
Which data type can store more than one type of data structure?
A. JSON
B. BINARY
C. VARCHAR
D. VARIANT
Answer: D
Explanation:
Explanation:
Question: 377
How would a user run a multi-cluster warehouse in maximized mode?
A. Configure the maximum clusters setting to "Maximum."
B. Turn on the additional clusters manually after starting the warehouse.
C. Set the minimum Clusters and maximum Clusters settings to the same value.
D. Set the minimum clusters and maximum clusters settings to different values.
Answer: D
Question: 378
Data storage for individual tables can be monitored using which commands and/or objects? (Choose two.)
A. SHOW STORAGE BY TABLE;
B. SHOW TABLES;
C. Information Schema -> TABLE_HISTORY
D. Information Schema -> TABLE_FUNCTION
E. Information Schema -> TABLE_STORAGE_METRICS
Answer: B, E
Explanation:
Explanation:
Question: 380
How would a user execute a series of SQL statements using a task?
A. Include the SQL statements in the body of the task CREATE TASK mytask .. AS INSERT INTO target1 SELECT .. FROM stream_s1 WHERE .. INSERT INTO target2 SELECT .. FROM stream_s1 WHERE ..
B. A stored procedure can have only one DML statement per stored procedure invocation and therefore the user should sequence stored procedure calls in the task definition CREATE TASK mytask .... AS call stored_proc1(); call stored_proc2();
C. Use a stored procedure executing multiple SQL statements and invoke the stored procedure from the task. CREATE TASK mytask .... AS call stored_proc_multiple_statements_inside();
D. Create a task for each SQL statement (e.g. resulting in task1, task2, etc.) and string the series of SQL statements by having a control task calling task1, task2, etc. sequentially.
Answer: C
Explanation:
uestion: 381
What is the minimum Snowflake edition needed for database failover and fail-back between Snowflake accounts for business continuity and disaster recovery?
A. Standard
B. Enterprise
C. Business Critical
D. Virtual Private Snowflake
Answer: B
Explanation:
Question: 382
A user has a standard multi-cluster warehouse auto-scaling policy in place.
Which condition will trigger a cluster to shut-down?
A. When after 2-3 consecutive checks the system determines that the load on the most-loaded cluster could be redistributed.
B. When after 5-6 consecutive checks the system determines that the load on the most-loaded cluster could be redistributed.
C. When after 5-6 consecutive checks the system determines that the load on the least-loaded cluster could be redistributed.
D. When after 2-3 consecutive checks the system determines that the load on the least-loaded cluster could be redistributed.
Answer: D
Explanation:
Question: 383
What is cached during a query on a virtual warehouse?
A. All columns in a micro-partition
B. Any columns accessed during the query
C. The columns in the result set of the query
D. All rows accessed during the query
Answer: B
Explanation:
Question: 384
Which of the following activities consume virtual warehouse credits in the Snowflake environment?
(Choose two.)
A. Caching query results
B. Running EXPLAIN and SHOW commands
C. Cloning a database
D. Running a custom query
E. Running COPY commands
Answer: D,E
Explanation:
Question: 385
A data provider wants to share data with a consumer who does not have a Snowflake account. The provider creates a reader account for the consumer following these steps:
1. Created a user called "CONSUMER"
2. Created a database to hold the share and an extra-small warehouse to query the data
3. Granted the role PUBLIC the following privileges: Usage on the warehouse, database, and schema, and SELECT on all the objects in the share Based on this configuration what is true of the reader account?
A. The reader account will automatically use the Standard edition of Snowflake.
B. The reader account compute will be billed to the provider account.
C. The reader account can clone data the provider has shared, but cannot re-share it.
D. The reader account can create a copy of the shared data using CREATE TABLE AS...
Answer: C
uestion: 389
Which of the following are characteristics of security in Snowflake?
A. Account and user authentication is only available with the Snowflake Business Critical edition.
B. Support for HIPAA and GDPR compliance is available for UI Snowflake editions.
C. Periodic rekeying of encrypted data is available with the Snowflake Enterprise edition and higher
D. Private communication to internal stages is allowed in the Snowflake Enterprise edition and higher.
Answer: C
Question: 393
A company needs to read multiple terabytes of data for an initial load as part of a Snowflake migration. The company can control the number and size of CSV extract files.
How does Snowflake recommend maximizing the load performance?
A. Use auto-ingest Snowpipes to load large files in a serverless model.
B. Produce the largest files possible, reducing the overall number of files to process.
C. Produce a larger number of smaller files and process the ingestion with size Small virtual warehouses.
D. Use an external tool to issue batched row-by-row inserts within BEGIN TRANSACTION and COMMIT commands.
Ans C
Question: 394
If a Snowflake user decides a table should be clustered, what should be used as the cluster key?
A. The columns that are queried in the select clause.
B. The columns with very high cardinality.
C. The columns with many different values.
D. The columns most actively used in the select filters.
Answer: D
Question: 395
What is the MINIMUM Snowflake edition required to use the periodic rekeying of micro-partitions?
A. Enterprise
B. Business Critical
C. Standard
D. Virtual Private Snowflake
Answer: A
Question: 396
What privilege should a user be granted to change permissions for new objects in a managed access schema?
A. Grant the OWNERSHIP privilege on the schema.
B. Grant the OWNERSHIP privilege on the database.
C. Grant the MANAGE GRANTS global privilege.
D. Grant ALL privileges on the schema.
Answer: C
Question: 398
Which privilege must be granted to a share to allow secure views the ability to reference data in multiple databases?
A. CREATE_SHARE on the account
B. SHARE on databases and schemas
C. SELECT on tables used by the secure view
D. REFERENCE_USAGE on databases
Answer: D
Question: 400
A Snowflake user executed a query and received the results. Another user executed the same query 4 hours later. The data had not changed. What will occur?
A. No virtual warehouse will be used, data will be read from the result cache.
B. No virtual warehouse will be used, data will be read from the local disk cache.
C. The default virtual warehouse will be used to read all data.
D. The virtual warehouse that is defined at the session level will be used to read all data.
Answer: A
Question: 401
Which statements reflect key functionalities of a Snowflake Data Exchange? (Choose two.)
A. If an account is enrolled with a Data Exchange, it will lose its access to the Snowflake Marketplace.
B. A Data Exchange allows groups of accounts to share data privately among the accounts.
C. A Data Exchange allows accounts to share data with third, non-Snowflake parties.
D. Data Exchange functionality is available by default in accounts using the Enterprise edition or higher.
E. The sharing of data in a Data Exchange is bidirectional. An account can be a provider for some datasets and a consumer for others.
Answer: B,E
Question: 402
Which database objects can be shared with the Snowflake secure data sharing feature? (Choose two.)
A. Files
B. External tables
C. Secure User-Defined Functions (UDFs)
D. Sequences
E. Streams
Answer: B, C
Question: 403
Query parsing and compilation occurs in which architecture layer of the Snowflake Cloud Data Platform?
A. Cloud services layer
B. Compute layer
C. Storage layer
D. Cloud agnostic layer
Answer: A
Question: 406
Which statement describes pruning?
A. The filtering or disregarding of micro-partitions that are not needed to return a query.
B. The return of micro-partitions values that overlap with each other to reduce a query's runtime.
C. A service that is handled by the Snowflake Cloud Services layer to optimize caching.
D. The ability to allow the result of a query to be accessed as if it were a table.
Answer: A
Question: 409
Using variables in Snowflake is denoted by using which SQL character?
A. @
B. &
C. $
D. #
Answer: C
Question: 410
What happens to the shared objects for users in a consumer account from a share, once a database has been created in that account?
A. The shared objects are transferred.
B. The shared objects are copied.
C. The shared objects become accessible.
D. The shared objects can be re-shared.
Answer: C
Question: 411
Which parameter can be used to instruct a COPY command to verify data files instead of loading them into a specified table?
A. STRIP_NULL_VALUES
B. SKIP_BYTE_ORDER_MARK
C. REPLACE_INVALID_CHARACTERS
D. VALIDATION_MODE
Answer: D
Which of the following statements describe features of Snowflake data caching? (Choose two.)
A. When a virtual warehouse is suspended, the data cache is saved on the remote storage layer.
B. When the data cache is full, the least-recently used data will be cleared to make room.  
C. A user can only access their own queries from the query result cache.
D. A user must set USE_METADATA_CACHE to TRUE to use the metadata cache in queries.
E. The RESULT_SCAN table function can access and filter the contents of the query result cache.  

Question: 247
Which semi-structured file formats are supported when unloading data from a table? (Select THREE).
A. ORC
B. XML
C. Avro
D. Parquet
E. JSON
Answer: D, E, C
Explanation:
Semi-structured JSON, Parquet
Question: 244
Which of the following describes external functions in Snowflake?
A. They are a type of User-defined Function (UDF).
B. They contain their own SQL code.
C. They call code that is stored inside of Snowflake.
D. They can return multiple rows for each row received
Answer: A
Question: 250
What is the purpose of multi-cluster virtual warehouses?
A. To create separate data warehouses to increase query optimization
B. To allow users the ability to choose the type of compute nodes that make up a virtual warehouse cluster
C. To eliminate or reduce Queuing of concurrent queries
D. To allow the warehouse to resize automatically
Answer: C
Question: 257
True or False: Snowpipe via REST API can only reference External Stages as source.
A. True
B. False
Answer: B
Question: 264
What type of query benefits the MOST from search optimization?
A. A query that uses only disjunction (i.e., OR) predicates
B. A query that includes analytical expressions
C. A query that uses equality predicates or predicates that use IN
D. A query that filters on semi-structured data types
Answer: C
Question 520
### 2. Which of the following statements about Secure Data Sharing in Snowflake are TRUE?  
A) Shared data is copied to the consumer’s account and incurs storage charges.  
B) Consumers are charged only for compute resources used to query shared data.  
C) Shared objects are read-only for the consumer.  
D) The provider can revoke access to a share at any time.  
E) Shared data can be modified by the consumer if the provider allows it.  
BCD
Question: 46
How many shares can be consumed by single Data Consumer?
A. 1
B. 10
C. 100, but can be increased by contacting support
D. Unlimited
Answer: D
Question: 59
Which of the following statements are true of Snowflake data loading?
Choose 3 answers
A. VARIANT "null" values are not the same as SQL Null values
B. It is recommended to do frequent, single row DMLS
C. It is recommended to validate the data before loading into the Snowflake target table
D. It is recommended to use staging tables to manage MERGE statements
Answer: A, C, D
Question: 63
As a best practice, clustering keys should only be defined on tables of which minimum size?
A. Multi-Kilobyte (KB) Range
B. Multi-Megabyte (MB) Range
C. Multi-Gigabyte (GB) Range
D. Multi-Terabyte (TB) Range
Question: 65
Fail-safe is unavailable on which table types?
A. Temporary
B. Transient
C. Provisional
D. Permanent
Answer: A, B
Question: 81
Which of the following statements is true of data loading?
Select one.
A. Resizing the virtual warehouse from x-Small to Small will process a single file twice as fast
B. The "deal file size for loading is 16MB to match micro-partition size
C. Many files in the 10-lOOMB range tend to land In the 'sweet spot" for load parallelism Once loaded, there is no option to force a reload of an already loaded file
Answer: C
Question: 86
The number of queries that a Virtual Warehouse can concurrently process is determined by:
Choose 2 answers
A. The complexity of each query
B. The CONCURRENT_QUERY_UMIT parameter set on the Snowflake account
C. The size of the data required for each query
D. The tool that s executing the query
Answer: A, C
Question: 92
True or False: Snowflake allows its customers to directly access the micro-partition files that make up its tables.
A. True
B. False
Answer: B
Question: 116
Which interfaces can be used to create and/or manage Virtual Warehouses?
A. The Snowflake Web Interface (UI)
B. SQL commands
C. Data integration tools
D. All of the above
Answer: D
Question: 118
What is the minimum Snowflake edition that customers planning on storing protected information in
Snowflake should consider for regulatory compliance?
A. Standard
B. Premier
C. Enterprise
D. Business Critical Edition
Answer: D
AQuestion: 138
True or False: AWS Private Link provides a secure connection from the Customer’s on-premise data center to the Snowflake.
A. True
B. False
Ans B
Question: 270
What are supported file formats for unloading data from Snowflake? (Choose three.)
A. XML
B. JSON
C. Parquet
D. ORC
E. AVRO
F. CSV
Answer: B, C, F
Question: 145
True or False: Snowflake supports federated authentication in all editions.
A. True
B. False
Answer: A
Question: 145
True or False: When a new Snowflake object is created, it is automatically owned by the user who
created it.
A. True
B. False
Ans A
Question: 171
Which of the following indicates that it may be appropriate to use a clustering key for a table? (Select TWO).
A. The table contains a column that has very low cardinality
B. DML statements that are being issued against the table are blocked
C. The table has a small number of micro-partitions
D. Queries on the table are running slower than expected
E. The clustering depth for the table is large
Answer: D, E
Explanation:
Question: 172
Which Snowflake object enables loading data from files as soon as they are available in a cloud storage location?
A. Pipe
B. External stage
C. Task
D. Stream
Answer: A
Question: 174
What is the default character set used when loading CSV files into Snowflake?
A. UTF-8
B. UTF-16
C. ISO S859-1
D. ANSI_X3.A
Answer: A

Question: 188
How often are encryption keys automatically rotated by Snowflake?
A. 30 Days
B. 60 Days
C. 90 Days
D. 365 Days
Answer: A
Question: 189
What are value types that a VARIANT column can store? (Select TWO)
A. STRUCT
B. OBJECT
C. BINARY
D. ARRAY
E. CLOB
Answer: B, D

Question: 194 
What happens when a cloned table is replicated to a secondary database? (Select TWO) 
A. A read-only copy of the cloned tables is stored. 
B. The replication will not be successful. 
C. The physical data is replicated 
D. Additional costs for storage are charged to a secondary account 
E. Metadata pointers to cloned tables are replicated 
A, E
 
Question: 195 
Which data types does Snowflake support when querying semi-structured data? (Select TWO) 
A. VARIANT 
B. ARRAY 
C. VARCHAR 
D. XML 
E. BLOB 
 
Question: 212 
Which account__usage views are used to evaluate the details of dynamic data masking? (Select 
TWO) 
A. ROLES 
B. POLICY_REFERENCES 
C. QUERY_HISTORY 
D. RESOURCE_MONITORS 
E. ACCESS_HISTORY 
F. MASKING_POLICIES 
Answer: B,F 
 
 
Question: 271 
The Snowflake cloud services layer is responsible for which tasks? (Choose two.) 
A. Local disk caching 
B. Authentication and access control 
C. Metadata management 
D. Query processing 
E. Database storage 
Answer: B, C, D 
 
Question: 412 
Which Snowflake tool would be BEST to troubleshoot network connectivity? 
A. SnowCLI 
B. SnowUI 
C. SnowSQL 
D. SnowCD 
Answer: D 
 
Question: 407 
How can a user change which columns are referenced in a view? 
A. Modify the columns in the underlying table 
B. Use the ALTER VIEW command to update the view 
C. Recreate the view with the required changes 
D. Materialize the view to perform the changes 
Answer: C 
 
Question: 405 
Which SQL command can be used to see the CREATE definition of a masking policy? 
A. SHOW MASKING POLICIES 
B. DESCRIBE MASKING POLICY 
C. GET_DDL 
D. LIST MASKING POLICIES 
Answer: C 
Explanation: 
Wrong 
 
Question: 277 
What are the responsibilities of Snowflake's Cloud Service layer? (Choose three.) 
A. Authentication 
B. Resource management 
C. Virtual warehouse caching 
D. Query parsing and optimization 
E. Query execution 
F. Physical storage of micro-partitions 
Answer: A, B, D 
 Question: 399 
Which feature allows a user the ability to control the organization of data in a micro-partition? 
A. Range Partitioning 
B. Search Optimization Service 
C. Automatic Clustering 
D. Horizontal Partitioning 
Answer: C 
Question: 397 
A materialized view should be created when which of the following occurs? (Choose two.) 
A. There is minimal cost associated with running the query. 
B. The query consumes many compute resources every time it runs. 
C. The base table gets updated frequently. 
D. The query is highly optimized and does not consume many compute resources. 
E. The results of the query do not change often and are used frequently. 
Answer: B, E 
Explanation: 
 Question: 391 
The bulk data load history that is available upon completion of the COPY statement is stored where and for how long? 
A. In the metadata of the target table for 14 days 
B. In the metadata of the pipe for 14 days 
C. In the metadata of the target table for 64 days 
D. In the metadata of the pipe for 64 days 
Answer:A 

 Question: 390 
User INQUISITIVE_PERSON has been granted the role DATA_SCIENCE. The role DATA_SCIENCE has privileges OWNERSHIP on the schema MARKETING of the database ANALYTICS_DW. 
Which command will show all privileges granted to that schema? 
A. SHOW GRANTS ON ROLE DATA_SCIENCE 
B. SHOW GRANTS ON SCHEMA ANALYTICS_DW.MARKETING 
C. SHOW GRANTS TO USER INQUISITIVE_PERSON 
D. SHOW GRANTS OF ROLE DATA_SCIENCE 
Answer: B 

 Question: 387 
What effect does WAIT_FOR_COMPLETION = TRUE have when running an ALTER WAREHOUSE command and changing the warehouse size? 
A. The warehouse size does not change until all queries currently running in the warehouse have completed. 
B. The warehouse size does not change until all queries currently in the warehouse queue have completed. 
C. The warehouse size does not change until the warehouse is suspended and restarted. 
D. It does not return from the command until the warehouse has finished changing its size. 
Answer: D 
Question: 214 
Which is the MINIMUM required Snowflake edition that a user must have if they want to use AWS/Azure Privatelink or Google Cloud Private Service Connect? 
A. Standard 
B. Premium 
C. Enterprise 
D. Business Critical 
Answer: D 

Question: 230
What is a machine learning and data science partner within the Snowflake Partner Ecosystem?
A. Informatica
B. Power Bl
C. Adobe
D. Data Robot
Answer: D

Question: 216
A marketing co-worker has requested the ability to change a warehouse size on their medium virtual warehouse called mktg_WH.
Which of the following statements will accommodate this request?
A. ALLOW RESIZE ON WAREHOUSE MKTG_WH TO USER MKTG__LEAD;
B. GRANT MODIFY ON WAREHOUSE MKTG_WH TO ROLE MARKETING;
C. GRANT MODIFY ON WAREHOUSE MKTG_WH TO USER MKTG__LEAD;
D. GRANT OPERATE ON WAREHOUSE MKTG_WH TO ROLE MARKETING;
Answer: 

Question: 165
Which of the following are benefits of micro-partitioning? (Select TWO)
A. Micro-partitions cannot overlap in their range of values
B. Micro-partitions are immutable objects that support the use of Time Travel.
C. Micro-partitions can reduce the amount of I/O from object storage to virtual warehouses
D. Rows are automatically stored in sorted order within micro-partitions
E. Micro-partitions can be defined on a schema-by-schema basis
Answer: B, C

Question: 262
Which of the following describes the Snowflake Cloud Services layer?
A. Coordinates activities in the Snowflake account
B. Executes queries submitted by the Snowflake account users
C. Manages quotas on the Snowflake account storage
D. Manages the virtual warehouse cache to speed up queries
Answer: A

B### 1. Which of the following Snowflake objects can be shared using Secure Data Sharing?  
A) Databases  
B) Tables  
C) Secure Views  
D) Virtual Warehouses  
E) User-defined Functions (UDFs)  
**Correct answers:** A, B, C, E

Question 524
### 6. Which Snowflake sharing option allows you to manage a group of accounts and offer a share to that group?  (1) 
A) Data Exchange  
B) Direct Share  
C) Data Listing  
D) Clean Room  
E) Secure View  
Ans A
Question: 242
When unloading to a stage, which of the following is a recommended practice or approach?
A. Set SINGLE: = true for larger files
B. Use OBJECT_CONSTRUCT ( * ) when using Parquet
C. Avoid the use of the CAST function
D. Define an individual file format
Ans D 
Question: 73
True or False: When a user creates a role, they are initially assigned ownership of the role and they maintain ownership until it is transferred to another user.
A. True
B. False
Answer: A

Question: 275
What is the SNOWFLAKE.ACCOUNT_USAGE view that contains information about which objects were read by queries within the last 365 days (1 year)?
A. VIEWS_HISTORY
B. OBJECT_HISTORY
C. ACCESS_HISTORY
D. LOGIN_HISTORY
Answer: C

 Question: 408 
Which commands should be used to grant the privilege allowing a role to select data from all current tables and any tables that will be created later in a schema? (Choose two.) 
A. grant USAGE on all tables in schema DB1.SCHEMA to role MYROLE; 
B. grant USAGE on future tables in schema DB1.SCHEMA to role MYROLE; 
C. grant SELECT on all tables in schema DB1.SCHEMA to role MYROLE; 
D. grant SELECT on future tables in schema DB1.SCHEMA to role MYROLE; 
E. grant SELECT on all tables in database DB1 to role MYROLE; 
F. grant SELECT on future tables in database DB1 to role MYROLE; 
Answer: C, D
**Question 520. What is the primary benefit of assigning a USAGE privilege on a schema that contains sensitive data?**  
A) It enables row-level security  
B) It allows access requests for sensitive data without role duplication  
C) It grants all CRUD operations  
D) It allows schema deletion  
**Answer:** B) It allows access requests for sensitive data without role duplication


**Question 521. In the recommended pattern, how are access roles typically inherited?**  
A) From top-level to all users  




Question: 77
True or False: When active, a pipe requires a dedicated Virtual Warehouse to execute.
A. True
B. False
Answer: B



Question: 102
Which of the following roles is recommended to be used to create and manage users and roles?
A. SYSADMIN
B. SECURITYADMIN
C. PUBLIC
D. ACCOUNTADMIN
Ans B
Question: 404
Which of the following is the Snowflake Account_Usage.Metering_History view used for?
A. Gathering the hourly credit usage for an account
B. Compiling an account's average cloud services cost over the previous month
C. Summarizing the throughput of Snowpipe costs for an account
D. Calculating the funds left on an account's contract
Answer: A
Question: 115
The PUT command: (Choose two.)
A. Automatically creates a File Format object
B. Automatically uses the last Stage created
C. Automatically compresses files using Gzip
D. Automatically encrypts files
Answer: C,D

Question: 158
True or False: When you create a custom role, it is a best practice to immediately grant that role to ACCOUNTADMIN.
A. True
B. False
Answer: B

Which of the following statements are true of Virtual Warehouses? (Choose all that apply.)
A. Customers can change the size of the Warehouse after creation
B. A Warehouse can be resized while running
C. A Warehouse can be configured to suspend after a period of inactivity
D. A Warehouse can be configured to auto-resume when new queries are submitted
Answer: A,B,C,D



Question: 226
Which of the following compute resources or features are managed by Snowflake? (Select TWO).
A. Execute a COPY command
B. Updating data
C. Snowpipe
D. AUTOMATIC__CLUSTERING
E. Scaling up a warehouse
Answer: C, D

uestion: 183
Which of the following objects can be shared through secure data sharing? (1) 
A. Masking policy
B. Stored procedure
C. Task
D. External table
Answer: D
Question: 175
A sales table FCT_SALES has 100 million records.
The following Query was executed
SELECT COUNT (1) FROM FCT__SALES; ?The query has not been run before
How did Snowflake fulfill this query?
A. Query against the result set cache
B. Query against a virtual warehouse cache
C. Query against the most-recently created micro-partition
D. Query against the metadata excite
Ans  D 
Question: 133
What privileges are required to create a task?
A. The global privilege create task is required to create a new task.
B. Tasks are created at the Application level and can only be created by the Account Admin role.
C. Many Snowflake DDLs are metadata operations only, and create task DDL can be executed without virtual warehouse requirement or task specific grants.
D. The role must have access to the target schema and the create task privilege on the schema itself.
Ans D

Question: 235
A company strongly encourages all Snowflake users to self-enroll in Snowflake's default Multi-Factor Authentication (MFA) service to provide increased login security for users connecting to Snowflake.
Which application will the Snowflake users need to install on their devices in order to connect with MFA?
A. Okta Verify
B. Duo Mobile
C. Microsoft Authenticator
D. Google Authenticator
Ans C 
Question 515
Which of the following statements are FALSE concerning a data consumer account in Snowflake?
A. A single consumer account can contain objects from different providers
B. A consumer account can create clones of a shared database
C. A consumer account can perform time travel on a table within the shared database
D. A consumer account cannot forward (i.e. reshare) the shared databases and objects
B & C 
Question: 254
If 3 size Small virtual warehouse is made up of two servers, how many servers make up a Large warehouse?
A. 4
B. 8
C. 16
D. 32
Answer: c small, medium, karge 2,4, 8 
Explanation:
Size specifies the amount of compute resources available per cluster in a warehouse. Snowflake supports the following warehouse sizes:

Question 531
### 13. Which of these can be revoked at any time by the provider?  (2) 
A) Access to a share  
B) Access to a specific object in a share  
C) Access to the entire Snowflake account  
D) Access to the provider’s virtual warehouse  
E) Access to external tables only  
**Correct answers:** A, B
Question: 117
When a Pipe is recreated using the CREATE OR REPLACE PIPE command:
A. The Pipe load history is reset to empty
B. The REFRESH parameter is set to TRUE
C. Previously loaded files will be ignored
D. All of the above
Answer: A

Question: 50
True or False: When Snowflake is configured to use Single Sign-on (sso), Snowflake receive the usernames and credentials from the sso service and loads them into the customer's Snowflake account.
. True
B. false:
Answer: B

Question: 177
What is a key feature of Snowflake architecture? (1)
A. Zero-copy cloning creates a mirror copy of a database that updates with the original
B. Software updates are automatically applied on a quarterly basis
C. Snowflake eliminates resource contention with its virtual warehouse implementation
D. Multi-cluster warehouses allow users to run a query that spans across multiple clusters
E. Snowflake automatically sorts DATE columns during ingest for fast retrieval by date
Answer: C
Question: 388 
Which formats does Snowflake store unstructured data in? (Choose two.) 
A. GeoJSON 
B. Array 
C. XML 
D. Object 
E. BLOB 
Answer:C D 
Question: 106
True or False: The COPY command must specify a File Format in order to execute.
A. True
B. False
Answer: B
Question: 224
What transformations are supported in a CREATE PIPE ... AS COPY ... FROM (....) statement? (Select TWO.)
A. Data can be filtered by an optional where clause
B. Incoming data can be joined with other tables
C. Columns can be reordered
D. Columns can be omitted
E. Row level access can be defined
Answer: a, D
 Question: 272
When publishing a Snowflake Data Marketplace listing into a remote region what should be taken into consideration? (Choose two.)
A. There is no need to have a Snowflake account in the target region, a share will be created for each user.
B. The listing is replicated into all selected regions automatically, the data is not.
C. The user must have the ORGADMIN role available in at least one account to link accounts for replication.
D. Shares attached to listings in remote regions can be viewed from any account in an organization.
E. For a standard listing the user can wait until the first customer requests the data before replicating it to the target region.
Answer: B, E

B### 1. Which of the following Snowflake objects can be shared using Secure Data Sharing?  
A) Databases  
B) Tables  
C) Secure Views  
D) Virtual Warehouses  
E) User-defined Functions (UDFs)  
**Correct answers:** A, B, C, E

Question: 242
When unloading to a stage, which of the following is a recommended practice or approach?
A. Set SINGLE: = true for larger files
B. Use OBJECT_CONSTRUCT ( * ) when using Parquet
C. Avoid the use of the CAST function
D. Define an individual file format
Ans D 

• Question 501
What built-in Snowflake features support automatic data clustering? (Choose two.)
• A. Automatic Clustering Service
• B. Materialized Views
• C. Streams
• D. Tasks
• E. External Tables
 A, B 
Question 504
Which Snowflake features help reduce costs by minimizing compute usage? (Choose two.)
• A. Auto-suspend warehouses
• B. Multi-cluster warehouses
• C. Result caching
• D. Streams
• E. Cloning
 AC
Question 505
Which of the following are required to enable time travel in Snowflake? (Choose two.)
• A. Setting the DATA_RETENTION_TIME_IN_DAYS parameter
• B. Enabling automatic clustering
• C. Creating a stream on the table
• D. Using transient or permanent tables
• E. Granting SE
LECT privilege to users
AD
Question 506
When using the Snowflake Connector for Kafka, what data formats are supported for the messages? (Choose two.)
A. CSV
B. XML
• C. Avro 
• D. JSON  
• E. Parquet
C D 
 Question 507
Which compression types are supported by Snowflake for loading data files? (Choose two.)
• A. LZ4
• B. GZIP  
• C. DEFLATE
• D. XZ
• E. ZIP  
BE
 Question 508
n Architect uses COPY INTO with the ON_ERROR=SKIP_FILE option to bulk load CSV files into a table called TABLEA, using its table stage. One file named file5.csv fails to load. The Architect fixes the file and re-loads it to the stage with the exact same file name it had previously.
which commands should the Architect use to load only file5.csv file from the stage? (Choose two.)
• A. COPY INTO tablea FROM @%tablea RETURN_FAILED_ONLY = TRUE;
• B. COPY INTO tablea FROM @%tablea;  
• C. COPY INTO tablea FROM @%tablea FILES = ('file5.csv'); 
• D. COPY INTO tablea FROM @%tablea FORCE = TRUE;
• E. COPY INTO tablea FROM @%tablea NEW_FILES_ONLY = TRUE;
• F. COPY INTO tablea FROM @%tablea MERGE = TRUE;
• Correct Answers:
• C. FILES = ('file5.csv') allows you to specify exactly which file(s) to load, regardless of their load history.
• D. FORCE = TRUE forces Snowflake to re-load the file even if it has the same name and has already been loaded before.
Other options explained:
• A. RETURN_FAILED_ONLY = TRUE only returns information about failed files, it doesn’t control which files are loaded.
• B. (no file selection) would try to load all eligible files, not just file5.csv.
• E. NEW_FILES_ONLY = TRUE only loads files that have not been loaded before, but file5.csv (with the same name) was already processed, so this would not reload it.
• F. MERGE = TRUE is not a valid COPY INTO option.
 
Question 510
A user has the appropriate privilege to see unmasked data in a column.
If the user loads this column data into another column that does not have a masking policy, what will occur?
• A. Unmasked data will be loaded in the new column.
• B. Masked data will be loaded into the new column.
• C. Unmasked data will be loaded into the new column but only users with the appropriate privileges will be able to see the unmasked data.
• D. Unmasked data will be loaded into the new column and no users will be able to see the unmasked data.

 A 

Question 513
Company A would like to share data in Snowflake with Company B. Company B is not on the same cloud platform as Company A.
What is required to allow data sharing between these two companies?
• A. Create a pipeline to write shared data to a cloud storage location in the target cloud provider.
• B. Ensure that all views are persisted, as views cannot be shared across cloud platforms.
• C. Setup data replication to the region and cloud platform where the consumer resides.
• D. Company A and Company B must agree to use a single cloud platform: Data sharing is only possible if the companies share the same cloud provider. 

C

Question 514
Which of the following statements are TRUE concerning a data consumer account in Snowflake? 
A. All objects in the shared database are always read-only for the consumer of the share
B. A consumer of a share does not necessarily need an account with Snowflake to be able to consume data
C. A consumer account must be in the same region and on the same cloud provider as the data provider account
D. A consumer needs to pay for the compute and storage of the shared data
E. A database ‘share’ can be ‘imported’ by at most 10 consumer accounts
Correct​ ​Answers: A and B


Question 521
### 3. Which of these are valid options for sharing data in Snowflake?  
A) Data Listing  
B) Data Exchange  
C) Clean Room  
D) Email Attachment  
E) Direct Share  
**Correct answers:** A, B, C, E

Question 522
### 4. Regarding reader accounts in Snowflake, which statements are correct?  
A) Reader accounts can only consume data from the provider account that created them.  
B) Reader accounts can perform DML operations such as insert and update.  
C) Reader accounts allow sharing data with consumers who do not have a full Snowflake account.  
D) Each reader account belongs to the provider account that created it.  
E) Reader accounts can create new shares for other accounts.  
**Correct answers:** A, C, D

Question 523
### 5. Which of the following objects are always read-only for the consumer in Secure Data Sharing?  
A) Tables  
B) Databases  
C) Secure Views  
D) Virtual Warehouses  
E) Dynamic Tables  
**Correct answers:** A, B, C, E


Question 525
### 7. What happens if a provider adds new objects to a share?  
A) The consumer must manually refresh to see new objects.  
B) The new objects become immediately available to all consumers.  
C) Consumers are notified by email.  
D) The provider must create a new share.  
E) There is a delay of up to 24 hours before consumers see new objects.  
**Correct answers:** B

Question 526
### 8. In Snowflake, what is a "share"?  
A) A user account with special privileges  
B) A named object that encapsulates all the information required to share a database  
C) A virtual warehouse for sharing compute  
D) A type of secure view  
E) An S3 bucket for external storage  
**Correct answers:** B

Question 527
### 9. Which of the following can be included in a share?  
A) Tables  
B) Secure materialized views  
C) User-defined functions  
D) Virtual warehouses  
E) External tables  
**Correct answers:** A, B, C, E

Question 528
### 10. How is access to shared data controlled in a consumer account?  
A) By configuring a firewall  
B) Using role-based access control  
C) By sharing compute resources  
D) By creating a private link  
E) By granting object ownership  
**Correct answers:** B

Question 529
### 11. Which of these is NOT a method for sharing data in Snowflake?  
A) Direct Share  
B) Clean Room  
C) External Table Share  
D) Data Listing  
E) Data Exchange  
**Correct answers:** C

Question 530
### 12. What are the charges for a consumer when accessing shared data in Snowflake?  
A) Storage charges for shared data  
B) Compute charges when querying shared data  
C) Charges for creating a share  
D) Network transfer fees for shared data  
E) No charges at all  

**Correct answers:** B
Question 532
### 14. Which statements about third-party (reader) accounts are correct?  
A) They are created by the data provider.  
B) They can consume data from any Snowflake provider.  
C) Users can query imported data but not perform DML operations.  
D) They require a separate Snowflake license.  
E) Each belongs to the provider account that created it.  
**Correct answers:** A, C, E

Question 533
### 15. What is true about the data flow for Secure Data Sharing?  
A) Data is physically copied to the consumer's account.  
B) Only metadata and access rights are managed; no data is copied.  
C) Shared data does not use storage in the consumer account.  
D) Consumers are charged for both storage and compute.  
E) Access is near-instantaneous for consumers.  
**Correct answers:** B, C, E

**Question 519. Which privilege must a role have to view a schema in Snowflake?**  
A) USAGE privilege on the database  
B) SELECT privilege on the schema  
C) USAGE privilege on the schema  
D) INSERT privilege on the schema  
**Answer:** C) USAGE privilege on the schema


**Question 522. What is the main reason for avoiding a fork in the RBAC hierarchy for sensitive data?**  
A) To minimize number of objects  
B) To reduce privilege escalation  
C) To simplify role management and avoid duplication  
D) To enable external table access  
**Answer:** C) To simplify role management and avoid duplication
 
**Question 523. Which privilege must be granted to a role before it can access objects within a schema?**  
A) INSERT  
B) USAGE  
C) ALTER  
D) DROP  
**Answer:** B) USAGE

**Question 526. 
If a user requires access to both sensitive and non-sensitive data in the same database, what must be done?**  
A) Duplicate all roles  
B) Assign both functional and sensitive roles  
C) Grant full admin rights  
D) Move data to separate databases  
**Answer:** B) Assign both functional and sensitive roles

**Question 528. 
Which statement best describes the hierarchy of objects in Snowflake?**  
A) Table > Schema > Database  
B) Schema > Table > Database  
C) Database > Schema > Table  
D) Database > Table > Schema  
**Answer:** C) Database > Schema > Table
**Question 529. 
What must be present for a user’s role to grant object-level access in a schema?**  
A) USAGE privilege on both database and schema  
B) SELECT on all tables  
C) DROP on all objects  
D) CREATE on the schema only  
**Answer:** A) USAGE privilege on both database and schema
**Question 530. 
How are sensitive and non-sensitive data classifications handled in the described pattern?**  
A) Using separate databases  
B) Using separate schemas within the same database  
C) By duplicating all privileges  
D) By using external tables only  
**Answer:** B) Using separate schemas within the same database
 **Question 533. 
What advantage does the simplified RBAC hierarchy provide?**  
A) More granular privileges  
B) Easier extensibility and management  
C) Unlimited role creation  
D) Automatic privilege escalation  
**Answer:** B) Easier extensibility and management
**Question 534. 
Which authentication method is considered least secure for accessing Snowflake?**  
A) Built-in username/password  
B) Key pair authentication  
C) Federated SSO  
D) External OAuth  
**Answer:** A) Built-in username/password
**Question 535. 
What is a primary benefit of using federated authentication with Snowflake?**  
A) Reduces the number of passwords users must manage  
B) Requires manual credential rotation  
C) Only works for non-human users  
D) Uses passwords stored in Snowflake only  
**Answer:** A) Reduces the number of passwords users must manage
**Question 538. 
SAML SSO in Snowflake is best suited for which type of users?**  
A) Service accounts  
B) Programmatic users  
C) Human, interactive users  
D) Machine learning scripts  
**Answer:** C) Human, interactive users
**Question 539. 
Which authentication method allows for the aggressive rotation of credentials without disrupting connectivity?**  
A) OAuth  
B) Username/password  
C) Key pair authentication  
D) SAML SSO  
**Answer:** C) Key pair authentication
**Question 540. 
What is a requirement for using federated SSO with Snowflake?**  
A) A SAML 2.0 compatible Identity Provider  
B) AWS PrivateLink  
C) Azure Key Vault  
D) Hashicorp Vault  
**Question 541.
 What does External OAuth provide in a programmatic scenario?**  
A) Passwordless access and centralized token management  
B) Local password storage  
C) Direct access with a single token for all users  
D) Only interactive user support  
**Answer:** A) Passwordless access and centralized token management
**Question 542. 
Which authentication option is NOT recommended by Snowflake due to security concerns?**  
A) SAML SSO  
B) Built-in username/password  
C) External OAuth  
D) Key pair authentication  
**Answer:** B) Built-in username/password
**Question 543. 
What is a limitation of SAML-based SSO in Snowflake at the time of this writing?**  
A) Only supports one Identity Provider per account  
B) Cannot be used with any endpoint  
C) Cannot be used with Tableau  
D) Requires external OAuth  
**Answer:** A) Only supports one Identity Provider per account
**Question 544. 
Which Snowflake authentication method is best for non-human users who need to avoid storing secrets in code?**  
A) Built-in username/password  
B) Key pair authentication with a secrets manager  
C) Federated SSO  
D) MFA with Duo  
**Answer:** B) Key pair authentication with a secrets manager
**Question 545. 
When connecting Power BI to Snowflake, which authentication method is most appropriate if centralized identity management is required?**  
A) Built-in username/password  
B) External OAuth  
C) SAML SSO  
D) Key pair authentication  
**Answer:** B) External OAuth
**Question 546. 
What is a benefit of configuring SSO for Snowflake with a SAML 2.0 IdP?**  
A) Users manage multiple credentials  
B) Users authenticate once for many resources  
C) Only non-human users are supported  
D) It disables MFA  
**Answer:** B) Users authenticate once for many resources
**Question 547. 
What is a scenario where SAML SSO is NOT recommended for Snowflake?**  
A) For use with Tableau  
B) For use with Power BI  
C) For Snowflake administrators whose passwords are stored in the IdP  
D) For human interactive users  
**Answer:** C) For Snowflake administrators whose passwords are stored in the IdP
**Question 548. 
Which of the following is a benefit of using External OAuth with Snowflake?**  
A) Centralized monitoring and deprovisioning of service identities  
B) Local password storage  
C) No support for SaaS applications  
D) Only supports one user  
**Answer:** A) Centralized monitoring and deprovisioning of service identities
**Question 549. 
Which method is suitable when a customer wants to manage private keys internally without using a cloud IdP?**  
A) External OAuth  
B) SAML SSO  
C) Key pair authentication  
D) Built-in username/password  
**Answer:** C) Key pair authentication
**Question 550. 
What should customers consider if an application does not support SAML SSO with Snowflake?**  
A) Only use SAML SSO anyway  
B) Use External OAuth or key pair authentication  
C) Use built-in username/password  
D) Deactivate MFA  
**Answer:** B) Use External OAuth or key pair authentication

---

**Question 554. What is NOT an appropriate scenario for key pair authentication in Snowflake?**  
A) When customers lack key infrastructure  
B) When customers want to use secrets managers  
C) When customers want to rotate keys aggressively  
D) When customers avoid storing secrets in code  
**Answer:** A) When customers lack key infrastructure

---

**Question 556. 
When should you use External OAuth for Snowflake authentication?**  
A) For programmatic access requiring central token management  
B) For human users only  
C) For scenarios requiring only local password storage  
D) Only with AWS PrivateLink  
**Answer:** A) For programmatic access requiring central token management



**Question 561. What is the most common network security pattern for Snowflake connectivity?**  
A) Using VPN tunnels  
B) Out-of-the-box connectivity with TLS 1.2 and OCSP  
C) Always using CSP PrivateLink  
D) Forcing all users through a proxy  
**Answer:** B) Out-of-the-box connectivity with TLS 1.2 and OCSP

**Question 563. What is the key function of a Snowflake Network Policy?**  
A) Encrypting user data  
B) Controlling access based on IP CIDR ranges  
C) Providing OCSP validation  
D) Managing TLS certificates  
**Answer:** B) Controlling access based on IP CIDR ranges
**Question 568. Which scenario is best suited for out-of-the-box Snowflake network protections?**  
A) Handling highly sensitive unmasked data  
B) Loading and unloading bulk data exclusively  
C) Users in the field consuming only masked or non-sensitive information  
D) Large scale server-side analytics  
**Answer:** C) Users in the field consuming only masked or non-sensitive information
**Question 570. What is a common misapplication to avoid with Snowflake Network Policies?**  
A) Applying a policy to only service accounts  
B) Using a policy for every user  
C) Using allow and deny lists  
D) Applying a policy to the account  
**Answer:** B) Using a policy for every user
**Question 571. What is a potential downside of overusing CSP private networking technologies in Snowflake connectivity?**  
A) Increased security  
B) Lower operational overhead  
C) Little additional security and high operational overhead  
D) Disables OCSP  
**Answer:** C) Little additional security and high operational overhead

---

**Question 573. What type of traffic uses an unencrypted channel (port 80) in Snowflake’s network connectivity?**  
A) Customer data  
B) OCSP (Online Certificate Status Protocol)  
C) Internal stage uploads  
D) User browser sessions  
**Answer:** B) OCSP (Online Certificate Status Protocol)
**Question 574. Why might an organization use TLS inspection, making OCSP communication moot?**  
A) To speed up TLS connections  
B) To allow unencrypted data transfer  
C) Because they terminate and re-issue certificates internally  
D) To block Snowflake access  
**Answer:** C) Because they terminate and re-issue certificates internally
**Question 575. What is the result if you block OCSP connectivity from a Snowflake client?**  
A) All connections remain unaffected  
B) Client may fail to validate TLS certificates  
C) Client disables encryption  
D) Network Policy is ignored  
**Answer:** B) Client may fail to validate TLS certificates
**Question 580. When is it appropriate to consider CSP private networking for Snowflake connectivity?**  
A) For all users  
B) Only for field users  
C) Where large volumes of data or extremely sensitive data flows  
D) Only if using VPN  
**Answer:** C) Where large volumes of data or extremely sensitive data flows
**Question 582. Which of the following is true regarding the use of External Stage in Snowflake?**  
A) It is required for all customers  
B) It is optional and uses customer’s cloud storage  
C) It replaces the Internal Stage  
D) It uses only encrypted channels  
**Answer:** B) It is optional and uses customer’s cloud storage
**Question 583. What is the main security role of TLS 1.2 in Snowflake network connectivity?**  
A) Enables VPN connections  
B) Encrypts all customer data communications  
C) Encrypts OCSP traffic only  
D) Allows multiple Network Policies  
**Answer:** B) Encrypts all customer data communications
*Question 585. Which role is a child of the SECURITYADMIN role in the default hierarchy?**  
A) SYSADMIN  
B) ACCOUNTADMIN  
C) USERADMIN  
D) PUBLIC  
**Answer:** C) USERADMIN
**Question 587. What is a recommended best practice for assigning the ACCOUNTADMIN role?**  
A) Assign it to all users  
B) Assign to a select few with MFA enabled  
C) Assign to database owners only  
D) Assign to users with no other roles  
**Answer:** B) Assign to a select few with MFA enabled
**Question 588. Why should you assign the ACCOUNTADMIN role to at least two users?**  
A) For load balancing  
B) To avoid password reset delays  
C) To grant more permissions  
D) To enable object creation  
**Answer:** B) To avoid password reset delays
**Question 589. Why should the ACCOUNTADMIN role NOT be used for object creation?**  
A) It lacks sufficient privileges  
B) It is only for billing  
C) It is intended for initial setup and account management  
D) It cannot be granted to users  
**Answer:** C) It is intended for initial setup and account management
**Question 590. What is the recommended role to use for automated scripts?**  
A) ACCOUNTADMIN  
B) USERADMIN  
C) SYSADMIN or lower in the hierarchy  
D) SECURITYADMIN  
**Answer:** C) SYSADMIN or lower in the hierarchy
**Question 591. What minimum privileges are required to query a table in schema mydb.myschema.mytable?**  
A) USAGE on database, USAGE on schema, SELECT on table  
B) USAGE on database only  
C) SELECT on table only  
D) USAGE on schema only  
**Answer:** A) USAGE on database, USAGE on schema, SELECT on table
**Question 593. What is a role hierarchy in Snowflake?**  
A) Granting users to roles directly  
B) Granting roles to other roles to form inheritance relationships  
C) Creating roles with no privileges  
D) Assigning only system roles  
**Answer:** B) Granting roles to other roles to form inheritance relationships
**Question 594. Which type of role grants access on database objects or account objects?**  
A) Functional role  
B) Object access role  
C) System role  
D) User role  
**Answer:** B) Object access role
**Question 597. Which SQL command grants read-only access to all tables in a database to a role?**  
A) GRANT SELECT ON ALL TABLES IN DATABASE <db> TO ROLE <role>;  
B) GRANT USAGE ON DATABASE <db> TO ROLE <role>;  
C) CREATE ROLE <role>;  
D) GRANT ROLE <role> TO USER <user>;  
**Answer:** A) GRANT SELECT ON ALL TABLES IN DATABASE <db> TO ROLE <role>;
**Question 598. What is a primary benefit of using database roles for managing access?**  
A) Database owners can manage access independently within their own databases  
B) Privileges can cross between databases  
C) Only ACCOUNTADMIN can manage database roles  
D) No need to grant privileges  
**Answer:** A) Database owners can manage access independently within their own databases
**Question 599. When a database role is granted to an account role, what privilege is implicitly granted?**  
A) OWNERSHIP on the database  
B) USAGE on the database  
C) SELECT on all tables  
D) MANAGE GRANTS  
**Answer:** B) USAGE on the database
**Question 600. What is the purpose of a managed access schema?**  
A) Allow object owners to grant privileges  
B) Centralize privilege management to schema owner or MANAGE GRANTS role  
C) Enable UBAC only  
D) Remove USAGE privileges  
**Answer:** B) Centralize privilege management to schema owner or MANAGE GRANTS role
**Question 601. What feature allows automatic granting of privileges on new objects?**  
A) Managed access schemas  
B) Future grants  
C) Cloning  
D) Object access roles  
**Answer:** B) Future grants
**Question 602. When a database or schema is cloned, what happens to privileges on contained objects?**  
A) They are lost  
B) They are retained in the cloned container  
C) They transfer to the source  
D) Only SELECT is retained  
**Answer:** B) They are retained in the cloned container
**Question 603. Which access model is generally recommended for production and enterprise-level governance in Snowflake?**  
A) User-based access control (UBAC)  
B) Role-based access control (RBAC)  
C) Open access  
D) Public access  
**Answer:** B) Role-based access control (RBAC)
**Question 605. How can grant proliferation be avoided when using UBAC?**  
A) Use managed access schemas  
B) Use only RBAC  
C) Assign all privileges to PUBLIC  
D) Use future grants  
**Answer:** A) Use managed access schemas
**Question 607. To run SHOW GRANTS on a table, which privileges are required?**  
A) USAGE on database, USAGE on schema, any privilege on table  
B) USAGE on database only  
C) OWNERSHIP on table  
D) MANAGE GRANTS only  
**Answer:** A) USAGE on database, USAGE on schema, any privilege on table

 
### Snowflake SnowPro Architect Exam Questions (Privileges and Grants)
Questions numbered from 609, based on the privileges/grants reference text:

---

**Question 609. Which privilege allows a user to set a column-level security masking policy on a table or view column?**  
A) APPLY JOIN POLICY  
B) APPLY MASKING POLICY  
C) APPLY ROW ACCESS POLICY  
D) APPLY PRIVACY POLICY  
**Answer:** B) APPLY MASKING POLICY

---

**Question 610. What privilege is required to add or drop a row access policy on a table or view?**  
A) APPLY AUTHENTICATION POLICY  
B) APPLY ROW ACCESS POLICY  
C) APPLY TAG  
D) CREATE DATABASE  
**Answer:** B) APPLY ROW ACCESS POLICY
**Question 613. Which privilege enables the creation of a new virtual warehouse in Snowflake?**  
A) CREATE DATABASE  
B) CREATE WAREHOUSE  
C) MANAGE WAREHOUSES  
D) CREATE ROLE  
**Answer:** B) CREATE WAREHOUSE
**Question 614. Which privilege grants the ability to view, comment on, and manage all support cases for the current account in Snowsight?**  
A) MANAGE ACCOUNT SUPPORT CASES  
B) MANAGE USER SUPPORT CASES  
C) MANAGE ORGANIZATION SUPPORT CASES  
D) MANAGE ACCOUNTS  
**Answer:** A) MANAGE ACCOUNT SUPPORT CASES
**Question 615. What privilege must a role have to grant or revoke privileges on objects it does not own?**  
A) MANAGE WAREHOUSES  
B) MANAGE GRANTS  
C) MONITOR EXECUTION  
D) MANAGE ACCOUNTS  
**Answer:** B) MANAGE GRANTS
**Question 616. Which privilege is required to perform operations that require MODIFY, MONITOR, and OPERATE on all warehouses in an account?**  
A) MANAGE WAREHOUSES  
B) ALL [PRIVILEGES]  
C) OWNERSHIP  
D) OPERATE  
**
**Question 617. What privilege grants the ability to run tasks owned by a role?**  
A) EXECUTE MANAGED TASK  
B) EXECUTE TASK  
C) EXECUTE DATA METRIC FUNCTION  
D) OPERATE  
**Answer:** B) EXECUTE TASK
**Question 619. The MODIFY privilege on a warehouse allows a role to do what?**  
A) Change the size or properties of a warehouse  
B) Grant privileges to other users  
C) Monitor SQL statements  
D) Create new warehouses  
**Answer:** A) Change the size or properties of a warehouse
**Question 620. Which privilege enables a user to create new user accounts in Snowflake?**  
A) CREATE ROLE  
B) CREATE USER  
C) CREATE ACCOUNT  
D) MANAGE USER SUPPORT CASES  
**Answer:** B) CREATE USER
**Question 621. What privilege is required to execute alerts owned by a role in Snowflake?**  
A) EXECUTE ALERT  
B) MONITOR EXECUTION  
C) EXECUTE DATA METRIC FUNCTION  
D) MANAGE GRANTS  
**Answer:** A) EXECUTE ALERT
**Question 622. Which privilege grants the ability to view the login history for a user in Snowflake?**  
A) MONITOR  
B) OWNERSHIP  
C) IMPERSONATE  
D) MODIFY  
**Answer:** A) MONITOR
**Question 624. Which privilege is required for a data provider to create a new share in Snowflake?**  
A) CREATE SHARE  
B) CREATE EXTERNAL ACCESS INTEGRATION  
C) MANAGE SHARE TARGET  
D) IMPORT SHARE  
**Answer:** A) CREATE SHARE
**Question 625. What privilege must a user have to view, comment on, and manage all support cases for the current user in Snowsight?**  
A) MANAGE ACCOUNT SUPPORT CASES  
B) MANAGE USER SUPPORT CASES  
C) MANAGE GRANTS  
D) MANAGE ACCOUNTS  
**Answer:** B) MANAGE USER SUPPORT CASES
**Question 626. Which privilege enables executing a SELECT statement on a table?**  
A) USAGE  
B) SELECT  
C) OPERATE  
D) MONITOR  
**Answer:** B) SELECT
**Question 627. What privilege allows a user to perform operations that require writing to a stage?**  
A) READ  
B) WRITE  
C) USAGE  
D) OPERATE  
**Answer:** B) WRITE
**Question 629. What privilege enables referencing an external volume in commands and viewing its details?**  
A) USAGE  
B) READ  
C) OPERATE  
D) MONITOR  
**Answer:** A) USAGE
**Question 630. To create a new schema in a database, which privilege is required?**  
A) CREATE SCHEMA  
B) CREATE DATABASE  
C) MODIFY  
D) OWNERSHIP  
**Answer:** A) CREATE SCHEMA
**Question 633. What privilege allows a user to read session context in Snowflake?**  
A) MONITOR EXECUTION  
B) READ SESSION  
C) USAGE  
D) SELECT  
**Answer:** B) READ SESSION
**Question 634. Which privilege is required to query a materialized view?**  
A) MONITOR  
B) SELECT  
C) USAGE  
D) OPERATE  
Ans : SELECT 
**Question 635. What privilege allows a user to add or drop a tag on a Snowflake object?**  
A) APPLY TAG  
B) APPLY PRIVACY POLICY  
C) APPLY SNAPSHOT POLICY  
D) MODIFY  
**Answer:** A) APPLY TAG
**Question 636. Which privilege is required to create a new mask policy in a schema?**  
A) CREATE MASKING POLICY  
B) CREATE PRIVACY POLICY  
C) APPLY MASKING POLICY  
D) MODIFY  
**Answer:** A) CREATE MASKING POLICY
**Question 638. What privilege grants the ability to suspend or resume a compute pool?**  
A) OPERATE  
B) MODIFY  
C) USAGE  
D) OWNERSHIP  
**Answer:** A) OPERATE

### 640 onwards. Snowflake SnowPro Architect Exam Questions (Privileges & Real-life Scenarios)
**Question 640. Which privilege allows a user to set a column-level security masking policy?**  
A) APPLY MASKING POLICY  
B) APPLY PRIVACY POLICY  
C) APPLY FEATURE POLICY  
D) APPLY SESSION POLICY  
**Answer:** A) APPLY MASKING POLICY
**Question 641. Which privilege enables an admin to create a new virtual warehouse?**  
A) CREATE WAREHOUSE  
B) CREATE DATABASE  
C) CREATE SHARE  
D) CREATE ROLE  
**Answer:** A) CREATE WAREHOUSE
**Question 642. Scenario: Your company wants to ensure only a select group can manage the lifecycle of accounts (create/delete). Which privilege should you grant?**  
A) MANAGE ACCOUNTS  
B) MANAGE GRANTS  
C) CREATE ACCOUNT  
D) MANAGE WAREHOUSES  
**Answer:** A) MANAGE ACCOUNTS
**Question 643. What privilege must be granted to allow a user to create a managed account (reader account) in Snowflake?**  
A) CREATE ACCOUNT  
B) CREATE USER  
C) CREATE ROLE  
D) CREATE SHARE  
**Answer:** A) CREATE ACCOUNT
**Question 644. Which global privilege allows a user to activate a network policy for the account?**  
A) APPLY SESSION POLICY  
B) ATTACH POLICY  
C) APPLY PASSWORD POLICY  
D) MONITOR SECURITY  
**Answer:** B) ATTACH POLICY
**Question 645. Scenario: The finance team wants to restrict access to payroll data using tags. Which privilege is needed?**  
A) APPLY TAG  
B) APPLY MASKING POLICY  
C) APPLY FEATURE POLICY  
D) APPLY SESSION POLICY  
**Answer:** A) APPLY TAG
**Question 646. What privilege is required to grant or revoke privileges on objects not owned by the role?**  
A) MANAGE GRANTS  
B) MANAGE ACCOUNTS  
C) OWNERSHIP  
D) ALL [PRIVILEGES]  
**Answer:** A) MANAGE GRANTS
**Question 647. Scenario: A data provider needs to create a share for secure data sharing. What privilege is required?**  
A) CREATE SHARE  
B) CREATE DATABASE  
C) IMPORT SHARE  
D) MANAGE SHARE TARGET  
**Answer:** A) CREATE SHARE
*Question 649. Scenario: Your organization wants to run a serverless alert. Which two privileges must the owner role have?**  
A) EXECUTE ALERT and EXECUTE MANAGED ALERT  
B) EXECUTE TASK and OWNERSHIP  
C) MANAGE GRANTS and MANAGE ACCOUNTS  
D) CREATE ALERT and IMPORT SHARE  
**Answer:** A) EXECUTE ALERT and EXECUTE MANAGED ALERT
**Question 666. Which privilege allows a user to run tasks owned by the role?**  
A) EXECUTE TASK  
B) EXECUTE MANAGED TASK  
C) MONITOR EXECUTION  
D) CREATE TASK  
**Answer:** A) EXECUTE TASK
**Question 667. Scenario: A business analyst wants to view the structure of an event table but not the data. Which privilege is required?**  
A) REFERENCES  
B) SELECT  
C) MONITOR  
D) USAGE  
**Answer:** A) REFERENCES
**Question 668. Which privilege allows a user to execute a SELECT statement on a hybrid table?**  A) SELECT  
B) USAGE  
C) OPERATE  
D) MONITOR  
**Answer:** A) SELECT
 **Question 669**  
Which context function in a masking policy returns the name of the role in use for the current session?  
A) INVOKER_ROLE  
B) CURRENT_DATABASE  
C) CURRENT_ROLE  
D) IS_GRANTED_TO_INVOKER_ROLE  
**Answer: C**
 **Question 671**  What does the IS_ROLE_IN_SESSION('ANALYST') function return if the current session role does NOT inherit privileges from the ANALYST role?  
A) TRUE  
B) FALSE  
C) NULL  
D) ‘********’  
**Answer: B**
**Question 673**  
Which context function is most appropriate for differentiating between the session role and the owner role of a view when writing a masking policy?  
A) CURRENT_ROLE  
B) CURRENT_SCHEMA  
C) INVOKER_ROLE  
D) IS_ROLE_IN_SESSION  
**Answer: C**
**Question 674**  
A company applies a masking policy to a sensitive column in a table. What happens if a view is created on that table?  
A) No masking policy applies on the view  
B) The view inherits the masking policy  
C) Only view owner sees unmasked data  
D) Masking policy must be reapplied  
**Answer: B**
**Question 679**  
What does INVOKER_SHARE() return when called in a masking policy applied to a shared table?  
A) The role in use for the session  
B) The share that accessed the table or view  
C) The owner of the table  
D) NULL  

for these questions, derive a tutorial for the Snowflake Advanced architect certification that give me a layman's explanation of the concepts involved here in these questions:- 
**Answer: B**
**Question 685**  
A company needs to audit which roles have inherited privileges from a custom role. Which Snowflake SQL command helps verify this?  
A) SHOW GRANTS OF ROLE  
B) SHOW MASKING POLICIES  
C) DESCRIBE DATABASE  
D) SHOW WAREHOUSES  
**Answer: A**
**Question 690**  
A business architect needs to ensure compliance for sensitive columns in a multi-tenant environment, where masking behavior must change based on both session role and owner role. What Snowflake feature enables this?  
A) Row Access Policies  
B) Multi-table masking policies  
C) Combined use of CURRENT_ROLE and INVOKER_ROLE in masking policies  
D) Schema-level masking  
**Answer: C**
**Question 691**  
Which masking policy function should be avoided if collision-free masking is required for JOIN operations across multiple tables?  
A) HASH  
B) External Tokenization  
C) CASE  
D) REGEXP_REPLACE  
**Answer: A**
**Question 693**  
If the masking policy is set as:  
CASE  
  WHEN IS_ROLE_IN_SESSION('ANALYST') THEN val  
  ELSE 'MASKED'  
END;  
Which role(s) will see unmasked values?  
A) Only ANALYST  
B) ANALYST and roles inheriting from ANALYST  
C) Only the owner  
D) No one  
**Answer: B**
### 695 ONWARDS Snowflake SnowPro Architect Exam (Questions 694–708): ALTER USER, Privileges, and Real-World Scenarios
**695. An architect needs to force a user to update their password at next login. Which option should be included in the ALTER USER command?**  
A) SET DEFAULT_ROLE = 'USER'  
B) SET MUST_CHANGE_PASSWORD = TRUE  
C) SET TYPE = PERSON  
D) SET ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = TRUE  
**696. When a user’s TYPE is set to SERVICE, which ALTER USER command is NOT allowed?**  
A) SET DEFAULT_NAMESPACE = 'PUBLIC'  
B) SET PASSWORD = 'newpassword'  
C) SET DISABLE_MFA = TRUE  
D) SET DEFAULT_WAREHOUSE = 'WH1'  
**Answer: C**
**697. A business requires that queries containing syntax errors do NOT expose the failed SQL text in query history. Which user parameter should be set?**  
A) ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = FALSE  
B) ENABLE_UNREDACTED_SECURE_OBJECT_ERROR = TRUE  
C) STRICT_JSON_OUTPUT = TRUE  
D) STATEMENT_TIMEOUT_IN_SECONDS = 90  
**Answer: A**
**698. What is the effect of running `ALTER USER john ABORT ALL QUERIES;`?**  
A) John is locked out and cannot log in  
B) John’s running and scheduled queries are aborted, but he can submit new queries  
C) John’s password is reset  
D) John’s MFA is disabled  
**Answer: B**
**699. Which ALTER USER action is used to revoke a user’s OAuth delegated authorization for a specific integration and role?**  
A) REMOVE DELEGATED AUTHORIZATION OF ROLE <role_name> FROM SECURITY INTEGRATION <integration_name>  
B) SET DISABLED = TRUE  
C) RESET PASSWORD  
D) UNSET TAG <tag_name>  
**Answer: A**
**700. A new security policy needs to be enforced for password complexity. Which ALTER USER subcommand should the architect use?**  
A) SET PASSWORD POLICY <policy_name>  
B) SET AUTHENTICATION POLICY <policy_name>  
C) SET SESSION POLICY <policy_name>  
D) SET TAG <policy_name>  
**Answer: A**
**702. A user needs to set their own default warehouse. Which ALTER USER command is valid for this?**  
A) ALTER USER SET DEFAULT_WAREHOUSE = 'WH2';  
B) ALTER USER <username> SET DEFAULT_WAREHOUSE = 'WH2';  
C) ALTER USER SET TYPE = SERVICE;  
D) ALTER USER SET PASSWORD = 'newpass';  
**Answer: A**
**703. What is the effect of unsetting MULTIPLE properties in one ALTER USER statement?**  
A) Only one property can be unset at a time  
B) Properties are reset to their defaults  
C) Properties are deleted  
D) Properties are set to NULL in the database  
**Answer: B**
**707. After setting DISABLED = TRUE for a user, what else happens besides preventing login?**  
A) All running and scheduled queries/statements for the user are aborted  
B) The user’s password is deleted  
C) The user’s default role is changed  
D) All MFA methods are removed  
**Answer: A**
### Snowflake SnowPro Architect Certification — AWS PrivateLink Focus  
**Starting at Question 6849**
#### Question 6849  
Which of the following is *not* a direct benefit of using AWS PrivateLink with Snowflake?  
A) Eliminating the need for public internet traffic  
B) Allowing direct, secure connectivity between AWS VPCs and Snowflake  
C) Enabling cross-region access between AWS VPCs and Snowflake  
D) Supporting integration with AWS Direct Connect  
**Answer:** C
#### Question 6850  
When authorizing AWS PrivateLink for your Snowflake account, which system function should the ACCOUNTADMIN execute?  
A) SYSTEM$REVOKE_PRIVATELINK  
B) SYSTEM$AUTHORIZE_PRIVATELINK  
C) SYSTEM$GET_PRIVATELINK  
D) SYSTEM$GET_PRIVATELINK_CONFIG  
**Answer:** B
#### Question 6851  
A business wants to ensure all Amazon S3 traffic between their Snowflake clients and S3 buckets does not traverse the public internet. Which Snowflake-recommended configuration achieves this?  
A) Configure an Amazon S3 gateway endpoint  
B) Block all outbound traffic  
C) Use only public endpoints  
D) Disable S3 access from Snowflake  
**Answer:** A
#### Question 6858  
In which scenario would you need to regenerate a federated token for PrivateLink operations?  
A) The token has expired after 12 hours  
B) The Snowflake account has been deleted  
C) The AWS region has changed  
D) The VPC CIDR block was updated  
**Answer:** A
#### Question 6859  
A company’s Snowflake clients are unable to connect after PrivateLink enablement. Which of the following is a *likely* cause?  
A) OCSP cache server port 80 is blocked in the security group  
B) Too many users connected simultaneously  
C) The Snowflake account password expired  
D) The region ID is set incorrectly in the account name  
**Answer:** A
#### Question 6860  
Which of the following tasks is *not* Snowflake’s responsibility when configuring AWS PrivateLink?  
A) Providing the privatelink-vpce-id  
B) Configuring VPC security group rules  
C) Supplying system functions for enablement  
D) Documenting required DNS entries  
**Answer:** B
#### Question 6863  
A global enterprise wants to restrict PrivateLink access to only specific users, S3 resources, routes, and subnets. Which AWS construct allows this level of control?  
A) S3 gateway endpoint policy  
B) Snowflake network policy  
C) VPC peering  
D) SYSTEM$GET_PRIVATELINK_CONFIG  
**Answer:** A
Question 684  
### 1. When a Snowflake schema is cloned, which of the following is TRUE regarding the privileges on the cloned schema?  
A) The clone inherits all privileges granted on the source schema itself  
B) The clone does not inherit privileges granted on the source schema, but child object privileges are inherited  
C) The clone only inherits privileges if COPY GRANTS is used  
D) All privileges must be re-granted manually  
**Answer: B**
Question 688  
### 5. When using the COPY GRANTS clause in CREATE TABLE ... CLONE, which privilege is NOT copied to the new table?  
A) SELECT  
B) INSERT  
C) OWNERSHIP  
D) UPDATE  
Question 690  
### 7. A business wants to clone a schema containing tasks and alerts. What is the default state of these objects in the clone?  
A) Both are active  
B) Both are suspended  
C) Tasks are active, alerts are suspended  
D) Both are deleted  
**Answer: B**
Question 691  
### 8. If a pipe object referencing an internal stage exists in a schema being cloned, what happens to the pipe in the clone?  
A) The pipe is cloned and remains active  
B) The pipe is not cloned  
C) The pipe is cloned but paused  
D) The pipe is cloned only if AUTO_INGEST=TRUE  
**Answer: B**
Question 692  
### 9. When cloning a privacy-protected table and its privacy policy exist in the same schema, what happens if the schema is cloned?  
A) Only the table is cloned  
B) Only the privacy policy is cloned  
C) Both table and privacy policy are cloned  
D) Neither is cloned  
**Answer: C**
Question 693  
### 10. In a situation where DDL operations (like renaming tables) happen during a large database cloning operation, what is a possible result?  
A) The clone automatically resolves all conflicts  
B) A SQL compilation error due to object name conflicts  
C) The cloning operation is restarted automatically  
D) Cloning speed improves  
**Answer: B**
Question 695  ### 12. A financial services firm wants to ensure tag-based masking policies are applied to cloned tables. Which scenario will ensure the cloned table is protected by the masking policy in the cloned schema?  
A) Tag and masking policy are in different schemas  
B) Tag, masking policy, and table all exist in the same schema  
C) Tag is in the source schema, masking policy in the clone  
D) Only the table exists in the schema  
**Answer: B**
Question 697  
### 14. In a business scenario, if a cloned table has a foreign key referencing a table in another database, what does the foreign key in the clone reference?  
A) The corresponding cloned table  
B) The original source table  
C) The reference is dropped  
D) A new dummy table  
**Answer: B**
Question 698  
### 15. After cloning a database that contains Java UDFs, under which condition will the Java UDFs be cloned?  
A) If the Java UDFs meet certain conditions specified by Snowflake  
B) Always  
C) Never  
D) Only if COPY GRANTS is used  
**Answer: A**

### Snowflake SnowPro Architect Certification Practice Questions (Federated Authentication & SSO)
---

**Question 701**  
When configuring Okta as an identity provider (IdP) for Snowflake, which attribute must match between Okta and Snowflake for user mapping?  
A) Display Name  
B) Email Address  
C) Department  
D) User Group  
**Answer: B**
**Question 703**  
A business wants to allow only certain users to access Snowflake via federated authentication. Which configuration in AD FS will enforce this?  
A) Assign users to a specific Active Directory group and permit only that group in the relying party trust  
B) Disable SAML encryption  
C) Use the default claim rules  
D) Assign users to a local Snowflake role  
**Answer: A**
### Question 716
Which of the following best describes the purpose of a network policy in Snowflake?
A) To define data retention policies  
B) To control inbound access to the Snowflake service and internal stage  
C) To schedule data loads  
D) To manage user roles  
E) To configure virtual warehouses  
**Answer:** B
### Question 717What is true about the allowed list in a Snowflake network policy?
A) Only identifiers in the allowed list have access; all others are blocked  
B) It overrides the blocked list in all cases  
C) It allows access to all users by default  
D) It cannot contain network rules  
E) It only applies to public IPs  
**Answer:** A
## Question 718
A company wants to allow access to its Snowflake account from all IP addresses in 192.168.1.0/24 except for 192.168.1.99. Which approach should be used?
A) Add all IPs except 192.168.1.99 to ALLOWED_IP_LIST  
B) Use two network rules: one allowing the range, one blocking the exception  
C) Use a single network rule with the exception listed  
D) Only use the BLOCKED_IP_LIST  
E) This setup is not possible in Snowflake  
**Answer:** B
### Question 720
If a network policy has the same IP address in both allowed and blocked lists, which takes precedence?
A) Allowed list  
B) Blocked list  
C) Neither; access is granted  
D) User-level policy  
E) Account-level policy  
**Answer:** B
### Question 721
What is the recommended best practice for creating network policies and network rules in Snowflake?
A) Create large, monolithic rules for simplicity  
B) Use small, targeted rules with descriptive comments  
C) Only use allowed lists  
D) Only use blocked lists  
E) Avoid using comments for security  
**Answer:** B
### Question 724
Which of these is NOT a valid type of identifier for a network rule in Snowflake?
A) IPv4 address  
B) AWS VPCE ID  
C) Azure LinkID  
D) Host Port  
E) All are valid  
**Answer:** D
### Question 725
A business wants to restrict access to its internal stage on AWS using network rules. What must the account administrator do first?
A) Set ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES to true  
B) Create a virtual warehouse  
C) Enable MFA  
D) Grant usage on a table  
E) Create a masking policy  
**Answer:** A
### Question 727
A multinational company wants to allow only North America and Europe users to connect to Snowflake. What is the best practice?
A) Create one rule with all IPs  
B) Create separate network rules by region and add to policy  
C) Add all users to the allowed list  
D) Use a single large rule for both regions  
E) Only block other continents  
**Answer:** B
### Question 729
Which privilege is required on the schema to create a network rule?
A) OWNERSHIP  
B) CREATE NETWORK RULE  
C) CREATE POLICY  
D) USAGE  
E) CREATE SCHEMA  
**Answer:** B
717. In a real-life business scenario, if you want to create a copy of a production database for testing new features without impacting existing data, which feature would you use?
A) Zero-copy cloning  
B) Materialized view  
C) Data masking  
D) Data sharing  
**Answer:** A) Zero-copy cloning
718. What privilege is required to clone a database in Snowflake?
A) SELECT  
B) USAGE  
C) OWNERSHIP  
D) MONITOR  
**Answer:** B) USAGE
719. When cloning a database that contains hybrid tables, which parameter should an architect use to bypass them if required?
A) IGNORE EXTERNAL TABLES  
B) IGNORE HYBRID TABLES  
C) SKIP HYBRID TABLES  
D) EXCLUDE HYBRID TABLES  
**Answer:** B) IGNORE HYBRID TABLES
720. Which of the following statements about cloned tables is TRUE?
A) A clone includes the load history of the source table  
B) A clone does not reflect changes made to the source after cloning  
C) A clone cannot be written to  
D) A clone is read-only by default  
**Answer:** B) A clone does not reflect changes made to the source after cloning
721. In which scenario would you use the AT | BEFORE clause with the CLONE command?
A) To schedule a clone for a future date  
B) To migrate data to a different region  
C) To create a clone as of a specific point in the past  
D) To switch storage layers  
**Answer:** C) To create a clone as of a specific point in the past
724. In a business case, a financial services company needs to analyze transactions as they were at the end of each quarter. Which cloning feature should be used?
A) AT (TIMESTAMP => ...)  
B) IGNORE HYBRID TABLES  
C) INCLUDE INTERNAL STAGES  
D) COPY GRANTS  
**Answer:** A) AT (TIMESTAMP => ...)
729. A data engineering team is required to provide a clone of a schema, including all named internal stages. Which parameter should they use in the CLONE statement?
A) INCLUDE INTERNAL STAGES  
B) COPY GRANTS  
C) IGNORE HYBRID TABLES  
D) AT (TIMESTAMP => ...)  
**Answer:** A) INCLUDE INTERNAL STAGES
### Snowflake Snowpro Architect Storage Integration Questions  
#### Starting from Question 731
**Question 732**  
A company wants to allow all buckets to be accessed through a storage integration except for two specific buckets. Which configuration achieves this?  
A) Set STORAGE_ALLOWED_LOCATIONS to ('*') and list blocked buckets in STORAGE_BLOCKED_LOCATIONS  
B) Set STORAGE_BLOCKED_LOCATIONS to ('*') and list allowed buckets in STORAGE_ALLOWED_LOCATIONS  
C) List all allowed buckets in STORAGE_ALLOWED_LOCATIONS only  
D) Only use the ENABLED parameter  
E) Use STORAGE_AWS_OBJECT_ACL  
**Answer:** A) Set STORAGE_ALLOWED_LOCATIONS to ('*') and list blocked buckets in STORAGE_BLOCKED_LOCATIONS  
**Question 734**  
Which parameter is required when creating a storage integration for Microsoft Azure?  
A) STORAGE_AWS_ROLE_ARN  
B) AZURE_TENANT_ID  
C) STORAGE_AWS_EXTERNAL_ID  
D) STORAGE_AWS_OBJECT_ACL  
E) STORAGE_BLOCKED_LOCATIONS  
**Answer:** B) AZURE_TENANT_ID  
**Question 736**  
In a real-world scenario, your client’s Snowflake account is in the EU, but their data is in an AWS S3 bucket in the US. What will incur an extra charge?  
A) Loading data from S3 to Snowflake  
B) Unloading data from Snowflake to S3 in a different region  
C) Data transfer within the same region  
D) Using Google Cloud Storage  
E) Using Azure Blob Storage  
**Answer:** B) Unloading data from Snowflake to S3 in a different region  
**Question 737**  
Which of the following is NOT a valid storage provider for Snowflake storage integrations?  
A) S3  
B) GCS  
C) AZURE  
D) FTP  
E) S3CHINA  
**Answer:** D) FTP  
**Question 740**  
A stage is linked to a storage integration, but after using CREATE OR REPLACE STORAGE INTEGRATION, the stage cannot access the storage. Why?  
A) The integration name was changed  
B) The association is lost due to a new hidden ID  
C) The stage must be recreated  
D) The integration is disabled  
E) The bucket is not allowed  
**Answer:** B) The association is lost due to a new hidden ID  
**Question 743**  
What happens if you enclose the entire STORAGE_BLOCKED_LOCATIONS value in quotes when creating a storage integration?  
A) The value is accepted  
B) The value is ignored and parameter setting is ineffective  
C) An error is thrown  
D) Only the first bucket is blocked  
E) All buckets are blocked  
**Answer:** B) The value is ignored and parameter setting is ineffective  
**Question 744**  
Which of these is a business scenario best addressed by using STORAGE_ALLOWED_LOCATIONS with specific paths?  
A) Allowing all buckets for all users  
B) Restricting access to a subfolder within a bucket for a project team  
C) Blocking access to all storage  
D) Granting ACCOUNTADMIN role to project team  
E) Using only Azure Blob Storage  
**Answer:** B) Restricting access to a subfolder within a bucket for a project team  
**Question 745**  
If a government agency wants to use Snowflake storage integrations with AWS S3 in a government region, what must be true?  
A) The Snowflake account must be hosted in the same government region  
B) The integration must use Google Cloud Storage  
C) Use the S3CHINA provider  
D) Any Snowflake account can be used  
E) Use only Azure Blob Storage  
**Answer:** A) The Snowflake account must be hosted in the same government region  
### Snowflake SnowPro Architect Certification – External Access & Real-World Scenarios
[
#### Question 746
Which object is required when setting up external network access for a Snowflake UDF to access an external REST API?
A) Virtual Warehouse  
B) Secret  
C) Database  
D) Storage Integration  
E) File Format  
**Answer:** B) Secret

#### Question 747What privilege must be granted to a role so that a developer can read credentials from a secret?
A) OWNERSHIP  
B) USAGE  
C) READ  
D) EXECUTE  
E) MODIFY  
**Answer:** C) READ

#### Question 748
A business wants to allow users to install Python packages from PyPi in Snowpark Container Services. What is the correct first step?
A) Grant USAGE on the warehouse  
B) Create an external access integration  
C) Create a security integration  
D) Register a new database  
E) Create a user-defined function  
**Answer:** B) Create an external access integration

#### Question 749
Which of the following is TRUE about OAuth integration for accessing Google APIs from Snowflake?
A) Only a network rule is required  
B) A security integration must specify client credentials and endpoints  
C) No secret is needed  
D) A database must be dedicated  
E) Only user roles with SYSADMIN can use OAuth  
**Answer:** B) A security integration must specify client credentials and endpoints

#### Question 750
If a developer needs to access AWS S3 with IAM credentials from a Snowflake UDF, which object is used to store AWS temporary credentials?
A) External Function  
B) Security Integration  
C) Network Policy  
D) Secret  
E) Storage Account  
**Answer:** D) Secret
#### Question 753
To enable a business user in the 'user' role to call a UDF that accesses an external network, which privilege must be granted?
A) USAGE on function  
B) USAGE on integration  
C) READ on secret  
D) MODIFY on database  
E) EXECUTE on warehouse  
**Answer:** A) USAGE on function
#### Question 755
In a business scenario, a team wants to automate translation of product descriptions in Snowflake using Google Translate API. What must be created in Snowflake to securely handle OAuth tokens?
A) Resource Monitor  
B) File Format  
C) Secret  
D) Task  
E) Stream  
**Answer:** C) Secret
#### Question 756
When connecting to an external service that requires basic authentication, which Snowflake object holds the username and password?
A) External Access Integration  
B) Secret  
C) Security Integration  
D) UDF Handler  
E) Role  
**Answer:** B) Secret
#### Question 757Which privilege must a developer have to create a UDF that uses an external access integration?
A) WRITE on warehouse  
B) MODIFY on integration  
C) USAGE on schema and integration  
D) EXECUTE on database  
E) MONITOR on account  
**Answer:** C) USAGE on schema and integration
#### Question 760
A company has strict security policies. What is the best practice for allowing UDFs to access external APIs in production?
A) Use PUBLIC role for all secrets  
B) Grant minimal required privileges on secrets, integrations, and schemas  
C) Allow direct password storage in UDF code  
D) Enable USAGE on all databases  
E) Use admin credentials for all integrations  
**Answer:** B) Grant minimal required privileges on secrets, integrations, and schemas
### Snowflake SnowPro Architect Certification Exam Questions: External Functions
**Question 747**  
In a typical business scenario, your company wants to use an external machine learning model hosted on AWS Lambda from Snowflake. What is the minimum infrastructure you need to set up to enable this?  
A) Only AWS Lambda function  
B) API integration, Proxy service, and AWS Lambda function  
C) Snowflake virtual warehouse only  
D) Azure Function and API Management  
E) Snowflake Task  
**Answer: B) API integration, Proxy service, and AWS Lambda function**
**Question 748**  Which of the following statements about external functions in Snowflake is correct?  
A) They execute their code inside Snowflake’s compute  
B) They are used only for batch processing jobs  
C) They call code executed outside Snowflake  
D) They can be used in Secure Data Sharing  
E) They must be written in JavaScript  
**Answer: C) They call code executed outside Snowflake**
**Question 749**  
A business wants to call an external API that returns city names for zip codes. Which Snowflake object is used to store the security credentials and integration information with the proxy service?  
A) Remote Service  
B) External Table  
C) API Integration  
D) Snowflake Network Policy  
E) Database Role  
**Answer: C) API Integration**
**Question 752**  
Which of the following is NOT a supported use case for Snowflake external functions?  
A) Calling a remote machine learning model  
B) Integrating with third-party APIs  
C) Using in a masking policy shared via Secure Data Sharing  
D) Data enrichment from external services  
E) Invoking AWS Lambda functions  
**Answer: C) Using in a masking policy shared via Secure Data Sharing**
**Question 755**  When using an external function, which format must the remote service accept as input and return as output?  
A) CSV  
B) XML  
C) JSON  
D) Parquet  
E) Avro  
**Answer: C) JSON**
**Question 756**  
A retail company needs to create an external function that can be called from both Snowflake and other applications. Which of these is an advantage of using an external function?  
A) Code is always written in SQL  
B) Remote services can use any HTTP server stack  
C) External functions have no security considerations  
D) They automatically scale without configuration  
E) They do not support overloading  
**Answer: B) Remote services can use any HTTP server stack**
**Question 757**  
Which cloud platforms can host the proxy service for a Snowflake external function?  
A) Only AWS  
B) Only Microsoft Azure  
C) AWS, Azure, and GCP  
D) Only GCP  
E) Only Snowflake  
**Answer: C) AWS, Azure, and GCP**
**Question 758**  Which of the following is true regarding the use of external functions and Snowflake Secure Data Sharing?  
A) External functions are fully supported in Secure Data Sharing  
B) Only internal UDFs can be used in Secure Data Sharing  
C) External functions cannot be shared with data consumers via Secure Data Sharing  
D) Only masking policies can use external functions in shares  
E) External functions require no special configuration for sharing  
**Answer: C) External functions cannot be shared with data consumers via Secure Data Sharing**
**Question 759**  
A business scenario: Your organization wants to call an Azure Function from a Snowflake instance running on GCP. What is required for this to work?  
A) Both Snowflake and remote service must run on the same cloud platform  
B) Proxy service and remote service must be on supported platforms  
C) Snowflake Tasks must be enabled  
D) Data sharing must be configured  
E) Only internal UDFs can be used  
**Answer: B) Proxy service and remote service must be on supported platforms**
**Question 760**  
Which of the following is a valid reason to choose an external function over an internal UDF in Snowflake?  
A) Need to use libraries not accessible from within Snowflake  
B) Desire to minimize latency  
C) To avoid use of cloud services  
D) To use only SQL-based logic  
E) To enable Secure Data Sharing  
**Answer: A) Need to use libraries not accessible from within Snowflake**
748. When analyzing resource usage by cost center in Snowflake, what feature facilitates this?
A) Account-level grants  
B) Object tagging on warehouses  
C) Virtual warehouse scaling  
D) Data masking policies  
**Answer: B**
751. In a real-world scenario, how can an architect ensure sensitive PII columns are automatically protected by masking policies?
A) Set a masking policy on every column individually  
B) Assign masking policies to tags, and assign tags to columns  
C) Use replication to propagate masking  
D) Only use account-level tags  
**Answer: B**
752. If a tag is set at the database level, which of the following will inherit it?A) Only the database  
B) All schemas and their child objects within the database  
C) Only tables in the database  
D) Only roles in the database  
**Answer: B**
753. Which of the following statements about tag inheritance is true?
A) Tags can only be inherited by parent objects  
B) Tags are inherited by child objects in the securable object hierarchy  
C) Tags cannot be inherited  
D) Tags are inherited by objects in different accounts  
**Answer: B**
754. What view or function can you use to determine how a tag was associated with a Snowflake object?
A) INFORMATION_SCHEMA.OBJECT_PRIVILEGES  
B) ACCOUNT_USAGE.TAG_REFERENCES  
C) QUERY_HISTORY  
D) INFORMATION_SCHEMA.USER_REFERENCES  
**Answer: B**
757. An organization wants to analyze project costs across multiple warehouses and projects. Which approach is most effective?
A) Use a single warehouse for all projects  
B) Assign tags to warehouses with project or cost center values  
C) Use separate accounts for each project  
D) Only assign tags to tables  
**Answer: B**
758. Which of the following objects does NOT support tags in Snowflake?
A) External functions  
B) Network policies  
C) Snowflake users  
D) Unmanaged files external to the account  
**Answer: D**
759. If a table has 10 columns, and each column has 5 tags, what is the maximum number of additional tags you can set on columns in that table?
A) 0  
B) 25  
C) 50  
D) 40  
**Answer: D**
746. Which of the following is a benefit of using organizations in Snowflake?
A) Centralized view of all accounts within your business  
B) Access to unlimited storage at no cost  
C) Automatic masking of all sensitive data  
D) Real-time database triggers  
**Answer: A**
747. What type of account allows organization administrators to manage multi-account organizations and access usage data from ORGANIZATION_USAGE schema?
A) Snowflake Open Catalog account  
B) Organization account  
C) Trial account  
D) Regular Snowflake account  
**Answer: B**
748. A business wants to share data securely with consumers in a different cloud region. Which Snowflake feature enables this?
A) Time Travel  
B) Secure Data Sharing across regions  
C) External Functions  
D) Virtual Private Snowflake  
**Answer: B**
749. How can an organization administrator view all accounts under their organization using SQL?
A) SHOW ORGANIZATIONS  
B) SHOW ACCOUNTS  
C) LIST ALL ACCOUNTS  
D) GET ORGANIZATION ACCOUNTS  
**Answer: B**
750. What must be done before requesting Snowflake Support to delete an organization?
A) Change the organization name  
B) Delete all accounts in the organization except the one used for deletion  
C) Enable failover for all accounts  
D) Migrate all data to external storage  
**Answer: B**
751. Which account type is used by service and catalog admins to manage Snowflake Open Catalog?
A) Organization account  
B) Regular Snowflake account  
C) Snowflake Open Catalog account  
D) Trial account  
**Answer: C**
752. If a company wants to monitor usage across all its Snowflake accounts, what should they use?
A) Organization Usage views  
B) Data Cloning  
C) Resource Monitors only  
D) Account Replication Logs  
**Answer: A**
755. Which of the following is NOT a benefit of organizations in Snowflake?
A) Self-service account creation  
B) Data durability via replication and failover  
C) Free unlimited user seats  
D) Seamless cross-region data sharing  
**Answer: C**
757. In a real-world merger, a company wants to rename its Snowflake organization to reflect new branding. What is the correct procedure?
A) Change the name using ALTER ORGANIZATION  
B) Contact Snowflake Support to request a name change  
C) Edit it in the web UI only  
D) Rename the organization via SQL  
**Answer: B**
758. When deleting an organization, why is only the last account deleted by Snowflake Support?
A) To ensure auditing can be completed  
B) Because only support can finalize organization deletion  
C) So that all remaining accounts are transferred to a different organization  
D) To preserve billing data  
**Answer: B**

759. Which account type is typically used for Snowflake trials?
A) Organization account  
B) Regular Snowflake account  
C) Open Catalog account  
D) Catalog admin account  
**Answer: B**

760. In a business scenario, why would an architect use account replication and failover in an organization?
A) To provide high data availability and disaster recovery across regions  
B) To automate billing across accounts  
C) To eliminate the need for secure views  
D) To synchronize all passwords  
**Answer: A**
48. Which of the following commands generates an encrypted private key for Snowflake authentication?  
A) openssl genrsa 1024 | openssl pkcs8 -topk8 -out rsa_key.p8 -nocrypt  
B) openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8  
C) openssl genrsa 2048 -out rsa_key.p8  
D) openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub  
**Answer: B) openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8**  
751. Which role is required at minimum to grant the privilege to assign a public key?  
A) ACCOUNTADMIN  
B) SECURITYADMIN  
C) SYSADMIN  
D) USERADMIN  
**Answer: B) SECURITYADMIN**  
752. Which file format should the private key be stored in for Snowflake authentication?  
A) PEM  
B) DER  
C) CER  
D) CRT  
**Answer: A) PEM**  
754. Which of the following is a recommended practice for storing private key passphrases?  
A) Share via email  
B) Store in a secure location  
C) Write on a sticky note  
D) Save as plain text in code  
**Answer: B) Store in a secure location**  
755. What happens if you use an encrypted private key for Snowflake key pair authentication?  
A) You must provide a passphrase during the initial connection  
B) The passphrase is sent to Snowflake  
C) The key cannot be used  
D) Only ODBC driver supports it  
**Answer: A) You must provide a passphrase during the initial connection**  
757. Which is NOT a supported method for generating a public key for Snowflake?  
A) openssl rsa -in <private_key> -pubout -out <public_key>  
B) Using Snowflake UI  
C) openssl genrsa 2048  
D) openssl pkcs8 -topk8  
**Answer: B) Using Snowflake UI**  
758. What is the purpose of assigning a public key to a user in Snowflake?  
A) To enable key pair authentication  
B) To encrypt all network traffic  
C) To backup user data  
D) To assign roles  
**Answer: A) To enable key pair authentication**  
760. Which of the following ensures that key pair authentication is set up securely for a Snowflake user?  
A) Use at least a 2048-bit encrypted private key with a strong passphrase  
B) Store private keys in a shared drive  
C) Use 1024-bit keys for faster login  
D) Use public key as private key  
**Answer: A) Use at least a 2048-bit encrypted private key with a strong passphrase**  
762. When a key is rotated, what is the final step for removing the old public key from a user profile?  
A) ALTER USER ... UNSET RSA_PUBLIC_KEY  
B) DROP USER  
C) REMOVE KEY <key>  
D) ALTER USER ... REMOVE PUBLIC_KEY  
**Answer: A) ALTER USER ... UNSET RSA_PUBLIC_KEY**  
763. In a business scenario, your internal audit requires proof that key pair authentication is in use. Which command would you use?  
A) DESC USER <username>  
B) SHOW WAREHOUSES  
C) GRANT ROLE  
D) USE DATABASE  
**Answer: A) DESC USER <username>**  
764. If a developer needs to use both Python connector and SnowSQL for key pair authentication, which key type is compatible?  
A) Encrypted or unencrypted PEM private key  
B) Only encrypted keys  
C) Only unencrypted keys  
D) Only DER keys  
**Answer: A) Encrypted or unencrypted PEM private key**  
765. What is the most secure way to generate a passphrase for your private key according to PCI DSS?  
A) Follow the PCI DSS standard guidance  
B) Use your pet's name  
C) Reuse your Snowflake password  
D) Use 'password123'  
**Answer: A) Follow the PCI DSS standard guidance**  
766. Which of these actions can cause a failed authentication when using key pair authentication?  
A) Mismatched public and private key  
B) Using correct public and private key  
C) Using the right passphrase  
D) Using an ODBC driver  
**Answer: A) Mismatched public and private key**  
767. In the event of a security breach, what is the BEST immediate action for key pair authentication in Snowflake?  
A) Rotate and replace the compromised key pair  
B) Ignore the breach  
C) Update the user password  
D) Change the account admin  
**Answer: A) Rotate and replace the compromised key pair**  
768. Which privilege allows a custom role to assign a public key to a service user?  
A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS  
B) CREATE ROLE  
C) USAGE  
D) MODIFY USER ROLES  
**Answer: A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS**  
769. Which command will assign a new public key to a user’s secondary key slot?  
A) ALTER USER example_user SET RSA_PUBLIC_KEY_2='<key>'  
B) SET USER example_user PUBLIC_KEY_2='<key>'  
C) ALTER USER example_user ADD PUBLIC_KEY_2='<key>'  
D) ALTER USER example_user SET SECONDARY_KEY='<key>'  
**Answer: A) ALTER USER example_user SET RSA_PUBLIC_KEY_2='<key>'**  
770. Your company policy requires all private keys to be encrypted. What should you do when generating your key pair for Snowflake?  
A) Use openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8  
B) Use openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt  
C) Use openssl req -newkey rsa:2048  
D) Use openssl genrsa 1024  
**Answer: A) Use openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8**  
771. What is a business benefit of supporting multiple active keys for authentication in Snowflake?  
A) Enables uninterrupted key rotation  
B) Allows unlimited users  
C) Increases storage  
D) Disables MFA  
**Answer: A) Enables uninterrupted key rotation**  
772. When associating public keys, which delimiter should be excluded in the SQL statement?  
A) -----BEGIN PUBLIC KEY-----  
B) -----END PUBLIC KEY-----  
C) Both A and B  
D) None  
**Answer: C) Both A and B**  
773. What does Snowflake use to match the private key submitted during authentication?  
A) The user’s active public key  
B) The username  
C) The password  
D) The warehouse name  
**Answer: A) The user’s active public key**  
74. Which of these is a risk if private keys are not handled securely in a business?  
A) Unauthorized access to Snowflake  
B) Faster performance  
C) Cheaper storage  
D) Simpler queries  
**Answer: A) Unauthorized access to Snowflake**  
775. For compliance, your company requires verification that rotated keys are active. Which step is necessary after assigning a new public key?  
A) Update the code to connect using the new private key  
B) Drop the user  
C) Revoke all warehouse privileges  
D) Export user data  
**Answer: A) Update the code to connect using the new private key**  
776. In a real-world scenario, your security team mandates that only designated roles can assign public keys. Which privilege do you grant to the custom role?  
A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS  
B) CREATE DATABASE  
C) MANAGE ACCOUNT  
D) USAGE  
**Answer: A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS**  
777. Which is NOT a benefit of using key pair authentication in Snowflake?  
A) Enhanced security  
B) Compliance with governance standards  
C) Simplified password sharing  
D) Support for key rotation  
**Answer: C) Simplified password sharing**  
778. When verifying a user’s public key fingerprint, which two outputs must match?  
A) Output from DESC USER and openssl dgst command  
B) Output from ALTER USER and GRANT ROLE  
C) Output from SHOW USERS and openssl pkcs8  
D) Output from CREATE USER and ODBC driver  
**Answer: A) Output from DESC USER and openssl dgst command**  

### Snowflake SnowPro Architect Certification: Reader Accounts & Data Sharing  
**Questions 746–780**
**746. What is the maximum number of reader accounts a provider can create by default?**  
A) 5  
B) 10  
C) 15  
D) 20  
E) 25  
**Answer: D) 20**
**750. Which of the following actions is NOT permitted in a reader account?**  
A) Creating materialized views  
B) Running SELECT queries  
C) Uploading new data  
D) Reviewing shared data  
E) Creating resource monitors  
**Answer: C) Uploading new data**
**752. What is the preferred way to unload data from a reader account?**  
A) Using a storage integration  
B) COPY INTO <table>  
C) COPY INTO <location> with connection credentials  
D) Using UNLOAD command  
E) Using CREATE PIPE  
**Answer: C) COPY INTO <location> with connection credentials**
**754. What is the required type specified in the CREATE MANAGED ACCOUNT command to create a reader account?**  
A) ADMIN  
B) CONSUMER  
C) USER  
D) READER  
E) SHARED  
**Answer: D) READER**
**756. Which operation is allowed in a reader account?**  
A) INSERT  
B) UPDATE  
C) DELETE  
D) Creating materialized views  
E) CREATE MASKING POLICY  
**Answer: D) Creating materialized views**

**759. In a multi-region business setup, how can you ensure continued data access for reader accounts during a regional outage?**  
A) Create one reader account in the affected region  
B) Use network policies  
C) Use Client Redirect with reader accounts in different regions  
D) Rely on data retention  
E) Use CREATE MASKING POLICY  
**Answer: C) Use Client Redirect with reader accounts in different regions**
**761. What must you do after creating a reader account before it is ready for use?**  
A) Wait 24 hours  
B) Add it to one or more shares and configure the account  
C) Contact Snowflake Support  
D) Grant ACCOUNTADMIN to a user  
E) Run a backup  
**Answer: B) Add it to one or more shares and configure the account**
**762. What is the retention period for deleted reader accounts before a new one can be created in its place?**  
A) 24 hours  
B) 2 days  
C) 7 days  
D) 14 days  
E) 30 days  
**Answer: C) 7 days**
**763. Which of the following is NOT a restriction for reader accounts?**  
A) Cannot create masking policies  
B) Cannot upload new data  
C) Cannot modify existing data  
D) Cannot run SELECT queries  
E) Cannot execute INSERT  
**Answer: D) Cannot run SELECT queries**
**764. In a real-life scenario, a provider wants to share data with a partner who is not a Snowflake customer. What should they use?**  
A) Direct database share  
B) Create a reader account  
C) Enable SSO  
D) Use external tables  
E) Use data masking  
**Answer: B) Create a reader account**
**765. What is the main difference between a reader account and a standard consumer account?**  
A) Reader accounts can modify data  
B) Reader accounts are owned and managed by the provider  
C) Consumer accounts have more restrictions  
D) Reader accounts incur charges to the consumer  
E) Reader accounts need a licensing agreement  
**Answer: B) Reader accounts are owned and managed by the provider**
**771. Which action is restricted in a reader account but allowed in a standard consumer account?**  
A) SELECT  
B) Creating masking policies  
C) Viewing shares  
D) Creating materialized views  
E) Resource monitoring  
**Answer: B) Creating masking policies**
**772. A provider wants to monitor the number of reader accounts they have. Which command should they use?**  
A) SHOW USERS  
B) SHOW MANAGED ACCOUNTS  
C) SHOW SHARES  
D) SHOW DATABASES  
E) SHOW ROLES  
**Answer: B) SHOW MANAGED ACCOUNTS**
**773. In a real-life case, a third-party partner asks for support on an issue in their reader account. Who should they contact?**  
A) Snowflake Support directly  
B) The provider account administrator  
C) The ACCOUNTADMIN of their own account  
D) The network administrator  
E) The region support team  
**Answer: B) The provider account administrator**
**774. Which of the following operations is NOT allowed in a reader account?**  
A) Creating a materialized view  
B) Modifying data with UPDATE  
C) Running SELECT queries  
D) Viewing shared data  
E) Using COPY INTO <location>  
**Answer: B) Modifying data with UPDATE**
**775. What happens if you attempt to drop a reader account?**  
A) The account is archived  
B) All access is immediately restricted and objects are dropped  
C) Only the admin is removed  
D) The account is retained for 7 days  
E) The account becomes read-only  
**Answer: B) All access is immediately restricted and objects are dropped**
**776. Which of these commands can be used to view information about reader accounts in SQL?**  
A) SHOW MANAGED ACCOUNTS  
B) SHOW READER ACCOUNTS  
C) SHOW ACCOUNTS  
D) SHOW ROLES  
E) SHOW USERS  
**Answer: A) SHOW MANAGED ACCOUNTS**
**777. In a business case, you want to delegate the ability to create reader accounts to your data team. What is required?**  
A) Grant CREATE USER to the data team  
B) Grant CREATE ACCOUNT privilege to the team's role  
C) Grant ACCOUNTADMIN to the data team  
D) Create a new warehouse  
E) No additional privileges needed  
**Answer: B) Grant CREATE ACCOUNT privilege to the team's role**
 **779. What is the relationship between reader accounts and Snowflake support services?**  
A) Reader accounts have their own support channel  
B) Only the provider can contact Snowflake support  
C) Reader accounts have 24/7 support  
D) Support is not available to either account  
E) Reader accounts can open tickets directly  
**Answer: B) Only the provider can contact Snowflake support**
**780. When sharing data with a reader account, which of the following must be done after account creation?**  
A) Assign it to a warehouse  
B) Add it to one or more shares  
C) Enable billing  
D) Grant ACCOUNTADMIN role  
E) Create a new database  
**Answer: B) Add it to one or more shares**
746. Which statement is TRUE regarding maintaining user passwords when federated authentication is enabled in Snowflake?  
A) User passwords must only be managed by Snowflake.  
B) User passwords should be managed solely in the IdP.  
C) User passwords cannot exist in either Snowflake or IdP.  
D) Users must always have a password in Snowflake.  
**Answer: B**
748. Which property should be set to FALSE for users who do not maintain passwords in Snowflake when using federated authentication?  
A) ENABLE_PASSWORD  
B) MUST_CHANGE_PASSWORD  
C) PASSWORD_EXPIRES  
D) FORCE_SSO  
**Answer: B**
749. If you drop or disable a user in Snowflake, what is the result for that user when attempting to log in via Okta?  
A) They can log in as usual.  
B) They receive an error message and cannot connect.  
C) They are redirected to a different account.  
D) They have read-only access.  
**Answer: B**
751. What is the main requirement for browser-based SSO to function correctly with Snowflake clients?  
A) The client must run on a server.  
B) The client must be able to open a web browser on the user’s machine.  
C) The client must use a VPN.  
D) The client must use MFA.  
**Answer: B**
here
776. How can a Snowflake architect ensure that at least one administrator can always access Snowflake?  
A) Use only federated authentication for all users  
B) Keep at least one admin account with a Snowflake-managed password  
C) Use SSO for all admin logins  
D) Only use Okta for admins  
**Answer: B**
777. What is the recommended practice for handling passwords in Snowflake when federated authentication is enabled?  
A) Maintain passwords in both Snowflake and IdP  
B) Maintain passwords only in the IdP  
C) Do not use any passwords  
D) Store passwords in client applications  
**Answer: B**
778. In a business scenario, a consultant needs temporary access to Snowflake. What is the best way to revoke their access when finished?  
A) Remove from Okta only  
B) Disable or drop the user in Snowflake  
C) Change their password  
D) Remove their MFA device  
**Answer: B**
### Snowflake Snowpro Architect Certification – Network Rules & Real-Life Scenarios  
#### 750. Which privilege is required to create a network rule in a schema?  
A) OWNERSHIP on NETWORK RULE  
B) CREATE NETWORK RULE on SCHEMA  
C) USAGE on DATABASE  
D) ADMIN on ACCOUNT  
E) MODIFY on DATABASE  
*Answer:** B) CREATE NETWORK RULE on SCHEMA
#### 753. Which role by default has access to Snowflake-managed network rules such as SNOWFLAKE.EXTERNAL_ACCESS.PYPI_RULE?  
A) SECURITYADMIN  
B) SYSADMIN  
C) ACCOUNTADMIN  
D) ORGADMIN  
E) NETWORKADMIN  
**Answer:** C) ACCOUNTADMIN
#### 755. Which Snowflake object groups network identifiers into logical units for access control?  
A) Network policy  
B) Security integration  
C) Network rule  
D) Stage  
E) External function  
**Answer:** C) Network rule
#### 757. What happens when you attempt to modify the TYPE property of an existing Snowflake network rule?  
A) Modification is applied  
B) Only administrators can do it  
C) Modification is not allowed  
D) The rule is dropped and recreated  
) The schema is locked  
**Answer:** C) Modification is not allowed
#### 758. Which statement is TRUE about replicating network rules in Snowflake?  A) Only account-level objects are replicated  
B) Network rules are not replicated  
C) Network rules are replicated with their parent database  
D) Only OWNERSHIP privileges are replicated  
E) Replication requires SYSADMIN role  
**Answer:** C) Network rules are replicated with their parent database
#### 760. Which command is used to modify the identifiers of an existing network rule in SQL?  
A) ALTER NETWORK RULE  
B) UPDATE NETWORK RULE  
C) MODIFY NETWORK RULE  
D) CHANGE NETWORK RULE  
E) EDIT NETWORK RULE  
*Answer:** A) ALTER NETWORK RULE
0
#### 763. Which feature in Snowflake references network rules to control inbound network traffic?  
A) UDF integration  
B) Network policies  
C) External functions  
D) External access integration  
) Masking policies  
*Answer:** B) Network policies
#### 764. A customer wants to see all network rules in their account using SQL. Which object should they query?  
A) NETWORK_RULES Account Usage view  
B) NETWORK_IDENTIFIERS table  
C) NETWORK_ROLES view  
D) NETWORK_RULES_HISTORY table  
E) NETWORK_LOGS view  
**Answer:** A) NETWORK_RULES Account Usage view
#### 765. What is a required privilege to list all network rules in a schema?  A) USAGE on SCHEMA  
B) OWNERSHIP on NETWORK RULE  
C) READ on DATABASE  
D) MONITOR on ACCOUNT  
E) ALTER on NETWORK RULE  
*Answer:** A) USAGE on SCHEMA
#### 766. What must match for all identifiers in a single network rule’s VALUE_LIST?  
A) Database  
B) Identifier type  
C) Port  
D) Region  
E) Owner  
**Answer:** B) Identifier type
#### 767. Which feature uses network rules to restrict where requests can be sent from a UDF or procedure?  
A) Network policies  
B) External network access  
C) Snowpipe  
D) Data masking  
E) Failover groups  
**Answer:** B) External network access
#### 771. What is required to create a network rule using Snowsight?  
A) CREATE NETWORK RULE privilege on the schema  
B) ACCOUNTADMIN role only  
C) Create network policy privilege  
D) USAGE on warehouse  
E) OWNERSHIP on database  
**Answer:** A) CREATE NETWORK RULE privilege on the schema
#### 772. Which SQL command grants the ability to create network rules in a schema to a role?  
A) GRANT CREATE NETWORK RULE ON SCHEMA ... TO ROLE ...  
B) GRANT NETWORK ADMIN ON SCHEMA ... TO ROLE ...  
C) GRANT CREATE POLICY ON SCHEMA ... TO ROLE ...  
D) GRANT USAGE ON NETWORK RULE TO ROLE ...  
E) GRANT OWNERSHIP ON SCHEMA ... TO ROLE ...  
**Answer:** A) GRANT CREATE NETWORK RULE ON SCHEMA ... TO ROLE ...
#### 775. Which view or table function can be called to identify network rule references in a Snowflake account?  
A) NETWORK_RULE_REFERENCES  
B) NETWORK_RULE_HISTORY  
C) NETWORK_IDENTIFIER_REFERENCES  
D) NETWORK_ROLE_REFERENCES  
E) NETWORK_POLICY_REFERENCES  
**Answer:** A) NETWORK_RULE_REFERENCES
#### 776. Which action is NOT possible when modifying a Snowflake network rule?  
A) Adding a new identifier  
B) Removing an identifier  
C) Changing the comment  
D) Changing the TYPE property  
E) Editing the VALUE_LIST  
**Answer:** D) Changing the TYPE property
752. If an architect wants to prevent Snowflake users from accessing Snowflake without SSO, what should be done in Okta?
A) Map defaultWarehouse for all users  
B) Uncheck “Generate a new random password”  
C) Enable Push Groups  
D) Assign PUBLIC role  
E) Enable Enhanced Group Push  
**Answer: B) Uncheck “Generate a new random password”**
746. Which access control model in Snowflake allows object owners to grant access to others?  
A) Role-based Access Control (RBAC)  
B) Discretionary Access Control (DAC)  
C) User-based Access Control (UBAC)  
D) Mandatory Access Control (MAC)  
E) Attribute-based Access Control (ABAC)  
**Answer: B) Discretionary Access Control (DAC)**
747. In Snowflake, what entity is directly assigned privileges to perform actions on securable objects?  
A) User  
B) Role  
C) Group  
D) Warehouse  
E) Database  
**Answer: B) Role**
748. When using RBAC in Snowflake, how are privileges typically assigned to end users?  
A) Directly to users  
B) Through roles assigned to users  
C) Through grants on databases  
D) Through network policies  
E) By default privileges only  
**Answer: B) Through roles assigned to users**
749. Which privilege must a role possess to transfer ownership of an object in Snowflake?  
A) USAGE  
B) OWNERSHIP  
C) CREATE  
D) MANAGE GRANTS  
E) SELECT  
**Answer: B) OWNERSHIP**
750. In a managed access schema, who has the authority to grant privileges on objects?  
A) Object owner  
B) Schema owner or role with MANAGE GRANTS  
C) Any user  
D) Database owner  
E) PUBLIC role  
**Answer: B) Schema owner or role with MANAGE GRANTS**
751. What is the top-most container in Snowflake's securable object hierarchy?  
A) Account  
B) Database  
C) Organization  
D) Schema  
E) Warehouse  
**Answer: C) Organization**
752. Which system-defined role is recommended for managing account-level operations in Snowflake?  
A) SYSADMIN  
B) ACCOUNTADMIN  
C) PUBLIC  
D) USERADMIN  
E) SECURITYADMIN  
**Answer: B) ACCOUNTADMIN**
754. In a real business scenario, you need to allow a developer temporary access to a specific table. What is the best practice?  
A) Grant the privilege directly to the user  
B) Assign a custom role with access and grant it to the user  
C) Add the user to PUBLIC  
D) Grant OWNERSHIP on the table  
E) Use the SYSADMIN role  
**Answer: B) Assign a custom role with access and grant it to the user**
757. Which of the following is a pseudo-role in Snowflake that is granted to every user and role by default?  
A) PUBLIC  
B) SYSADMIN  
C) USERADMIN  
D) SECURITYADMIN  
E) ACCOUNTADMIN  
**Answer: A) PUBLIC**
761. If a user has multiple roles assigned in Snowflake, how does the system determine which privileges are available during the session?  
A) Based on the privileges of the default role only  
B) Based on the privileges of the active primary role and any active secondary roles  
C) The union of all assigned roles  
D) The intersection of all assigned roles  
E) Only system-defined roles are considered  
**Answer: B) Based on the privileges of the active primary role and any active secondary roles**
762. Which command grants the ability to transfer object ownership in Snowflake?  
A) GRANT OWNERSHIP  
B) GRANT USAGE  
C) GRANT MANAGE GRANTS  
D) GRANT SELECT  
E) REVOKE OWNERSHIP  
**Answer: A) GRANT OWNERSHIP**
**Answer: A) Service role**
764. What happens when you assign privileges to the PUBLIC role in Snowflake?  
A) Only admins can access the object  
B) All users and roles have access to the object  
C) Only custom roles have access  
D) PUBLIC role loses privileges  
E) The object becomes hidden  
**Answer: B) All users and roles have access to the object**
**Answer: A) Increased likelihood of privilege escalation and accidental data exposure**
768. What does the MANAGE GRANTS privilege allow in Snowflake?  
A) Granting and revoking privileges on all objects  
B) Creating objects  
C) Dropping roles  
D) Creating users  
E) Setting default roles  
**Answer: A) Granting and revoking privileges on all objects**
769. If you want to limit a role's privileges to a single database, what type of role should you use?  
A) Database role  
B) Account role  
C) Instance role  
D) Application role  
E) PUBLIC role  
**Answer: A) Database role**
770. Which statement about the PUBLIC role is correct?  
A) It can be dropped  
B) All users automatically receive its privileges  
C) It cannot own objects  
D) It is only available to admins  
E) It must be granted manually  
**Answer: B) All users automatically receive its privileges**
**Answer: A) Centralizes privilege management at the schema level**
772. In a real-life business scenario, why would you use secondary roles in a session?  
A) To combine privileges from multiple roles for complex queries  
B) To restrict access to a single schema  
C) To avoid using system-defined roles  
D) To override database roles  
E) To disable cross-database joins  
**Answer: A) To combine privileges from multiple roles for complex queries**

776. What is the function of the PRIMARY role in a Snowflake session?  
A) Determines which role’s privileges are used to authorize object creation  
B) Activates all privileges for the session  
C) Allows access to every database  
D) Overrides all other roles  
E) Assigns privileges to PUBLIC  
**Answer: A) Determines which role’s privileges are used to authorize object creation**

777. In a business scenario, what is the risk of assigning the OWNERSHIP privilege to a user on a sensitive table?  
A) The user can transfer ownership and grant access to others  
B) The user cannot access the table  
C) The user’s privileges are revoked  
D) The table becomes read-only  
E) The table is dropped automatically  
**Answer: A) The user can transfer ownership and grant access to others**
778. Which role is designed specifically for managing security privileges, including the ability to grant and revoke them?  
A) SECURITYADMIN  
B) SYSADMIN  
C) PUBLIC  
D) USERADMIN  
E) ACCOUNTADMIN  
**Answer: A) SECURITYADMIN**
780. What is the default role used in a session if none is specified and no default is set for a user?  
A) PUBLIC  
B) SYSADMIN  
C) ACCOUNTADMIN  
D) SECURITYADMIN  
E) USERADMIN  
**Answer: A) PUBLIC**
781. What is a role hierarchy in Snowflake?  
A) A structure where roles are granted to other roles, allowing privilege inheritance  
B) A list of users assigned to a role  
C) The privileges granted by SYSADMIN only  
D) A group of databases  
E) The relationship between users and objects  
**Answer: A) A structure where roles are granted to other roles, allowing privilege inheritance**

782. In a real-world scenario, why should system-defined roles' privileges not be mixed with entity-specific privileges?  
A) To prevent accidental privilege escalation and maintain separation of duties  
B) It increases query performance  
C) It is required by Snowflake licensing  
D) It enables PUBLIC role access  
E) It simplifies backup operations  
**Answer: A) To prevent accidental privilege escalation and maintain separation of duties**
783. What is the effect of granting the MANAGE GRANTS privilege to a custom role?  
A) The custom role can grant or revoke privileges globally  
B) The custom role can create databases  
C) The custom role can drop the account  
D) The custom role can create users  
E) The custom role loses all privileges  
**Answer: A) The custom role can grant or revoke privileges globally**
784. Which command is used to assign a role to a user in Snowflake?  
A) GRANT ROLE TO USER  
B) ASSIGN ROLE  
C) USE ROLE  
D) CREATE ROLE  
E) ALTER USER  
**Answer: A) GRANT ROLE TO USER**                                              *
786. What does the GRANT OWNERSHIP command do?  A) Transfers ownership of an object to another role  
B) Grants usage privileges on an object  
C) Creates a new role  
D) Drops the object  
E) Assigns a user to PUBLIC  
**Answer: A) Transfers ownership of an object to another role**
787. What is required for a role to perform SQL actions on objects in a database?  A) The role must be granted the required privileges on those objects  
B) The role must be a system-defined role  
C) The role must have MANAGE GRANTS  
D) The role must be the database owner  
E) The role must be PUBLIC  
**Answer: A) The role must be granted the required privileges on those objects**
788. In a business scenario, why should you avoid granting SELECT to PUBLIC for sensitive tables?  
A) All users would have access to the tables  
B) Only admins can access them  
C) It disables backup  
D) It increases costs  
E) The tables become read-only  
**Answer: A) All users would have access to the tables**
790. Which system-defined role cannot be dropped?  
A) SYSADMIN  
B) ACCOUNTADMIN  
C) USERADMIN  
D) SECURITYADMIN  
E) All of the above  
**Answer: E) All of the above**

791. How can you view the current active role in a Snowflake session?  
A) Use the CURRENT_ROLE function  
B) Run SHOW ROLES  
C) Query INFORMATION_SCHEMA  
D) Use GRANT ROLE  
E) Use DESCRIBE USER  
**Answer: A) Use the CURRENT_ROLE function**
792. Which type of role is used by providers to grant access to objects in a Snowflake Native App?  
A) Application role  
B) Service role  
C) Database role  
D) SYSADMIN  
E) PUBLIC  
**Answer: A) Application role**
793. What is the default outcome if no privileges are granted on an object to any of a user's roles?  
A) Access is denied  
B) Access is granted  
C) PUBLIC role is used  
D) Error is thrown  
E) User is logged out  
**Answer: A) Access is denied**
794. Which role is designed to encapsulate both SYSADMIN and SECURITYADMIN for top-level account management?  
A) ACCOUNTADMIN  
B) USERADMIN  
C) PUBLIC  
D) SERVICEADMIN  
E) APPLICATIONADMIN  
**Answer: A) ACCOUNTADMIN**
796. In a real-life scenario, why would you use organization user groups in Snowflake?  
A) To implement consistent roles across accounts  
B) To limit database access  
C) For backup purposes  
D) To manage warehouses  
E) To increase query speed  
**Answer: A) To implement consistent roles across accounts**
*Answer: A) CREATE SCHEMA**                                                           x
800. In the context of role inheritance, what is the effect of granting role A to role B?  
A) Role B inherits all privileges of role A  
B) Role A inherits all privileges of role B  
C) Users assigned to role A can activate role B  
D) Role A and B privileges are mutually exclusive  
E) Role B loses privileges  
**Answer: A) Role B inherits all privileges of role A**
802. If a business requires separation of duties between user management and object management, which roles should be used?  
A) USERADMIN for user management, SYSADMIN for object management  
B) ACCOUNTADMIN for all tasks  
C) PUBLIC for user management  
D) SECURITYADMIN for object management  
E) USERADMIN for everything  
**Answer: A) USERADMIN for user management, SYSADMIN for object management**
: A) Use GRANT OWNERSHIP to the new role**
### 750. How can you automate the refresh of a replication group in Snowflake?
A) Schedule a cron job on your cloud provider  
B) Use the REPLICATION_SCHEDULE parameter  
C) Enable replication on the consumer account only  
D) Use Snowflake Data Marketplace  
**Answer: B)**

Question: 500    lindian
What built-in Snowflake features make use of the change tracking metadata for a table? (Choose two.)
• A. The MERGE command
• B. The UPSERT command
• C. The CHANGES clause
• D. A STREAM object
• E. Thee CHANGE_DATA_CAPTURE command
• DE

Question 502
Which Snowflake objects can be used to automate scheduled SQL execution? (Choose two.)
• A. Tasks
• B. Streams
• C. Stages
• D. Pipes
• E. Procedures


Question 503
Which of the following are valid data loading mechanisms in Snowflake? (Choose two.)
• A. COPY INTO command
• B. Streams
• C. Snowpipe
• D. Materialized Views
• E. External Functions
AC 
connectivity architectures?**  
A) Snowflake driver/connector to Snowflake account URL  
B) Users’ browsers to Snowflake Apps layer  
C) Snowflake service to customer-owned cloud storage  
D) Snowflake driver/connector to VPN endpoint  
**Question 567. When using CSP PrivateLink with Snowflake, what changes for the driver or connector?
A) It requires custom code  
B) It connects using a privately hosted DNS address  
C) It must use a VPN  
D) It disables OCSP  
**Question 572. Which type of data is always encrypted in transit with Snowflake out-of-the-box connectivity?**  
A) OCSP traffic  
B) Customer data  
C) All traffic  
D) Internal stage traffic only  
**Question 577. Which of these is NOT recommended as a design for Snowflake network security?**  
A) Using one Network Policy per account  
B) Applying CSP private networking to all communications  
C) Using out-of-the-box protections for non-sensitive data  
D) Combining Network Policies and CSP private networking for sensitive workloads  
**Question 578. What is a current incompatibility for SAML-based SSO and Snowflake network endpoints?**  
A) Can only be used with a private endpoint  
B) Can be used with both public and private endpoints simultaneously  
C) Can only be used with either public or private endpoint, not both at the same time  
D) Can only be used with VPN  
**Question 579. Which type of Snowflake user is best restricted using Network Policies and CSP private networking?**  
A) Users in the field consuming reports  
B) Service account users handling unmasked sensitive data  
C) External partners  
D) All users  
**Answer:** B) Service account users handling unmasked sensitive dat                                              a
**Answer:** C) Can only be used with either public or private endpoint, not both at the same time
**Question 604. What is a key benefit of using RBAC over UBAC in Snowflake?**  
A) More granular direct user grants  
B) Scalability and centralized control  
C) Only supports private development  
D) Improves query performance  
**Answer:** B) Scalability and centralized control
**Question 608. Which statement about query results is true in Snowflake?**  
A) ACCOUNTADMIN can see results of all user queries  
B) Only the user who ran the query can view the results  
C) SYSADMIN can view all results  
D) PUBLIC can view all results  
**Answer:** B) Only the user who ran the query can view the results
**Question 628. Which privilege must be held to grant full control over a specific object in Snowflake?**  
A) ALL [PRIVILEGES]  
B) MONITOR  
C) OWNERSHIP  
D) MODIFY  
**Answer:** C) OWNERSHIP
**Question 631. Which privilege allows a user to execute a stored procedure?**  
A) USAGE  
B) MONITOR  
C) OPERATE  
D) SELECT  
Question  698 
A team implements a masking policy using IS_GRANTED_TO_INVOKER_ROLE in a view for SSNs. If neither the payroll nor analyst custom roles are in the view owner hierarchy, what will users querying the view see?  
A) Unmasked data  
B) Partially masked data  
C) Fully masked data  
D) NULL  
Answer: C
**Question 681**  
A business has a regulatory requirement that no sensitive data can be joined with external tables. What Snowflake feature is best suited to prevent collision-based inference attacks in this scenario?  
A) Hash masking  
B) Secure Join feature using Secure Views.
C) Invoker share masking  
D) Row Access Policies  
**Answer: B**                                                                                                     
**Question 684**  
Which statement about using masking policies with both CURRENT_ROLE and INVOKER_ROLE is correct?  
A) Only one can be used per policy  
B) Both can be combined for complex access logic  
C) They cannot be used in views  
D) They are only available in Standard Edition  
**Answer: B**
**701. Which property can only be set by a user with the ACCOUNTADMIN role?**  
A) DEFAULT_ROLE  
B) PREVENT_UNLOAD_TO_INTERNAL_STAGES  
C) MUST_CHANGE_PASSWORD  
D) DEFAULT_WAREHOUSE  
**Answer: B**
How can an Architect enable optimal clustering to enhance performance for different access paths on a given table?
• A. Create multiple clustering keys for a table.
• B. Create multiple materialized views with different cluster keys.
• C. Create super projections that will automatically create clustering.
• D. Create a clustering key that contains all columns used in the access paths
D
### 1. Which of the following Snowflake objects can be shared using Secure Data Sharing?  
A) Databases  
B) Tables  
C) Secure Views  
D) Virtual Warehouses  
E) User-defined Functions (UDFs)  
**Correct answers:** A, B, C, E                                                                                        
 **Question 525. 
When using this security pattern, who approves access to sensitive schemas?**  
A) The database administrator only  
B) The user requesting access  
C) An enterprise identity governance & access management system  
D) Automatic approval by Snowflake  
**Answer:** C) An enterprise identity governance & access management system
**Question 532.
 What is a potential misapplication to avoid according to the guidance?**  
A) Assigning SELECT to all users  
B) Duplicating access levels for sensitive and non-sensitive objects  
C) Using a single schema for all data  
D) Using SCIM for all role assignments  
**Answer:** B) Duplicating access levels for sensitive and non-sensitive objects
 **Question 537. 
In a scenario where a customer wants to avoid passing secrets over the network, which method is most appropriate?**  
A) Built-in username/password  
B) Key pair authentication  
C) SAML SSO  
D) External OAuth  
**Answer:** B) Key pair authentication                                                                                        
**Question 551. In Snowflake, what is true about key pair authentication for service accounts?**  
A) The private key must always travel over the network  
B) Service account need not possess the private key directly  
C) It is less secure than username/password  
D) It is only for human users  
**Answer:** B) Service account need not possess the private key directly
**Question 552. Which authentication method in Snowflake is most similar to how many SaaS applications handle identity?**  
A) External OAuth  
B) SAML SSO  
C) Key pair authentication  
D) Built-in username/password  
**Answer:** B) SAML SSO                                                                                                  
**Question 553. Which is a potential incompatibility for SAML-based SSO in Snowflake?**  
A) Can only be used on either public or private endpoints at one time  
B) Cannot be used with any IdP  
C) Does not support MFA  
D) Only works with built-in users  
**Answer:** A) Can only be used on either public or private endpoints at one time
**Question 555. What is a security benefit of key pair authentication in Snowflake?**  
A) The secret does not travel over the network  
B) The password is stored in Snowflake  
C) The private key must be shared  
D) No rotation is possible  
**Answer:** A) The secret does not travel over the network                                       
**Question 558. What is the outcome of using External OAuth for service accounts in Snowflake?**  
A) Service accounts only access Snowflake data through the OAuth configured service  
B) Service accounts must store passwords locally  
C) All credentials must be entered manually  
D) Service accounts can bypass central identity  
**Question 536. 
Which method is recommended for programmatic (non-human) access to Snowflake when key infrastructure is not available?**  
A) Username/password  
B) Key pair authentication  
C) External OAuth  
D) SAML SSO  
A
**Question 557. What is a limitation of OAuth in Snowflake at this time?**  
A) Snowflake OAuth is not applicable in the programmatic scenario  
B) External OAuth must be used for human users  
C) Key pair authentication is required for all users  
D) It cannot be used with any SaaS application  
**Answer:** A) Snowflake OAuth is not applicable in the programmatic scenario
**Answer:** A) Service accounts only access Snowflake data through the OAuth configured service
**Question 559. Which of these is NOT a basic connection included in all Snowflake network **Question 562. Which network connection is optional in the Snowflake connectivity pattern?**  
A) Connection to the Snowflake account URL  
B) Connection to the Snowflake Internal Stage  
C) Connection to a customer’s cloud storage (External Stage)  
D) Connection to OCSP provider  
**Answer:** C) Connection to a customer’s cloud storage (External Stage)
**Question 564. How many active Network Policies can be applied to a given context (account, integration, or user) at one time?**  
A) One  
B) Unlimited  
C) Five  
D) Three  
**Answer:** A) One
**Question 565. Which statement about Network Policy specificity is TRUE?**  
A) The least specific policy always wins  
B) The most specific policy always wins  
C) User policy is ignored if account policy exists  
D) Integration policy overrides all others  
**Answer:** B) The most specific policy always wins
**Question 566. What is a benefit of integrating Snowflake with CSP private networking options like AWS PrivateLink or Azure Private Link?**  
A) Allows direct access from any public network  
B) Creates a point-to-point, client-side initiated private network channel  
C) Disables OCSP checking  
D) Replaces the need for TLS encryption  
**Answer:** B) Creates a point-to-point, client-side initiated private network channel
**Question 592. When a custom role is created, who can modify or drop its created objects by default? 
A) Any role with SYSADMIN  
B) Only the custom role itself  
C) Any user  
D) ACCOUNTADMIN  
**Answer:** B) Only the custom role itself                                                                                             
**Answer:** B) Customer dat                                                                                                                      
**Question 581. What is required to use a Snowflake Network Policy for a specific integration (e.g., SCIM)?**  
A) The integration must have endpoints for network communications  
B) Only account-level policies are supported  
C) VPN must be enabled  
D) OCSP must be disabled  
**Answer:** A) The integration must have endpoints for network communications
**Question 584. What is the primary responsibility of the ACCOUNTADMIN role in Snowflake?**  
A) Managing database schemas  
B) Configuring parameters at the account level  
C) Creating tables and views  
D) Assigning billing codes  
**Answer:** B) Configuring parameters at the account level
**Question 586. Which role has the privilege to create warehouses, databases, and database objects?**  
A) ACCOUNTADMIN  
B) USERADMIN  
C) SYSADMIN  
D) SECURITYADMIN  
**Answer:** C) SYSADMIN
**Question 606. Which view can be used to monitor privileges granted to roles in your account?**  
A) SNOWFLAKE.ACCOUNT_USAGE.GRANTS_TO_ROLES  
B) SNOWFLAKE.ACCOUNT_USAGE.PRIVILEGES  
C) SNOWFLAKE.ACCOUNT_USAGE.USERS  
D) SNOWFLAKE.ACCOUNT_USAGE.ROLES  
**Answer:** A) SNOWFLAKE.ACCOUNT_USAGE.GRANTS_TO_ROLES
### Snowflake SnowPro Architect Exam Questions (Privileges and Grants)
Questions numbered from 609, based on the privileges/grants reference text:
**Question 611. Which privilege enables a user to create a new managed account (reader account)?**  
A) CREATE DATABASE  
B) CREATE ACCOUNT  
C) CREATE USER  
D) CREATE WAREHOUSE  
**Answer:** B) CREATE ACCOUNT
**Question 612. What global privilege allows a user to activate a network policy by associating it with an account?**  
A) ATTACH POLICY  
B) APPLY SNAPSHOT POLICY  
C) MANAGE ACCOUNTS  
D) MONITOR USAGE  
**Answer:** A) ATTACH POLICY
**Answer:** A) ALL [PRIVILEGES]
**Answer:** A) USAGE
### 640 onwards. Snowflake SnowPro Architect Exam Questions (Privileges & Real-life Scenarios)
**Question 665. Scenario: The DevOps team wants to change the size of a warehouse. Which privilege do they need?**  
A) MODIFY  
B) OPERATE  
C) MANAGE WAREHOUSES  
D) APPLYBUDGET  
**Answer:** A) MODIFY
**Answer: D**
**Question 672**  
A masking policy uses the CURRENT_DATABASE() function in its body. What does it return?  
A) Database in use for the session  
B) Database containing the protected table  
C) Database containing the protected view  
D) Default database for the user  
**Answer: A**
**Question 676**  
In a real-life scenario, an architect needs to ensure that only the DBA_EMPL_INFO role can see actual employee data when querying a view, while CSR_EMPL_INFO sees only hashed values. Which masking policy logic achieves this?  
A) Use only CURRENT_ROLE in the policy  
B) Use only INVOKER_ROLE in the policy  
C) Use both CURRENT_ROLE and INVOKER_ROLE with role checks  
D) Use CURRENT_DATABASE  
**Answer: A**
**Question 677**  
Which function returns TRUE if the user's current session role inherits the privileges of another specified role?  
A) IS_GRANTED_TO_INVOKER_ROLE  
B) IS_ROLE_IN_SESSION  
C) CURRENT_SCHEMA  
D) INVOKER_SHARE  
**Answer: B**
**Question 637. Which privilege enables a user to execute a dbt project and retrieve files from it?**  
A) USAGE  
B) OPERATE  
C) MONITOR  
D) MODIFY  
**Answer:** A) USAGE
**Question 682**  
If a masking policy is applied to a column and checks CURRENT_ROLE() IN ('ANALYST'), what happens if the current session is using a role that is a child of ANALYST in the hierarchy?  
A) Sees masked data  
B) Sees unmasked data  
C) Sees NULL  
D) Error  
**Answer: B**
**Question 683**  
An architect wants to allow a specific schema to always see unmasked data regardless of user role. Which context function should be used in the masking policy?  
A) CURRENT_SCHEMA  
B) CURRENT_ROLE  
C) INVOKER_ROLE  
D) IS_ROLE_IN_SESSION  
**Answer: A**
**694. Which ALTER USER action will immediately log a user out and prevent them from running any further queries?**  
A) ABORT ALL QUERIES  
B) SET DISABLED = TRUE  
C) RESET PASSWORD  
D) SET TYPE = SERVICE  
**Answer: B**
**Question 686**  
What is the impact of applying a masking policy with INVOKER_ROLE to a table that is accessed via a view?  
A) The view owner's role determines access  
B) The session user's role always determines access  
C) The masking policy is ignored  
D) The database owner determines access  
**Answer: A**
**Question 687**  
A business wants to mask employee data only if the querying role does not belong to the HR department. What is the most effective way to implement this in a masking policy?  
A) Check CURRENT_DATABASE  
B) Check CURRENT_ROLE for HR roles  
C) Use INVOKER_SHARE  
D) Use IS_ROLE_IN_SESSION for HR roles  
**Answer: D**
**Question 688**  
Which context function would you use to determine if the current session’s role is a descendant of a specified custom role in the role hierarchy?  
A) INVOKER_ROLE  
B) IS_ROLE_IN_SESSION  
C) CURRENT_SCHEMA  
D) IS_GRANTED_TO_INVOKER_ROLE  
**Answer: B**
**Question 689**  
If a masking policy uses the following logic:  
CASE  
  WHEN INVOKER_ROLE() IN ('FINANCE') THEN val  
  ELSE '***'  
END;  
Who will see unmasked values when querying a view?  
A) Only view owner role FINANCE  
B) Any session user with FINANCE as CURRENT_ROLE  
C) All users  
D) No one  
**Answer: A**
**Question 692**  
A developer wants to verify if the payroll role is in the view owner role hierarchy before granting unmasked access. Which statement should they execute?  
A) SHOW GRANTS TO ROLE view_owner_role  
B) SHOW GRANTS OF ROLE payroll  
C) DESCRIBE VIEW  
D) SELECT CURRENT_ROLE()  
**Answer: A**
### 695 ONWARDS Snowflake SnowPro Architect Exam (Questions 694–708): ALTER USER, Privileges, and Real-World Scenarios
**704. As an architect, you need to allow a workload running on AWS to authenticate without a password. Which ALTER USER property should you set?**  
A) WORKLOAD_IDENTITY with TYPE=AWS and ARN  
B) SET TYPE = LEGACY_SERVICE  
C) SET DISABLE_MFA = TRUE  
D) SET PASSWORD = 'aws-auth'  
**Answer: A**
**705. What does the RESET PASSWORD command do in ALTER USER?**  
A) Changes the user password immediately  
B) Generates a one-time URL for the user to reset their password  
C) Disables the user account  
D) Removes all MFA methods  
**Answer: B**
**708. A business user with AUDIT privileges needs to view unredacted secure object errors in metadata. Which property should be set?**  A) ENABLE_UNREDACTED_QUERY_SYNTAX_ERROR = TRUE  
B) ENABLE_UNREDACTED_SECURE_OBJECT_ERROR = TRUE  
C) STRICT_JSON_OUTPUT = TRUE  
D) DISABLE_MFA = TRUE  
**Answer: B**
#### Question 6852  
Which AWS CLI command is used to generate the federated token needed when enabling AWS PrivateLink for Snowflake?  
A) aws ec2 create-vpc-endpoint  
B) aws sts get-federation-token  
C) aws s3api create-endpoint  
D) aws iam get-access-token  
**Answer:** B
#### Question 6853  
A large financial services company wants to block all public access to their Snowflake account after confirming PrivateLink works. What is the appropriate next step?  
A) Delete all public endpoints  
B) Apply a network policy restricting access to specific CIDR ranges  
C) Contact AWS Support to remove public IPs  
D) Change the Snowflake account region  
**Answer:** B
#### Question 6854  
Which of the following is required for successfully configuring DNS for Snowflake Private Link access?  
A) CNAME records must point to public IP addresses  
B) CNAME records must resolve to private IP addresses within your VPC  
C) Create an A record for each endpoint  
D) Use only default AWS DNS  
**Answer:** B
#### Question 6855  
A retailer uses multiple Snowflake accounts (dev, test, prod) in the same AWS region. What is the recommended client configuration for OCSP with PrivateLink?  
A) Use the regionless OCSP hostname  
B) Use the OCSP hostname with the account identifier in the DNS  
C) Disable OCSP in client drivers  
D) Use only the default hostname  
**Answer:** B
#### Question 6856  
Which Snowflake system function can be used to verify that your account is successfully authorized for AWS PrivateLink?  
A) SYSTEM$GET_PRIVATELINK  
B) SYSTEM$REVOKE_PRIVATELINK  
C) SYSTEM$AUTHORIZE_PRIVATELINK  
D) SYSTEM$ALLOWLIST  
**Answer:** A
#### Question 6857  
A consulting firm is onboarding a managed cloud vendor to connect via PrivateLink. Which action is *required* for authorizing the vendor’s AWS account?  
A) Complete self-service enablement  
B) Retrieve the AWS account identifier from the vendor and contact Snowflake Support  
C) Use SYSTEM$GET_PRIVATELINK_CONFIG  
D) Add the vendor’s account to your network policy  
**Answer:** B

#### Question 6861  
After setting up AWS PrivateLink, a business sees S3 traffic for Snowflake still traversing the public internet. What is the *most probable* configuration step they missed?  
A) Did not configure an S3 gateway or interface endpoint  
B) Did not update client drivers  
C) Did not call SYSTEM$ALLOWLIST  
D) Did not enable OCSP  
**Answer:** A
#### Question 6862  
Which system function should be used to disable AWS PrivateLink for your Snowflake account?  
A) SYSTEM$REVOKE_PRIVATELINK  
B) SYSTEM$DISABLE_PRIVATELINK  
C) SYSTEM$AUTHORIZE_PRIVATELINK  
D) SYSTEM$REMOVE_PRIVATELINK  
**Answer:** A
Question 685  
### 2. In a business scenario, a company needs to clone a table with a sequence generating default values for a column. The table and sequence are in different schemas. After cloning the table, what does the cloned table reference?  
A) The cloned sequence  
B) The source sequence  
C) No sequence  
D) A new sequence created during the clone  
**Answer: B**
Question 686  
### 3. Which parameter must be set to ensure newly-cloned tables resume automatic clustering?  
A) AUTO_CLUSTER = TRUE  
B) CLUSTER_KEY = ENABLED  
C) ALTER TABLE <name> RESUME RECLUSTER  
D) CLUSTER_AUTO_RESUME = TRUE  
**Answer: C**
Question 687  
### 4. In a scenario where a database is cloned using Time Travel, what happens if a child table's data retention period is shorter than the requested time travel period?  
A) The child table is cloned with empty data  
B) The child table is not cloned and an error occurs  
C) The child table is cloned with only current data  
D) The child table is always cloned regardless of retention  
**Answer: B**
**Answer: C**
Question 689  
### 6. Which of the following statements about cloning event tables in Snowflake is correct?  
A) You can clone an event table into a regular table  
B) You can only clone from and to event tables  
C) Event tables cannot be cloned  
D) You can clone event tables into external tables  
**Answer: B**
Question 694  
### 11. What is the recommended practice when cloning an object with DATA_RETENTION_TIME_IN_DAYS set to 0 for some tables?  
A) Set DATA_RETENTION_TIME_IN_DAYS=1 before cloning and reset after  
B) No action is needed  
C) Set DATA_RETENTION_TIME_IN_DAYS=0 for all tables  
D) Drop all tables with retention=0 before cloning  
**Answer: A**
Question 696  
### 13. Which of the following statements about cloning external named stages is correct?  
A) Cloning an external stage clones the underlying cloud storage  
B) Cloning an external stage has no impact on the referenced cloud storage  
C) Cloning an external stage deletes the original stage  
D) Cloning is not supported for external stages  
**Answer: B**
Question 699  
### 16. When cloning a table protected by a differential privacy policy, but not the policy itself, what is the result?  
A) The clone is not privacy-protected  
B) The privacy policy is applied to the clone  
C) The clone is privacy-protected but the policy is not cloned  
D) The clone fails  
**Answer: C**
Question 700  
### 17. What is the default state of a pipe object with AUTO_INGEST=TRUE after being cloned?  
A) Active  
B) STOPPED_CLONED  
C) PAUSED  
D) RUNNING  
**Answer: B**
### Snowflake SnowPro Architect Certification Practice Questions (Federated Authentication & SSO)
**Question 702**  
Which step is required when setting up a relying party trust for Snowflake in AD FS?  
A) Configure OAuth 2.0 protocol  
B) Enable SAML 2.0 WebSSO support  
C) Add DNS entries for Snowflake  
D) Assign all users to the Snowflake group  
**Answer: B**
**Question 704**  
When setting up Okta for Snowflake federated authentication, which of the following is TRUE about the SubDomain field in the Okta application?  
A) It must always be set to "snowflake"  
B) It should contain the Snowflake account identifier, and include "privatelink" if using private connectivity  
C) It should be left blank  
D) It must use the user's email address  
**Answer: B**

*Question 707**  
A company is migrating from Okta to a custom SAML 2.0 IdP. What should the architect verify regarding field values during configuration?  
A) That all field values are in uppercase  
B) That field values are case-sensitive and must match exactly  
C) That field values are encrypted  
D) That field values are unique per user  
**Answer: B**
**Question 705**  
If your Snowflake account name contains an underscore and you are using Okta, what should you do for the SubDomain field?  
A) Leave the underscore as is  
B) Replace the underscore with a hyphen  
C) Remove the underscore  
D) Add a slash before the underscore  
**Answer: B**
### Question 719
A Snowflake security administrator wants to ensure that connections from public IPv4 addresses are blocked, while allowing AWS PrivateLink access. Which configuration is correct?
A) Add the public IPv4 range to the allowed list  
B) Add the AWS VPCE ID to the allowed list and 0.0.0.0/0 to the blocked list  
C) Only use ALLOWED_IP_LIST with AWS VPCE ID  
D) Only use BLOCKED_IP_LIST with 0.0.0.0/0  
E) No configuration is needed; this is default behavior  
**Answer:** B
### Question 722
Which of the following is true about network policy precedence in Snowflake?
A) Account-level policies override user-level policies  
B) Security integration policies override user-level policies  
C) User-level policies override account and security integration policies  
D) The most recently created policy always takes precedence  
E) All policies are evaluated equally  
**Answer:** C
### Question 723
You need to temporarily bypass a network policy for a user. What should you do?
A) Set MINS_TO_BYPASS_NETWORK_POLICY with Snowflake Support  
B) Modify the user’s allowed list  
C) Add user’s IP to blocked list  
D) Deactivate the network policy  
E) Change the user’s role to SYSADMIN  
**Answer:** A
### Question 726
Which network rule mode should be used to protect both the Snowflake service and the internal stage?
A) EGRESS  
B) INGRESS  
C) HOST_PORT  
D) OUTBOUND  
E) INTERNAL_STAGE only  
**Answer:** B
### Question 728
If a user is already logged in and a new network policy blocks their current network location, what happens?
A) User can continue querying until logout  
B) User is immediately logged out  
C) User is prevented from executing further queries  
D) User is prompted to re-authenticate  
E) Nothing happens  
**Answer:** C
### Question 730
In a scenario where private connectivity is used for the Snowflake service but public for the internal stage, which is the recommended setup?
A) Use VPCE ID for both  
B) Use private IPs for both  
C) Use private IPs for service, IPV4 for internal stage  
D) Use VPCE ID for internal stage, IPV4 for service  
E) Only use allowed lists  
**Answer:** D

716. Which of the following objects CANNOT be cloned using the CREATE <object> ... CLONE command in Snowflake?
A) Schemas  
B) Tables  
C) External tables  
D) Databases  
**Answer:** C) External tables
722. While cloning a database with child tables whose historical data has been purged, what parameter allows you to skip these tables and proceed with the clone?
A) IGNORE TABLES WITH INSUFFICIENT DATA RETENTION  
B) SKIP PURGED TABLES  
C) OMIT OLD DATA  
D) BYPASS RETENTION CHECK  
**Answer:** A) IGNORE TABLES WITH INSUFFICIENT DATA RETENTION
723. When using the COPY GRANTS parameter in a CLONE statement for a table, what is the effect?
A) Only future grants are copied  
B) Only explicit access privileges are copied  
C) No privileges are copied  
D) Both explicit and future grants are copied  
**Answer:** B) Only explicit access privileges are copied
725. If a schema is cloned using a TIMESTAMP that predates the creation of a table in the original schema, what happens to that table?
A) It is cloned with empty data  
B) It is cloned as a view  
C) It is not cloned  
D) It is cloned but locked  
**Answer:** C) It is not cloned
726. If an architect clones a schema containing user tasks using CREATE SCHEMA ... CLONE ... AT (TIMESTAMP => ...), what happens to the tasks?
A) Tasks are always cloned  
B) Tasks are cloned only if enabled  
C) Tasks are not cloned  
D) Tasks are cloned as paused  
**Answer:** C) Tasks are not cloned
727. In which case would a clone operation fail when using COPY GRANTS?
A) If the target table does not exist  
B) If the source and clone are in the same schema  
C) If the database role already exists in the target database  
D) If the source table is empty  
**Answer:** C) If the database role already exists in the target database
728. What happens if you try to use both OR REPLACE and IF NOT EXISTS in the same CREATE ... CLONE statement?
A) The operation is successful  
B) The OR REPLACE is ignored  
C) The IF NOT EXISTS is ignored  
D) An error is returned  
**Answer:** D) An error is returned
730. Which of the following is NOT inherited by a cloned table in Snowflake?
A) Structure  
B) Data  
C) Load history  
D) Clustering keys  
**Answer:** C) Load history
### Snowflake Snowpro Architect Storage Integration Questions  
#### Starting from Question 731
**Question 731**  
Which parameter in the CREATE STORAGE INTEGRATION command is used to restrict access to specific buckets or paths?  
A) STORAGE_BLOCKED_LOCATIONS  
B) STORAGE_ALLOWED_LOCATIONS  
C) STORAGE_AWS_ROLE_ARN  
D) ENABLED  
E) COMMENT  
**Answer:** B) STORAGE_ALLOWED_LOCATIONS  
**Question 735**  
What is the effect of setting ENABLED = FALSE on a storage integration?  
A) Existing stages will still function, but new ones cannot be created  
B) Existing and new stages cannot access the storage location  
C) Only administrators can use the integration  
D) The integration is deleted  
E) It enables private connectivity  
**Answer:** B) Existing and new stages cannot access the storage location  
**Question 738**  
A business has several departments with their own S3 buckets. What is a best practice to restrict each department’s access to only their bucket using storage integrations?  
A) Use one integration with all buckets in STORAGE_ALLOWED_LOCATIONS  
B) Create separate storage integrations, each limited to a department’s bucket  
C) Grant ACCOUNTADMIN to all departments  
D) Use STORAGE_BLOCKED_LOCATIONS only  
E) Disable all integrations by default  
**Answer:** B) Create separate storage integrations, each limited to a department’s bucket  
**Question 739**  
Which role must a user have to create a storage integration in Snowflake by default?  
A) SYSADMIN  
B) SECURITYADMIN  
C) ACCOUNTADMIN  
D) PUBLIC  
E) INTEGRATIONADMIN  
**Answer:** C) ACCOUNTADMIN  
**Question 741**  
Which parameter grants the bucket owner full control over files unloaded to S3 via an integration?  
A) STORAGE_AWS_ROLE_ARN  
B) STORAGE_AWS_OBJECT_ACL  
C) STORAGE_BLOCKED_LOCATIONS  
D) STORAGE_ALLOWED_LOCATIONS  
E) ENABLED  
**Answer:** B) STORAGE_AWS_OBJECT_ACL  
**Question 742**  
A multinational company has Snowflake accounts in both China and the US. What consideration must be taken when configuring storage integrations for S3 in China?  
A) Use STORAGE_PROVIDER = 'S3' and protocol s3://  
B) Use STORAGE_PROVIDER = 'S3CHINA' and protocol s3china://  
C) Use STORAGE_PROVIDER = 'S3GOV' and protocol s3gov://  
D) Use only GCS  
E) Use only Azure  
**Answer:** B) Use STORAGE_PROVIDER = 'S3CHINA' and protocol s3china://  
### Snowflake SnowPro Architect Certification – External Access & Real-World Scenarios
#### Question 751
A company wants to restrict access to an external API to only a specific endpoint. Which object should specify this restriction?
A) Secret  
B) Database  
C) Network Rule  
D) Storage Integration  
E) Resource Monitor  
**Answer:** C) Network Rule
Which of the following statements best describes a real-world use case for external access integration in Snowflake?
A) Loading CSV files into tables  
B) Granting users access to dashboards  
C) Invoking external ML models via REST endpoints from UDFs  
D) Partitioning tables for faster queries  
E) Configuring failover clusters  
**Answer:** C) Invoking external ML models via REST endpoints from UDFs
#### Question 754
Which of these actions is necessary before creating an external access integration for an AWS S3 bucket using IAM?
A) Create a secret of type PASSWORD  
B) Create a network rule with TYPE = PRIVATE_HOST_PORT  
C) Create a file format for S3  
D) Grant PUBLIC access to S3  
E) Create a Snowflake task  
**Answer:** B) Create a network rule with TYPE = PRIVATE_HOST_PORT
#### Question 758
A business needs to update the refresh token for a Google OAuth secret in Snowflake. Which function is used after completing the OAuth consent process?
A) SYSTEM$AUTH_REFRESH  
B) SYSTEM$FINISH_OAUTH_FLOW  
C) SYSTEM$START_OAUTH_FLOW  
D) SYSTEM$UPDATE_TOKEN  
E) SYSTEM$REFRESH_CREDS  
**Answer:** B) SYSTEM$FINISH_OAUTH_FLOW
#### Question 759
Which of the following can be included in the ALLOWED_NETWORK_RULES parameter of an external access integration?
A) A database name  
B) A secret name  
C) A network rule  
D) A virtual warehouse  
E) A user role  
**Answer:** C) A network rule
### Snowflake SnowPro Architect Certification Exam Questions: External Functions
**Question 746**  
Which component is responsible for relaying requests from Snowflake to the remote service in an external function workflow?  
A) Snowflake Data Marketplace  
B) Proxy service  
C) API integration  
D) Remote service  
E) Snowflake virtual warehouse  
**Answer: B) Proxy service**
**Question 750**  
Which of the following is a limitation of external functions in Snowflake?  
A) They can only be written in SQL  
B) They do not support overloading  
C) They cannot be used in DEFAULT clauses of CREATE TABLE statements  
D) They support sharing via Secure Data Sharing  
E) They are only available on AWS  
**Answer: C) They cannot be used in DEFAULT clauses of CREATE TABLE statements**
**Question 751**  
A financial institution needs to ensure that all data sent to an external function is encrypted in transit. Which protocol is required for the remote service endpoint?  
A) HTTP  
B) FTP  
C) HTTPS  
D) SFTP  
E) WebSocket  
**Answer: C) HTTPS**
**Question 753**  
In a business scenario, a company wants to restrict access to an external function to only paid subscribers. Which component is best suited for implementing this business logic?  
A) Remote service  
B) Snowflake role  
C) Proxy service  
D) Snowflake Task  
E) External table  
**Answer: C) Proxy service**
**Question 754**  
What is the maximum response size per batch for Snowflake external functions?  
A) 1MB  
B) 5MB  
C) 10MB  
D) 50MB  
E) Unlimited  
**Answer: C) 10MB**
746. Which of the following best describes a Snowflake tag?
A) A user-level permission object  
B) A schema-level object that can be assigned to other objects  
C) A data masking policy  
D) A metadata-only object  
**Answer: B**
747. What happens if you set a tag on a table in Snowflake?
A) The tag is only associated with the table object itself  
B) The tag is inherited by all columns within the table  
C) The tag is removed from the schema  
D) The tag is not visible to any user  
**Answer: B**
749. A marketing team wants to apply their own tags to tables and views, but a central governance team wants consistent tag names. What Snowflake feature allows both approaches?
A) Only centralized management is supported  
B) Tags support both centralized and decentralized management  
C) Tags must be unique across the entire account  
D) Only decentralized management is supported  
**Answer: B**
750. What is the maximum number of unique tags that can be assigned to a single table in Snowflake?
A) 10  
B) 25  
C) 50  
D) 100  
**Answer: C**
755. What is the correct way to remove a tag from an object in Snowflake?
A) DROP TAG <tag_name>  
B) ALTER <object> UNSET TAG <tag_name>  
C) REVOKE TAG <tag_name>  
D) ALTER <object> DROP TAG <tag_name>  
**Answer: B**
756. In which scenario would tag-based masking policies require Enterprise Edition or higher?
A) When tags are only used for monitoring  
B) When tags are set on schemas only  
C) When using tag-based masking policies  
D) When migrating tags between accounts  
**Answer: C**
760. What is a key limitation regarding future grants and tags in Snowflake?
A) Future grants on tags are fully supported  
B) Future grants of privileges on tags are not supported  
C) Tags can only be granted to roles  
D) Future grants can only be set at the schema level  
**Answer: B**
753. Which of the following statements about organization creation is correct?
A) Customers directly create organizations through SQL  
B) Organizations are automatically created during self-service account signup  
C) Only Snowflake Support can create accounts  
D) Organizations must be created for each region separately  
**Answer: B**
754. What happens if the organization name is changed but the original account URL is retained?
A) The old URL is lost immediately  
B) The old URL remains valid for 90 days before being dropped  
C) The old URL is never removed  
D) The new URL is not accessible  
**Answer: B**
756. How does a user find the current organization name in SQL?
A) SELECT ORGANIZATION_NAME();  
B) SHOW ORGANIZATION NAME;  
A) c)cURRENT_ORGANIZATION_NAME  
D) GET ORGANIZATION;  
**Answer: C**
746. Which Snowflake client does NOT support key pair authentication?  
A) JDBC Driver  
B) Snowflake CLI  
C) ODBC Driver  
D) None of the above  
**Answer: D) None of the above**  
747. What is the minimum RSA key length required for key pair authentication in Snowflake?  
A) 1024 bits  
B) 2048 bits  
C) 3072 bits  
D) 4096 bits  
**Answer: B) 2048 bits**  
749. When rotating public keys, which Snowflake user property can you use in addition to RSA_PUBLIC_KEY?  
A) RSA_PUBLIC_KEY_2  
B) SECONDARY_PUBLIC_KEY  
C) ROTATED_PUBLIC_KEY  
D) PUBLIC_KEY_ROTATION  
**Answer: A) RSA_PUBLIC_KEY_2**  
750. What privilege is needed to assign a public key to a Snowflake user?  
A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS  
B) CREATE USER  
C) USAGE  
D) IMPORT KEY  
**Answer: A) MODIFY PROGRAMMATIC AUTHENTICATION METHODS**  
753. How can you verify the user’s public key fingerprint in Snowflake?  
A) DESC USER <user>  
B) SHOW KEYS <user>  
C) SELECT * FROM KEY_HISTORY  
D) SHOW USERS  
**Answer: A) DESC USER <user>**  
756. Which of these clients does NOT fully support encrypted private keys for authentication?  
A) Go driver  
B) Node.js Driver  
C) JDBC Driver  
D) ODBC Driver  
**Answer: A) Go driver**  
759. Your company wants to rotate keys for compliance. How many public keys can be associated with a user at the same time?  
A) 1  
B) 2  
C) 3  
D) Unlimited  
**Answer: B) 2**  
761. What is the recommended method for protecting the locally generated private key?  
A) Use file permission mechanisms of the OS  
B) Email it to yourself  
C) Print and file it  
D) Leave it unprotected  
**Answer: A) Use file permission mechanisms of the OS**
### Snowflake SnowPro Architect Certification: Reader Accounts & Data Sharing  
**747. What happens if you drop a reader account and immediately attempt to create a new one to stay within the account limit?**  
A) The new account is created immediately  
B) Creation fails, and you must wait 24 hours  
C) Creation fails, and you must wait 7 days  
D) The limit increases automatically  
E) Snowflake bills you for both accounts  
**Answer: C) Creation fails, and you must wait 7 days**
**748. Which role or privilege is required to create a reader account?**  
A) SYSADMIN  
B) ACCOUNTADMIN  
C) PUBLIC  
D) SECURITYADMIN  
E) USERADMIN  
**Answer: B) ACCOUNTADMIN**
**749. If a provider account creates a reader account, who is responsible for credit charges incurred by that reader account?**  
A) The reader account administrator  
B) Snowflake  
C) The provider account  
D) The consumer  
E) The system administrator  
**Answer: C) The provider account**
**751. In a business scenario, you need to limit warehouse usage in a reader account. What should you set up?**  
A) User quotas  
B) Data retention policies  
C) Resource monitor  
D) Network policies  
E) Masking policies  
**Answer: C) Resource monitor**
**753. Which DDL command is used to create a reader account using SQL?**  
A) CREATE READER ACCOUNT  
B) CREATE ROLE READER  
C) CREATE MANAGED ACCOUNT  
D) CREATE ACCOUNT READER  
E) CREATE CONSUMER ACCOUNT  
**Answer: C) CREATE MANAGED ACCOUNT**
**755. Which of the following is true about support for reader accounts?**  
A) Reader accounts have direct Snowflake support  
B) Provider account handles support requests from reader accounts  
C) Only Snowflake can answer reader questions  
D) Reader accounts have a separate support plan  
E) Support is not available to reader accounts  
**Answer: B) Provider account handles support requests from reader accounts**
**757. What happens to all objects in a reader account when it is dropped?**  
A) They are transferred to another account  
B) They are retained for 7 days  
C) They are immediately dropped  
D) They are archived  
E) They are shared with the provider account  
**Answer: C) They are immediately dropped**
**758. Which command lists all reader accounts for your provider account?**  
A) SHOW READER ACCOUNTS  
B) SHOW MANAGED ACCOUNTS  
C) LIST READER ACCOUNTS  
D) DESCRIBE READER ACCOUNTS  
E) SHOW ALL ACCOUNTS  
**Answer: B) SHOW MANAGED ACCOUNTS**
**760. Which of these can be delegated to users other than ACCOUNTADMIN via privilege grants?**  
A) Managing billing  
B) Creating reader accounts  
C) Creating masking policies  
D) Creating external tables  
E) Enabling support  
**Answer: B) Creating reader accounts**
**766. Which of the following commands is NOT executable in a reader account?**  
A) CREATE MASKING POLICY  
B) CREATE MATERIALIZED VIEW  
C) COPY INTO <location>  
D) SHOW MANAGED ACCOUNTS  
E) SELECT  
**Answer: A) CREATE MASKING POLICY**
**767. A provider has reached the limit of 20 reader accounts and needs more. What should they do?**  
A) Drop existing accounts only  
B) Contact Snowflake Support to request a limit increase  
C) Wait 30 days  
D) Use another Snowflake region  
E) Use consumer accounts instead  
**Answer: B) Contact Snowflake Support to request a limit increase**
**768. Which privilege must be granted to a custom role to allow creation of reader accounts?**  
A) CREATE DATABASE  
B) CREATE ACCOUNT  
C) CREATE USER  
D) CREATE SHARE  
E) CREATE ROLE  
**Answer: B) CREATE ACCOUNT**
**769. If a provider wants to rename a reader account, what must they use?**  
A) Web interface  
B) SQL commands  
C) Classic Console  
D) Support ticket  
E) Account Usage views  
**Answer: B) SQL commands**
**770. What identifier is a legacy way to reference a reader account?**  
A) Account URL  
B) Account name  
C) Account locator  
D) Account admin name  
E) Region name  
**Answer: C) Account locator**
**778. Which of the following commands is allowed in a reader account?**  
A) CREATE PIPE  
B) CREATE IMAGE REPOSITORY  
C) SHOW PROCEDURES  
D) CREATE MATERIALIZED VIEW  
E) CREATE ROW ACCESS POLICY  
**Answer: D) CREATE MATERIALIZED VIEW**
747. What happens if you create a user in Snowflake with no password when federated authentication is enabled?  
A) The user can log in using their Snowflake credentials.  
B) The user must use federated authentication to log in.  
C) The user cannot be created.  
D) The user can log in using any IdP.  
**Answer: B**
750. Which Snowflake client supports browser-based SSO for authentication?  
A) Python Connector v1.0  
B) JDBC Driver v3.2.7 or higher  
C) ODBC Driver v1.0  
D) SnowSQL v1.0  
**Answer: B**


753. What must be set as the authenticator for browser-based SSO in the Snowflake JDBC driver?  
A) externalbrowser  
B) SAML  
C) MFA  
D) default  
**Answer: A**
752. In which scenario is browser-based SSO NOT supported?  
A) Client application runs on a user’s machine.  
B) Client application runs on a server without browser access.  
C) User has MFA enabled.  
D) JDBC driver is used.  
**Answer: B**

754. Which of these is recommended for all non-administrator users in a Snowflake account with federated authentication?  
A) Enable Snowflake authentication  
B) Disable Snowflake authentication  
C) Grant ACCOUNTADMIN role  
D) Use only password authentication  
**Answer: B**

755. What is the purpose of enabling connection caching in Snowflake clients?  
A) To allow multiple users to share a connection  
B) To minimize the number of authentication prompts  
C) To bypass MFA  
D) To disable SSO  
**Answer: B**

756. To enable connection caching in Snowflake, which account parameter must be set to true?  
A) CACHE_CONNECTION  
B) ALLOW_ID_TOKEN  
C) ENABLE_CACHING  
D) TOKEN_CACHE  
**Answer: B**

757. Which user role is required to enable connection caching in Snowflake?  
A) SYSADMIN  
B) SECURITYADMIN  
C) ACCOUNTADMIN  
D) USERADMIN  
**Answer: C**

758. After disabling a user in Snowflake, what must be done for them to regain access?  
A) Change their password  
B) Enable the user in Snowflake  
C) Reset their MFA  
D) Update their IdP profile  
**Answer: B**

759. Which authentication method allows Snowflake client applications to authenticate without browser access, when using Okta?  
A) Browser-based SSO  
B) Native SSO  
C) SAML authentication  
D) Password authentication  
**Answer: B**

760. What must you configure as the authenticator parameter for native SSO with Okta in Snowflake clients?  
A) externalbrowser  
B) SAML  
C) The Okta URL endpoint  
D) MFA  
**Answer: C**
761. Which error may occur if too many authentication requests are sent to Okta Identity Engine in a short period?  
A) HTTP 401  
B) HTTP 404  
C) HTTP 429  
D) HTTP 500  
**Answer: C**
762. What is the default rate limit for Okta’s authentication endpoint used by Snowflake client drivers?  
A) 100 requests per user per minute  
B) 20 requests per user per 5 seconds  
C) Unlimited requests  
D) 1 request per user per second  
**Answer: B**
763. When using SSO with MFA in Snowflake, what is TRUE for users enrolled in MFA?  
A) MFA is not available with SSO  
B) MFA workflow is initiated within the SSO workflow  
C) MFA must be configured only in the IdP  
D) MFA is bypassed automatically  
**Answer: B**
764. If using native SSO with Okta, what is required regarding MFA?  
A) MFA must be enabled  
B) MFA must be disabled  
C) MFA has no effect  
D) MFA must be configured in both IdP and Snowflake  
**Answer: B**
765. Which of the following drivers supports connection caching in combination with MFA token caching?  
A) Python Connector v1.0  
B) JDBC Driver v3.12.8 or later  
C) ODBC Driver v2.0  
D) Node.js Driver v1.0  
**Answer: B**
766. How many different audience values does Snowflake support in the SAML 2.0 assertion from the IdP?  
A) 1  
B) 2  
C) 4  
D) 8  
**Answer: C**
767. What does Snowflake recommend regarding account administrator access when using federated authentication?  
A) Remove all admin passwords  
B) Maintain at least one admin with a Snowflake password  
C) Only use IdP for admin logins  
D) Only use MFA for admin accounts  
**Answer: B**

768. For a business with multiple Snowflake accounts and private connectivity, what SSO feature is most relevant?  
A) Multiple audience values in SAML assertion  
B) Only browser-based SSO  
C) Only native SSO  
D) MFA token caching  
**Answer: A**
769. What is the effect of setting MUST_CHANGE_PASSWORD to TRUE for a user with federated authentication?  
A) User is prompted to change password  
B) No effect; property is ignored  
C) User cannot log in  
D) User is forced to use SSO  
**Answer: B**
770. Which client application does NOT directly support browser-based SSO according to Snowflake documentation?  
A) SnowSQL v1.1.43  
B) JDBC Driver v3.2.7  
C) Power BI  
D) Python Connector v1.4.8  
**Answer: C**
771. What should a Snowflake architect do if their company requires all authentication prompts to be minimized for end users?  
A) Enable connection caching  
B) Disable MFA  
C) Use password authentication only  
D) Remove SSO  
**Answer: A**
772. In a real-life scenario, a user is unable to log into Snowflake after being disabled in the account, but can still access their Okta account. What is the cause?  
A) User is locked out in Okta  
B) User is not enabled in Snowflake  
C) User password expired  
D) MFA is misconfigured  
**Answer: B**

773. Which of the following business scenarios requires updating Snowflake client drivers to support Okta Identity Engine?  
A) Migrating from Okta Classic to Okta Identity Engine  
B) Switching to Azure AD  
C) Enabling MFA  
D) Upgrading to Snowflake Enterprise Edition  
**Answer: A**
774. What must be done before enabling connection caching for the Python connector?  
A) Install the "secure-local-storage" extra for the connector  
B) Set MFA to false  
C) Use version 1.0.0  
D) Enable browser-based SSO  
**Answer: A**
775. In a scenario where users must authenticate to Snowflake from a headless server, what is the recommended SSO approach?  
A) Use browser-based SSO  
B) Use native SSO via Okta  
C) Use MFA only  
D) Use only Snowflake authentication  
**Answer: B**
#### Questions 746–780
#### 746. Which of the following modes in a network rule is used to control outbound network requests in Snowflake?
A) INGRESS  
B) INTERNAL_STAGE  
C) EGRESS  
D) OUTBOUND  
E) DEFLECT  
**Answer:** C) EGRESS
### 747. Which of the following network identifiers can be used in a Snowflake network rule for incoming requests?  
A) VPC IDs  
B) LinkIDs of Azure private endpoints  
C) Domain names  
D) AWS S3 bucket names  
E) Database roles  
**Answer:** B) LinkIDs of Azure private endpoints
#### 748. To protect an AWS internal stage using a network rule, which mode must be used?  
A) EGRESS  
B) INGRESS  
C) INTERNAL_STAGE  
D) PROTECT  
E) EXTERNAL_ACCESS  
**Answer:** C) INTERNAL_STAGE
#### 749. What is the default port if you do not specify a port in a domain identifier in a network rule?  
A) 80  
B) 22  
C) 443  
D) 8080  
E) 25  
**Answer:** C) 443
#### 751. A Snowflake architect needs to restrict a UDF to only access external APIs from specific domains. What should be configured?  
A) Network policy with INGRESS mode  
B) Network rule with INTERNAL_STAGE mode  
C) Network rule with EGRESS mode  
D) VPC peering  
E) Storage integration  
**Answer:** C) Network rule with EGRESS mode
#### 752. Which of the following network identifiers is NOT supported for incoming request rules in Snowflake?  
A) IPv4 addresses  
B) VPCE IDs of AWS VPC endpoints  
C) VPC IDs  
D) LinkIDs of Azure private endpoints  
E) CIDR notation  
**Answer:** C) VPC IDs
#### 754. What is the effect of setting the ENFORCE_NETWORK_RULES_FOR_INTERNAL_STAGES parameter to TRUE?  
A) Enables replication of network rules  
B) Allows use of AWSVPCEID in INTERNAL_STAGE mode  
C) Allows CIDR notation for outgoing requests  
D) Enables Azure endpoints in EGRESS mode  
E) Grants usage on external tables  
**Answer:** B) Allows use of AWSVPCEID in INTERNAL_STAGE mode
#### 756. An architect wants to block all public network traffic to an internal stage on Azure. What is the correct approach?  
A) Use network rule with TYPE=AZUREVPCEID  
B) Block all ports in a network rule  
C) Block public traffic without network rule (network rules for internal stages are not supported on Azure)  
D) Use EGRESS mode with DENY action  
E) Set TYPE=DOMAIN in network rule  
**Answer:** C) Block public traffic without network rule
#### 759. A business requirement states that only traffic from 192.168.10.0 to 192.168.10.255 should access Snowflake. Which CIDR notation should be used in a network rule?  
A) 192.168.10.0/16  
B) 192.168.10.0/24  
C) 192.168.10.0/8  
D) 192.168.10.0/32  
E) 192.168.10.0/28  
**Answer:** B) 192.168.10.0/24
### 761. In a business scenario, an architect must allow all ports for a domain in a network rule. Which value should be used?  
A) example.com:443  
B) example.com:0  
C) example.com:1-65535  
D) example.com:*  
E) example.com:all  
**Answer:** B) example.com:
#### 762. Which of the following statements best describes the VALUE_LIST property in a network rule?  
A) List of database privileges  
B) List of network identifiers of the same type  
C) List of Snowflake users  
D) List of schemas  
E) List of roles  
**Answer:** B) List of network identifiers of the same type
#### 768. Which of the following is NOT a valid mode for a Snowflake network rule?  
A) INGRESS  
B) INTERNAL_STAGE  
C) EGRESS  
D) OUTBOUND_STAGE  
E) None of the above  
**Answer:** D) OUTBOUND_STAGE
#### 769. In a real-life scenario, a company must restrict Snowflake access to a list of specific AWS VPC endpoints. Which TYPE value should be used in the network rule?  
A) IPV4  
B) AWSVPCEID  
C) DOMAIN  
D) VPCID  
E) S3BUCKET  
**Answer:** B) AWSVPCEID
#### 770. Which Snowflake-managed network rule allows access to PyPi for installing packages in Snowpark Container?  
A) SNOWFLAKE.EXTERNAL_ACCESS.PYPI_RULE  
B) SNOWFLAKE.MANAGED.PYPI_NETWORK  
C) SNOWFLAKE.PUBLIC.PYPI_ACCESS  
D) SNOWFLAKE.EXTERNAL.PYPI_RULE  
E) SNOWFLAKE.AUTOMATION.PIP_RULE  
**Answer:** A) SNOWFLAKE.EXTERNAL_ACCESS.PYPI_RULE
#### 773. When using the INGRESS mode with TYPE=IPV4, what is the default scope of the network rule?  
A) External network access  
B) AWS internal stage only  
C) Snowflake service only  
D) All internal and external stages  
E) Azure private endpoints only  
**Answer:** C) Snowflake service only
#### 774. A business needs to organize and maintain network rules for different environments (dev, test, prod) using Snowsight. What should they use?  
A) Database-level network rules  
B) Schema-level network rules with descriptive comments  
C) Warehouse-level network rules  
D) Table-level access policies  
E) Account-level masking policies  
**Answer:** B) Schema-level network rules with descriptive comments
#### 777. Which of the following is a schema-level object in Snowflake?  
A) Network rule  
B) Network policy  
C) Virtual warehouse  
D) Account parameter  
E) Storage integration  
**Answer:** A) Network rule
#### 778. Which is a real-life business use case for EGRESS network rules?  
A) Limiting which domains a UDF can access on the internet  
B) Restricting inbound traffic to Snowflake  
C) Blocking access to Snowflake stages  
D) Assigning roles to users  
E) Encrypting data at rest  
**Answer:** A) Limiting which domains a UDF can access on the internet
 #### 780. Which of the following can be done with an ALTER NETWORK RULE statement?  
A) Change the network rule’s name  
B) Add or remove identifiers  
C) Change the TYPE property  
D) Move the rule to another schema  
E) Modify the mode property  
**Answer:** B) Add or remove identifiers
 746. Which feature allows Okta to automatically create new users in Snowflake when they are created in Okta?
A) Push New Users  
B) Push Groups  
C) Push User Deactivation  
D) Sync Password  
E) Reactivate Users  
**Answer: A) Push New Users**
747. In a scenario where a large company is migrating thousands of users to Snowflake via Okta SCIM, which limitation should the architect be most concerned with?
A) Okta does not support attribute mapping  
B) Maximum 500 concurrent requests per SCIM endpoint  
C) Cannot map defaultWarehouse  
D) Push Groups does not create users  
E) No role assignment is possible  
**Answer: B) Maximum 500 concurrent requests per SCIM endpoint**
748. An architect notices that updates to user profiles in Okta are not being reflected in Snowflake. What feature should they verify is enabled?
A) Push New Users  
B) Push Profile Updates  
C) Push User Deactivation  
D) Reactivate Users  
E) Enhanced Group Push  
**Answer: B) Push Profile Updates**
749. Which Snowflake role is recommended to own users and roles created by Okta during SCIM provisioning?
A) ACCOUNTADMIN  
B) SECURITYADMIN  
C) OKTA_PROVISIONER  
D) SYSADMIN  
E) PUBLIC  
**Answer: C) OKTA_PROVISIONER**
750. What action is required to invalidate an existing SCIM access token for a Snowflake security integration?
A) Revoke OKTA_PROVISIONER role  
B) Execute a DROP INTEGRATION statement  
C) Disable the user in Okta  
D) Unset the network policy  
E) Change the Snowflake password  
**Answer: B) Execute a DROP INTEGRATION statement**
751. A business wants to synchronize both users and roles from Okta to Snowflake. Which Okta feature is required for role management in Snowflake?
A) Push New Users  
B) Push Groups  
C) Push User Deactivation  
D) Sync Password  
E) Reactivate Users  
**Answer: B) Push Groups**
753. Which of the following statements is TRUE regarding SCIM network policies in Snowflake?
A) They override account-wide policies but are overridden by user policies  
B) They override user policies  
C) They cannot be set per integration  
D) They must always allow external IPs  
E) They are only available to ACCOUNTADMIN  
**Answer: A) They override account-wide policies but are overridden by user policies**
754. A company wants to bring existing Snowflake users under Okta management. What must the architect ensure before transferring ownership?
A) The user has the correct login_name property set  
B) The user is assigned defaultWarehouse  
C) The user is assigned PUBLIC role  
D) The user has a password set in Okta  
E) The user is deactivated in Okta  
**Answer: A) The user has the correct login_name property set**
755. What happens if the name for a Snowflake account contains an underscore when integrating with Okta?
A) Integration fails unless underscore is replaced with a hyphen  
B) Users cannot be provisioned  
C) Roles cannot be managed  
D) Snowflake disables SSO  
E) Enhanced Group Push is required  
**Answer: A) Integration fails unless underscore is replaced with a hyphen**
756. In a real-world scenario, what would likely cause user updates from Okta to fail in Snowflake after the SCIM integration?
A) The user is not owned by OKTA_PROVISIONER  
B) The user is not assigned a defaultWarehouse  
C) The user is missing an email address  
D) The user is in a nested group  
E) The SCIM endpoint uses HTTPS  
**Answer: A) The user is not owned by OKTA_PROVISIONER**
757. How can an architect enable Snowflake-initiated SSO after SCIM provisioning is complete?
A) Enable the feature manually in Snowflake  
B) Enable Enhanced Group Push in Okta  
C) Set a defaultSecondaryRoles attribute  
D) Assign ACCOUNTADMIN role to all users  
E) No additional steps are needed  
**Answer: A) Enable the feature manually in Snowflake**
758. Which of the following is NOT supported by Okta’s integration with Snowflake?
A) Importing Active Directory nested groups  
B) Creating new users  
C) Reactivating users  
D) Mapping custom attributes  
E) Deactivating users  
**Answer: A) Importing Active Directory nested groups**
759. What is the recommended method to replicate SCIM security integration configurations and network policies across Snowflake accounts?
A) Use replication and failover/failback  
B) Manually copy user accounts  
C) Export/import users via CSV  
D) Use Enhanced Group Push  
E) Set defaultRole for every user  
**Answer: A) Use replication and failover/failback**
760. Which SQL command allows you to view SCIM audit logs in Snowflake for troubleshooting?
A) SELECT * FROM TABLE(REST_EVENT_HISTORY('scim'))  
B) SHOW USERS  
C) DESCRIBE INTEGRATION okta_provisioning  
D) SELECT * FROM SNOWFLAKE.ACCOUNT_USAGE.USERS  
E) SHOW ROLES  
**Answer: A) SELECT * FROM TABLE(REST_EVENT_HISTORY('scim'))**
753. What happens if you grant privileges directly to a user in Snowflake?  
A) They are always considered  
B) They are only considered when USE SECONDARY ROLE is set to ALL  
C) They override role-based privileges  
D) They are ignored  
E) They persist only for the session  
**Answer: B) They are only considered when USE SECONDARY ROLE is set to ALL**
755. Which role should be used to create new users and roles in Snowflake?  
A) ACCOUNTADMIN  
B) USERADMIN  
C) SECURITYADMIN  
D) SYSADMIN  
E) ORGADMIN  
**Answer: B) USERADMIN**
A) The grantee role can be dropped  
B) Privileges of the granted role are inherited by the grantee  
C) The grantee loses privileges  
D) Users can activate both roles simultaneously  
E) The granted role becomes inactive  
**Answer: B) Privileges of the granted role are inherited by the grantee**
758. What command is used to change the primary role in an active Snowflake session?  
A) USE ROLE  
B) SET ROLE  
C) SWITCH ROLE  
D) ALTER ROLE  
E) GRANT ROLE  
**Answer: A) USE ROLE**
759. Which privilege is required to grant or revoke access on any object in the account globally?  
A) MANAGE GRANTS  
B) OWNERSHIP  
C) USAGE  
D) CREATE  
E) SELECT  
**Answer: A) MANAGE GRANTS**
760. In Snowflake, which type of role cannot be activated directly in a session?  
A) Account role  
B) Database role  
C) Custom role  
D) System-defined role  
E) Application role  
**Answer: B) Database role**
 
763. Which role is responsible for managing service endpoints in Snowflake?  
A) Service role  
B) Application role  
C) SYSADMIN  
D) ACCOUNTADMIN  
E) Database role  
766. Which role is best suited for managing user and role lifecycle in Snowflake?  
A) USERADMIN  
B) ACCOUNTADMIN  
C) SYSADMIN  
D) PUBLIC  
E) SECURITYADMIN  
**Answer: A) USERADMIN**
767. When transferring object ownership, what must be considered regarding privileges?  
A) All privileges except OWNERSHIP are revoked from the previous owner  
B) All privileges are retained by the previous owner  
C) The new owner must be a user  
D) Grants are automatically transferred to all users  
E) The object is dropped  
**Answer: A) All privileges except OWNERSHIP are revoked from the previous owner**
771. In Snowflake, what is the impact of using a managed access schema for compliance?  
A) Centralizes privilege management at the schema level  
B) Allows any object owner to grant access  
C) Disables privilege inheritance  
D) Prevents object creation  
E) Requires the PUBLIC role to manage grants 
773. What is the effect of granting a database role to a share?  
A) No other database roles can be granted to that database role  
B) The database role can be activated directly in a session  
C) The share is revoked  
D) The database is dropped  
E) Granting to account roles is blocked  
**Answer: A) No other database roles can be granted to that database role**
774. What is a best practice when creating custom roles that will own securable objects?  
A) Assign the top-most custom role to SYSADMIN  
B) Assign all privileges to PUBLIC  
C) Avoid role hierarchies  
D) Use only system-defined roles  
E) Assign to USERADMIN  
**Answer: A) Assign the top-most custom role to SYSADMIN**
775. Which privilege is NOT required by SECURITYADMIN to create a database role?  
A) CREATE DATABASE ROLE  
B) MANAGE GRANTS  
C) USAGE  
D) OWNERSHIP  
E) SELECT  
**Answer: E) SELECT**                                                                     a
779. Which role is recommended to manage all objects in the account except user and role management?  
A) SYSADMIN  
B) ACCOUNTADMIN  
C) PUBLIC  
D) USERADMIN  
E) SECURITYADMIN  
**Answer: A) SYSADMIN**
785. In Snowflake, which of the following is NOT a valid type of role?  
A) Application role  
B) Service role  
C) Database role  
D) Instance role  
E) Privilege role  
**Answer: E) Privilege role**
789. What is the recommended best practice for managing privileges on new tables in a schema?  
A) Use future grants  
B) Grant privileges directly to users  
C) Assign all to PUBLIC  
D) Use only system roles  
E) Grant USAGE only  
**Answer: A) Use future grants**
797. What is the impact of dropping a role that owns objects?  
A) You must transfer ownership before dropping the role  
B) The objects are dropped automatically  
C) The database is dropped  
D) The role is dropped regardless  
E) The objects become owned by PUBLIC  
**Answer: A) You must transfer ownership before dropping the role**
798. What is the correct way to grant multiple roles to a user in Snowflake?  
A) GRANT ROLE <role1>, <role2> TO USER <user>  
B) Grant each role individually to the user  
C) Use SET ROLE  
D) Assign the user to PUBLIC  
E) USE ROLE  
**Answer: B) Grant each role individually to the user**
801. What is the consequence of activating a new primary role in a session?  
A) The privileges of the new primary role are used for object creation  
B) All previous roles are dropped  
C) PUBLIC role is disabled  
D) The session is terminated  
E) All privileges are revoked  
**Answer: A) The privileges of the new primary role are used for object creation**
### 751. Which of the following can be included in a replication group?  
A) Databases and shares  
B) Warehouses and tables only  
C) External stages  
D) Data masking policies only  
**Answer: A)**
803. Which statement is true for privilege inheritance in a role hierarchy?  
A) Privileges are inherited by roles above in the hierarchy  
B) Only direct grants are inherited  
C) Users inherit nothing from roles  
D) Privileges are inherited only by system-defined roles  
E) Inheritance is not possible  
**Answer: A) Privileges are inherited by roles above in the hierarchy**
### 746. What must an organization administrator do before enabling data replication for cross-region sharing?
A) Grant REPLICATION privileges to all consumers  
B) Enable replication for the source and target accounts  
C) Set up data masking policies  
D) Grant SHARE privileges to all users  
**Answer: B)**

### 752. If you want to share only a subset of data from a master table to reduce replication costs, what should you do?
A) Use row-level security and replicate the whole database  
B) Create a new database with only the relevant rows  
C) Share the table directly without replication  
D) Use masking policies only  
**Answer: B)**
### 753. What is the purpose of creating a secure view when sharing data in Snowflake?
A) To encrypt data during replication  
B) To control and limit data exposure to consumers  
C) To enable faster refresh operations  
D) To avoid using replication groups  
**Answer: B)**

### 754. What must be done if a database already has individual replication enabled but needs to be added to a replication group?
A) Drop the database  
B) Disable individual database replication first  
C) Create a new schema  
D) Add the database to a share  
**Answer: B)**

### 755. In a business scenario, Acme wants to share data with a consumer in another region. What is the first step?
A) Grant access to the consumer  
B) Create and enable a replication group  
C) Set up a Snowflake Marketplace listing  
D) Refresh the replication group  
**Answer: B)**

### 756. Which operation is required from the target account to receive replicated data?
A) Create a secondary replication group as a replica  
B) Grant USAGE on the database  
C) Create a share in the target account  
D) Enable masking policies  
**Answer: A)**

### 757. What will happen if you add new accounts to a share in the target region and then refresh the replication group from the source?
A) The new accounts will be overwritten  
B) The new accounts will remain in the share  
C) The share will be deleted  
D) The share will be re-encrypted  
**Answer: B)**

### 758. Which SQL command is used to replicate objects to the current account in a target region?
A) ALTER DATABASE REFRESH  
B) ALTER REPLICATION GROUP REFRESH  
C) CREATE SHARE REFRESH  
D) GRANT USAGE REFRESH  
**Answer: B)**

### 759. Why might a business want to use streams and tasks when preparing data for cross-region sharing?
A) To automate masking policy application  
B) To copy only desired data changes to a new table  
C) To replicate objects other than databases  
D) To reduce compute costs on the target account  
**Answer: B)**

### 760. What is a key consideration when sharing data across regions and cloud platforms?
A) Ensuring all accounts use the same cloud provider  
B) Complying with legal and regulatory restrictions on data transfer  
C) Creating public shares for all consumers  
D) Disabling secure views  
**Answer: B)**

### 761. In Snowflake, what is the function of the REPLICATION_SCHEDULE parameter?
A) To set the time zone for replication  
B) To automate replication group refreshes  
C) To change the owner of a replication group  
D) To schedule warehouse auto-suspend  
**Answer: B)**

### 762. What must be granted to a share to allow a consumer to access a view?
A) USAGE on the database, USAGE on the schema, SELECT on the view  
B) USAGE on the warehouse only  
C) SELECT on the database only  
D) REPLICATION on the share  
**Answer: A)**

### 763. A company wants to share data from three databases with a partner in a different region. What is the best approach?
A) Create one share and one replication group including all three databases  
B) Replicate each database separately  
C) Use the Snowflake Marketplace  
D) Share only one database at a time  
**Answer: A)**

### 764. What action should be taken to transition from database replication to group-based replication?
A) Drop all shares  
B) Disable database replication first  
C) Create a new warehouse  
D) Grant masking policies  
**Answer: B)**

### 765. In a real-world scenario, what should be verified before replicating sensitive data to another country?
A) The region supports masking policies  
B) The destination account has a virtual warehouse  
C) Compliance with local legal and regulatory requirements  
D) The Snowflake edition is Enterprise or higher  
**Answer: C)**

### 766. Which of the following allows for automatic fulfillment of data products to other regions?
A) Manual refresh of replication groups  
B) Cross-Cloud Auto-fulfillment  
C) Data masking policies  
D) External tables only  
**Answer: B)**

### 767. What is required to start a task that updates data for replication?
A) Use ALTER TASK ... RESUME  
B) Run CREATE REPLICATION GROUP  
C) Use GRANT USAGE  
D) Run CREATE STREAM  
**Answer: A)**

### 768. When sharing data from multiple databases, what must be true for the replication group?
A) It must include all relevant databases and shares  
B) It must use only secure views  
C) It must exclude all masking policies  
D) It must be created in the target account  
**Answer: A)**

### 769. You notice data is not available in the consumer’s account after replication. What is the most likely cause?
A) The replication group was not refreshed  
B) The share was deleted  
C) The consumer lacks a virtual warehouse  
D) The database was not encrypted  
**Answer: A)**

### 770. What should a provider do to minimize egress and storage costs when sharing frequently updated data with many consumers in different regions?
A) Replicate the entire database to each consumer  
B) Create one copy per region and use shares for consumers in that region  
C) Use masking policies for each consumer  
D) Use the Data Marketplace  
**Answer: B)**

### 746. Which of the following statements about DDL and transactions in Snowflake is true?
A) DDL statements can be rolled back as part of an explicit transaction  
B) DDL statements implicitly commit any active transaction before executing  
C) DML statements implicitly commit active transactions  
D) DDL statements can be executed within a transaction and rolled back  
**Answer: B**

### 747. What happens if a session disconnects abruptly during an active transaction in Snowflake?
A) The transaction is immediately committed  
B) The transaction is left detached and may require manual abort  
C) The transaction is retried automatically  
D) The session is restored and resumes the transaction  
**Answer: B**

### 748. In Snowflake, which of the following is TRUE about AUTOCOMMIT?
A) When enabled, all statements are grouped into a single transaction  
B) When enabled, each statement outside an explicit transaction is automatically committed  
C) AUTOCOMMIT disables explicit transactions  
D) AUTOCOMMIT only affects DDL statements  
**Answer: B**

### 749. What is the recommended way to avoid confusion when using transactions in Snowflake?
A) Mix explicit and implicit transaction boundaries  
B) Use multiple BEGIN TRANSACTION statements in a row  
C) Avoid mixing implicit and explicit starts and ends in the same transaction  
D) Always use implicit transactions only  
**Answer: C**

### 750. Which operation will NOT block an INSERT statement on a standard table in Snowflake?
A) UPDATE  
B) DELETE  
C) ALTER TABLE  
D) None of the above  
**Answer: C**

### 751. What is the default isolation level supported in Snowflake transactions?
A) SERIALIZABLE  
B) READ UNCOMMITTED  
C) READ COMMITTED  
D) REPEATABLE READ  
**Answer: C**

### 752. In a multi-threaded client scenario, what can happen if two threads share the same Snowflake session?
A) Each thread gets its own transaction  
B) Threads can inadvertently roll back or commit each other's work  
C) Threads are strictly isolated  
D) Snowflake prevents concurrent thread access  
**Answer: B**

### 753. What happens when AUTOCOMMIT is set to FALSE and a stored procedure ends with an active transaction?
A) The transaction is automatically committed  
B) The transaction is left open for later use  
C) Snowflake implicitly rolls back the active transaction  
D) The session is terminated  
**Answer: C**

### 754. Which of the following SQL commands can be used to abort a running transaction in Snowflake?
A) ABORT TRANSACTION  
B) SYSTEM$ABORT_TRANSACTION  
C) END TRANSACTION  
D) CANCEL TRANSACTION  
**Answer: B**

### 755. When does Snowflake automatically abort and roll back a transaction that is blocking others and idle?
A) After 10 minutes  
B) After 1 hour  
C) After 5 minutes  
D) After 4 hours  
**Answer: C**

### 756. In Snowflake, what is the effect of executing multiple BEGIN TRANSACTION statements in a row?
A) Nested transactions are created  
B) Only the first BEGIN starts a transaction; others are ignored  
C) Each BEGIN creates a new transaction  
D) Transactions are rolled back  
**Answer: B**

### 757. What is the recommended approach for handling transactions in multi-threaded client applications using Snowflake?
A) Use a single connection for all threads  
B) Use separate connections for each thread  
C) Disable AUTOCOMMIT globally  
D) Avoid explicit transactions  
**Answer: B**

### 758. Which command returns a list of all active transactions for the current user in Snowflake?
A) SHOW TRANSACTIONS  
B) DESCRIBE TRANSACTIONS  
C) LIST TRANSACTIONS  
D) SELECT * FROM TRANSACTION_HISTORY  
**Answer: A**

### 759. Which of the following statements is true regarding locks and transactions in Snowflake?
A) Locks are released only on explicit COMMIT  
B) Locks are held indefinitely until released manually  
C) Locks are released on COMMIT or ROLLBACK  
D) Only DDL statements acquire locks  
**Answer: C**

### 760. What is the consequence of not pairing BEGIN/COMMIT blocks correctly in scoped transactions involving stored procedures?
A) The transaction is automatically committed  
B) Snowflake issues a warning but proceeds  
C) An error occurs, and the transaction is rolled back  
D) The statements are executed outside a transaction  
**Answer: C**

### 761. In a business scenario, you want to ensure all steps of a funds transfer (withdrawal and deposit) succeed or fail together. What should you do?
A) Use separate transactions for each step  
B) Group all related steps inside a single transaction  
C) Rely on AUTOCOMMIT for each step  
D) Use only DDL statements  
**Answer: B**

### 762. Which view would you use to analyze blocked transactions and locks in Snowflake?
A) LOCK_HISTORY  
B) LOCK_WAIT_HISTORY  
C) TRANSACTION_LOG  
D) BLOCKED_TRANSACTIONS  
**Answer: B**

### 763. What happens if a DDL statement is executed while a transaction is active in Snowflake?
A) The transaction and DDL are both rolled back  
B) The active transaction is implicitly committed before the DDL executes  
C) The DDL is ignored  
D) The DDL is executed within the current transaction  
**Answer: B**

### 764. In Snowflake, what happens to the changes from successful statements in a transaction if one statement fails but the transaction is committed?
A) All changes are rolled back  
B) Only changes from failed statements are rolled back; successful statements are committed  
C) The entire transaction fails  
D) Only the failed statement's changes are committed  
**Answer: B**

### 765. What is a best practice when designing transactions for business-critical operations in Snowflake?
A) Make transactions as large as possible  
B) Group only related statements that should succeed or fail together  
C) Use AUTOCOMMIT exclusively  
D) Avoid explicit transactions  
**Answer: B**

### 766. What is the result of running a multi-statement transaction where two concurrent UPDATE statements operate on different rows of a hybrid table?
A) They block each other  
B) They run in parallel without blocking  
C) Only one can succeed  
D) Both are rolled back  
**Answer: B**

### 767. Which parameter controls the amount of time a blocked statement waits for a lock before timing out in a standard table?
A) TRANSACTION_TIMEOUT  
B) LOCK_TIMEOUT  
C) WAIT_TIMEOUT  
D) STATEMENT_TIMEOUT  
**Answer: B**

### 768. After a failed DML statement within a transaction, what is the state of the transaction in Snowflake?
A) The transaction is automatically rolled back  
B) The transaction remains active and can be committed or rolled back  
C) The transaction is committed by default  
D) The session ends  
**Answer: B**

### 769. In Snowflake, which command can be used to monitor transactions by their ID?
A) MONITOR TRANSACTION  
B) DESCRIBE TRANSACTION  
C) LIST TRANSACTION  
D) SHOW TRANSACTION  
**Answer: B**

### 770. In a real-world business process, what is the key benefit of logging information in an independent scoped transaction, even if the main transaction fails?
A) Logs are lost if the main transaction fails  
B) The logging information is preserved regardless of the main transaction outcome  
C) Logging requires a manual commit  
D) Logging automatically rolls back  
**Answer: B**

### 746. Which of the following is a primary advantage of using an owner’s rights stored procedure in Snowflake?
A) The stored procedure can access the caller’s session variables  
B) The owner can delegate administrative tasks without granting broad privileges  
C) The procedure inherits the caller’s warehouse  
D) The caller can override the owner’s privileges  
**Answer: B**

### 747. In a caller’s rights stored procedure, which privileges are used to execute statements?
A) The privileges of the stored procedure’s owner  
B) The privileges of the caller’s role  
C) The privileges of the SYSADMIN role  
D) The privileges of the account administrator  
**Answer: B**

### 748. What is the default rights model when creating a stored procedure in Snowflake?
A) Caller’s rights  
B) Owner’s rights  
C) Account rights  
D) Session rights  
**Answer: B**

### 749. Which of the following is TRUE about session variables in a caller’s rights stored procedure?
A) They cannot be set or read inside the stored procedure  
B) They are isolated from the rest of the session  
C) They can be read and set, and changes persist after CALL  
D) They are automatically unset after the procedure finishes  
**Answer: C**

### 750. What happens if a user with insufficient privileges calls a caller’s rights stored procedure that attempts a restricted operation?
A) The operation succeeds due to inherited owner privileges  
B) The stored procedure fails with an error  
C) The privilege is temporarily granted  
D) The stored procedure is converted to owner’s rights at runtime  
**Answer: B**

### 751. Which command can change a procedure from owner’s rights to caller’s rights in Snowflake?
A) ALTER PROCEDURE  
B) UPDATE PROCEDURE  
C) GRANT PROCEDURE  
D) MODIFY PROCEDURE  
**Answer: A**

### 752. What can an owner’s rights stored procedure NOT do?
A) Use the warehouse of the caller  
B) Read the caller’s session variables  
C) Access the database where it was created  
D) Execute DML statements  
**Answer: B**

### 753. If a business wants to allow support staff to clean up old records without giving them DELETE on the entire table, what should the architect do?
A) Grant DELETE privilege directly to support staff  
B) Use an owner’s rights stored procedure with a restricted DELETE statement  
C) Use a caller’s rights stored procedure  
D) Use a view with DELETE capability  
**Answer: B**

### 754. Which statement about passing references to SQL objects into stored procedures is TRUE?
A) It allows caller’s rights procedures to access owner’s objects  
B) It allows owner’s rights procedures to operate with caller’s object privileges  
C) It is only supported for views  
D) It is not supported in Snowflake  
**Answer: B**

### 755. In what situation would a caller’s rights stored procedure be preferred over owner’s rights?
A) When the stored procedure must access the caller’s session variables  
B) When the caller should not see the procedure code  
C) When you want to delegate administrative tasks  
D) When the procedure must run with maximum privileges  
**Answer: A**

### 756. What happens to session-level variables set in a caller’s rights stored procedure?
A) They are visible only inside the procedure  
B) They are visible to the session after the procedure call  
C) They are automatically







 Question 1. A financial services company uses an internal named stage in Snowflake for daily transaction file ingestion. One day, the ingestion process fails, and the operations team must manually download the failed file and check for errors. The architect is tasked with improving this process to reduce manual intervention and speed up recovery. Which of the following Snowflake features would BEST address this requirement?

A) Implementing Snowpipe with automatic error notifications and leveraging the COPY_HISTORY function to identify failed files  
B) Switching to an external stage and using AWS Lambda to reprocess files  
C) Setting up a scheduled task to automatically delete failed files from the stage  
D) Using a Data Exchange listing for file recovery  

**Answer:** A

---

Question 2. During a business-critical data load event, an ingestion job fails and the file remains in the internal named stage. The operations team needs to quickly identify which rows in the file caused the failure in order to correct and reload only the problematic data. Which Snowflake feature can help the team isolate error details at a row level?

A) Re-running the entire COPY INTO command with ON_ERROR='CONTINUE'  
B) Reviewing the QUERY_HISTORY view for error codes  
C) Using the VALIDATION_MODE option with the COPY INTO command to return error rows  
D) Manually parsing the file outside of Snowflake using third-party tools  

**Answer:** C

---

Question 3. A retail company’s ingestion process uses an internal named stage. After a failure, the operations team downloads the file and checks for errors, which is time-consuming and error-prone. As the architect, you are asked to automate error handling and streamline recovery. Which architectural change would most effectively reduce manual effort while ensuring failed files are traceable for audit purposes?

A) Configure Snowflake tasks to automatically retry failed ingestions and move failed files to a designated error stage  
B) Set up a process to permanently delete all failed files after each attempt  
C) Require the operations team to email all failed files to the data engineering team  
D) Only allow ingestion during business hours to ensure staff are available for manual checks  

**Answer:** A

---

Question 4. A large retail company is using a Snowflake Business Critical edition account and wants to share sales data with a partner company that only has a Snowflake Enterprise edition account. Which of the following statements is correct regarding this scenario?

A) Data sharing is not possible between accounts of different editions.  
B) Data sharing is only possible if both accounts are on the same edition and region.  
C) Data sharing is possible from a Business Critical edition provider to an Enterprise edition consumer.  
D) Data sharing is restricted to Virtual Private Snowflake (VPS) accounts only.  

**Correct Answer:** C

---

Question 5. An architect at a financial institution with a Snowflake Business Critical edition account is asked about sharing sensitive data with a subsidiary operating on an Enterprise edition account. Which approach is feasible?

A) Data sharing cannot occur unless the subsidiary upgrades to Business Critical.  
B) The Business Critical account is permitted to share data with the Enterprise account, subject to Snowflake’s security and compliance controls.  
C) Only Secure Data Sharing is allowed between VPS and Enterprise accounts.  
D) Data sharing requires both accounts to be in the same organization.  

**Correct Answer:** B

---

Question 6. A multinational company intends to share its analytics data with multiple partners, some of which use Business Critical edition and others use Enterprise edition Snowflake accounts. What is the key consideration when architecting data sharing for this scenario?

A) Data sharing is only possible between accounts with the exact same edition.  
B) The data provider’s edition must be lower or equal to the data consumer’s edition.  
C) Data sharing is possible from higher to lower editions, such as Business Critical to Enterprise, with appropriate security and governance practices.  
D) Data sharing requires manual export and import of data between editions.  

**Correct Answer:** C

---

Question 7. The Business Intelligence team notices significant slowdowns in dashboard queries when multiple team members are running queries at the same time. As a Snowflake Architect, which of the following actions would best help you identify and troubleshoot the root cause?

A) Increase the warehouse size without analyzing query patterns.  
B) Review the Query History and Warehouse Load in the Snowflake UI to identify concurrency or resource bottlenecks.  
C) Immediately restrict user access to the dashboards.  
D) Ask users to schedule their queries at different times without further investigation.  

**Correct Answer:** B

---

Question 8. During periods of high activity, BI dashboard queries are experiencing slow response times. What is the most effective first step for a Snowflake Architect to take in order to diagnose the performance bottleneck?

A) Use Snowflake’s Query Profile to analyze individual queries and look for patterns such as queuing or resource contention.  
B) Archive all historical data in the Snowflake account.  
C) Contact Snowflake support without collecting any internal metrics.  
D) Migrate the workload to another cloud provider immediately.  

**Correct Answer:** A

---

Question 9. Multiple users experience delays when running queries for dashboards in parallel. Which approach should the Snowflake Architect recommend to ensure minimal impact on business operations while identifying the issue?

A) Monitor the performance of the virtual warehouse using Resource Monitors and consider enabling Multi-cluster Warehouses to handle concurrency.  
B) Disable dashboard access for all users until the issue is resolved.  
C) Reduce the size of the virtual warehouse to save costs.  
D) Drop and recreate all affected tables.  

**Correct Answer:** A

---

Question 10. The BI team at your company reports query slowdowns when many users run dashboards simultaneously. After investigating, you find that the virtual warehouse is frequently queuing queries during peak periods. As a Snowflake Architect, what is an appropriate solution to address this issue?

A) Reduce the size of the virtual warehouse to limit resource consumption.  
B) Enable auto-suspend on the warehouse to save costs during idle periods.  
C) Increase the number of clusters in a multi-cluster warehouse to handle more concurrent queries without queuing.  
D) Split the data into more databases to distribute the load.  

**Correct Answer:** C

---

Question 11. An Architect is analyzing a slow-running query using the `QUERY_HISTORY` function in Snowflake. They observe that the `COMPILATION_TIME` for the query is significantly greater than the `EXECUTION_TIME`. Which of the following best explains this observation?

A) The query is waiting for resources due to warehouse queuing.  
B) The query execution is delayed due to network latency between client and Snowflake.  
C) The query is complex, possibly involving dynamic SQL, extensive parsing, or large numbers of objects, which increases the time required for query compilation compared to execution.  
D) The data required for the query is stored in a remote region, causing high data transfer times.  

**Correct Answer:** C

**Explanation:**  
When `COMPILATION_TIME` is greater than `EXECUTION_TIME`, it typically means the query is complex to parse, plan, or optimize, often involving dynamic SQL, lots of objects/tables, or complicated logic. The actual execution (reading and processing data) is relatively fast, but most time is spent preparing the query.

---

Question 12. A Snowflake Architect is investigating a query that has a much higher COMPILATION_TIME than EXECUTION_TIME in the QUERY_HISTORY results. Which scenario is most likely causing this observation?

A) The warehouse was suspended and had to be resumed before the query could execute.  
B) The query references many objects, such as multiple tables or complex views, causing Snowflake to spend more time parsing and optimizing before execution.  
C) The underlying data files are heavily compressed, slowing down data retrieval.  
D) The network connection between the client and Snowflake was interrupted during execution.  

**Correct Answer:** B

---

Question 13. If you are using the `--mfa-passcode-in-password` flag with SnowSQL and the password prompt is forced (such as with `-P`), the password you enter should be a **concatenation of your Snowflake password and your current MFA token**.

Given your example:
- **Snowflake password:** SNOWFLAKE
- **MFA token:** 123456

**The password you should enter at the prompt will be:**
```
SNOWFLAKE123456
```

**Explanation:**  
You simply append the current 6-digit MFA code to the end of your normal password, with no spaces or separators. So in this case, enter `SNOWFLAKE123456` as the password when prompted.

---

Question 14. A Snowflake user’s password is `Winter2025`, and their current MFA token from their authenticator app is `654321`. When using SnowSQL with the `--mfa-passcode-in-password` flag and prompted for a password, which of the following should they enter?

A) 654321Winter2025  
B) Winter2025-654321  
C) Winter2025654321  
D) Winter2025 654321  

**Correct Answer:** C

---

Question 15. You are configuring SnowSQL for a user whose password is `SecurePass!` and who receives an MFA token of `987654`. When prompted for a password while using `--mfa-passcode-in-password`, what is the correct input?

A) SecurePass!987654  
B) 987654SecurePass!  
C) SecurePass!-987654  
D) SecurePass! 987654  

**Correct Answer:** A

---

Question 16. A company needs to securely share product catalog data from Snowflake with a partner that is not a Snowflake customer and uses Amazon S3 for storage. Both tables, PRODUCT_CATEGORY and PRODUCT_DETAILS, need to be joined and only the partner should have access. What is the most cost-effective and secure Snowflake solution for this requirement?

A) Create a secure view in Snowflake and share the credentials with the partner.  
B) Use Snowflake's external function to directly write data to the partner's S3 bucket, ensuring access is restricted.  
C) Export the joined and filtered data to a secure stage, then use Snowflake’s COPY INTO command to unload the data as files to the partner's Amazon S3 bucket with proper permissions.  
D) Give the partner VPN access to the Snowflake account and restrict queries at the network layer.  

**Correct Answer:** C

---

Question 17. The partner requires access only to the product catalog records, and data access should be strictly governed. Which Snowflake feature best enforces this requirement while exporting data to Amazon S3?

A) Use a masking policy on the PRODUCT_ID column.  
B) Implement a row access policy to filter data before unloading to S3.  
C) Create a public stage and let the partner download the files.  
D) Share the entire database with the partner through a data share.  

**Correct Answer:** B

---

Question 18. After exporting the required data from Snowflake to Amazon S3, what is the best way to ensure only the intended partner can access these files?

A) Store the files in a Snowflake internal stage with open access.  
B) Use S3 bucket policies to allow access only from the partner’s AWS account.  
C) Email the exported files directly to the partner.  
D) Make the S3 bucket public for easy access.  

**Correct Answer:** B

---

Question 19. A user connects to Snowflake and starts a new session. The user does not explicitly specify a warehouse in their query. In what order does Snowflake determine which warehouse to use for the session?

A) Role default warehouse → User default warehouse → Account default warehouse → No active warehouse  
B) User default warehouse → Role default warehouse → Account default warehouse → No active warehouse  
C) User default warehouse → Account default warehouse → Role default warehouse → No active warehouse  
D) Account default warehouse → User default warehouse → Role default warehouse → No active warehouse  

**Correct Answer:** B

**Explanation:**  
Snowflake determines the active warehouse for a session in the following hierarchy:  
1. The warehouse specified by the user in the session  
2. The user’s default warehouse  
3. The role’s default warehouse  
4. The account’s default warehouse  
5. If none are set, there is no active warehouse for the session

---

Question 20. A company needs to share its product catalog, stored in Snowflake tables PRODUCT_CATEGORY and PRODUCT_DETAILS, with a business partner who uses Amazon S3 for storage and is not a Snowflake customer. Which approach is the most cost-effective and secure for providing the data to the partner?

A) Create a Snowflake Data Share and grant access to the partner’s AWS account.  
B) Use the COPY INTO command to export the joined tables as files to a secure S3 bucket, and share access only with the partner’s AWS account.  
C) Email CSV exports of the tables to the partner.  
D) Create a public Snowflake stage and instruct the partner to download the files from there.  

**Correct Answer:** B

---

Question 21. To ensure that only the partner has access to the product catalog data exported to S3, which security measure should the Snowflake Architect prioritize?

A) Set the S3 bucket policy to allow access from any AWS user.  
B) Encrypt the exported files with a password and send the password by email.  
C) Apply an S3 bucket policy that grants read access exclusively to the partner’s AWS account.  
D) Store the files in an unencrypted S3 bucket for ease of access.  

**Correct Answer:** C

---

Question 22. The partner requires access only to specific records in the product catalog export. What is the most efficient way, using Snowflake features, to meet this requirement before unloading data to S3?

A) Use masking policies to hide data columns in the export.  
B) Filter and join the required records in a SQL query and use COPY INTO to export only the relevant data to S3.  
C) Export the entire tables and ask the partner to filter the records after downloading.  
D) Manually delete unwanted records from the S3 files after export.  

**Correct Answer:** B

---

Question 23. During a regional outage in AWS US East, the operations team fails over Snowflake workloads to a secondary account in Azure Europe West. After promoting the secondary account to primary, they notice a 30-minute data lag in analytics dashboards. What is the most likely reason for this data lag?

A) Network latency between AWS and Azure  
B) Replication schedule interval between primary and secondary accounts  
C) Insufficient compute resources in Azure Europe West  
D) Data corruption during failover  

**Correct Answer:** B

---

Question 24. After recovering from an outage and restoring the AWS US East region, which of the following is a valid post-outage step to ensure data consistency between the two Snowflake accounts?

A) Decrease the size of the compute warehouses in both regions  
B) Perform a manual replication or sync to reconcile data between the accounts  
C) Switch the replication schedule to every hour  
D) Delete the failover group in Azure Europe West  

**Correct Answer:** B

---

Question 25. What is one architectural benefit of having Snowflake accounts in both AWS US East and Azure Europe West for a global enterprise?

A) It increases licensing costs  
B) It enables regionally isolated compute resources  
C) It provides high availability and business continuity across cloud providers  
D) It complicates user access management  

**Correct Answer:** C

---

Question 26. A retail company uses Snowflake to store both structured transactional data and semi-structured JSON event data. What is the most appropriate method to maintain consistency across teams and enable efficient querying?

A) Store both data types in VARIANT columns without documentation  
B) Use structured tables for transactional data and VARIANT columns with standardized views for JSON event data  
C) Convert all JSON event data to CSV before loading into Snowflake  
D) Keep all data in separate databases for each team  

**Correct Answer:** B

---

Question 27. In designing a Snowflake data warehouse for mixed data types, which of the following actions best supports efficient querying of semi-structured event data?

A) Load JSON data as plain text in VARCHAR columns  
B) Store JSON data in VARIANT columns and create views that flatten the data for analytics  
C) Store all data in a single wide table with every possible attribute  
D) Use separate Snowflake accounts for event data and transactional data  

**Correct Answer:** B

---

Question 28. To ensure consistency and collaboration across multiple teams in a Snowflake-powered retail data warehouse, which strategy should the Architect prioritize?

A) Allow each team to define their own JSON key structures  
B) Document and standardize JSON key paths and table schemas, and provide shared views for common queries  
C) Restrict access to event data to only the data engineering team  
D) Disable semi-structured data support in Snowflake  

**Correct Answer:** B

---

Question 29. Which Snowflake feature allows a media company to continuously ingest JSON event data from cloud storage into Snowflake tables with low latency and automated file detection?

A) Snowflake Streams  
B) Snowpipe  
C) Time Travel  
D) External Tables  

**Correct Answer:** B

---

Question 30. A custom Java application streams real-time JSON event data. To achieve exactly-once delivery and handle failures gracefully, which Snowflake solution should the Architect recommend?

A) Use Snowpipe Streaming with the Snowflake Ingest SDK and offset tokens  
B) Batch load data daily using COPY INTO commands  
C) Store data in VARIANT columns and query periodically  
D) Load data via manual file uploads to the UI  

**Correct Answer:** A

---

Question 31. When implementing Snowpipe Streaming with the Snowflake Ingest SDK, what mechanism is used to resume ingestion after a failure without duplicating data?

A) File timestamps  
B) Offset tokens  
C) Retry counters  
D) Row-level security policies  

**Correct Answer:** B

---

Question 32. For a requirement of low-latency ingestion and real-time analytics, which configuration provides the most direct path from a custom application to Snowflake?

A) Java app writes to cloud storage, then Snowpipe loads files  
B) Java app uses Snowflake Ingest SDK to stream data directly into Snowflake tables  
C) Java app writes CSV files for batch ETL  
D) Java app sends data to a third-party data lake, then Snowflake external tables query the lake  

**Correct Answer:** B

---

Question 33. How does Snowflake ensure exactly-once ingestion when using Snowpipe Streaming and the Ingest SDK?

A) By using offset tokens that track the ingestion progress  
B) By requiring all files to be unique  
C) By running periodic deduplication jobs  
D) By preventing simultaneous connections  

**Correct Answer:** A

---

Question 34. Which architectural pattern best supports both failure handling and operational monitoring for a real-time Snowflake pipeline ingesting JSON event data from a web application?

A) Use Snowpipe Streaming with Ingest SDK, implement logging and alerting on ingestion errors and offsets  
B) Only rely on periodic batch loading and ignore failures  
C) Use Snowflake’s Time Travel to undo failed ingestions  
D) Store event data in external tables and query on demand  

**Correct Answer:** A

---

Question 35. A company with a Business Critical Snowflake account wants to enable disaster recovery for a sensitive customer database across regions and cloud providers. What is the first step the architect should take?

A) Encrypt the database with a master key  
B) Create a failover group including the customer database  
C) Configure a multi-cluster warehouse  
D) Set up time travel for the database  

**Correct Answer:** B

---

Question 36. After creating a failover group for a sensitive database, which step ensures the group is available in a secondary Snowflake account located in a different region and cloud provider?

A) Grant replication privileges and configure cross-region replication to the secondary account  
B) Export data to CSV files and upload to the secondary account  
C) Enable auto-scaling on the primary warehouse  
D) Use Snowflake Streams to synchronize data  

**Correct Answer:** A

---

Question 37. A healthcare company needs to restrict access to patient SSN data so that only users with the 'COMPLIANCE_OFFICER' role can view the unmasked information, while all other users see masked data. Which Snowflake context function should you use inside the masking policy to accomplish this requirement?

A) IS_ROLE_IN_SESSION  
B) CURRENT_ROLE  
C) USER_ROLE  
D) SESSION_USER  

**Correct Answer:** B

---

Question 38. An architect is designing a multi-tenant analytics solution where different clients should only be able to see their own data. The solution uses secure views and dynamic masking policies. Which context function should be used in the masking policy to ensure that the masking logic is enforced based on the role that runs the view, rather than the owner of the view?

A) INVOKER_ROLE  
B) CURRENT_ACCOUNT  
C) OBJECT_OWNER  
D) CURRENT_SESSION  

**Correct Answer:** A

---

Question 39. Your organization uses multiple roles with different privileges. You want a masking policy to check if a specific privileged role (e.g., 'DATA_AUDITOR') is active in the session, and only show unmasked data if that role is present. Which context function should the masking policy use to implement this condition?

A) CURRENT_USER  
B) CURRENT_ROLE  
C) IS_ROLE_IN_SESSION  
D) INVOKER_ROLE  

**Correct Answer:** C

---

Question 40. A retail company wants to provide its developers with access to a fresh copy of production sales data for testing new analytics features. The company needs to minimize storage costs and quickly create test environments in their Snowflake account. Which approach should the architect recommend?

A) Use CREATE TABLE ... AS SELECT (CTAS) statements to copy production data into new tables for each environment.  
B) Use zero-copy cloning into transient tables for each environment.  
C) Export production data to external files and reload into test tables.  
D) Use permanent tables for cloned environments to leverage fail-safe.

**Correct Answer:** B

---

Question 41. A financial services firm requires a pre-production environment to validate new features against real production data before release. The environment must be isolated, cost-effective, and easy to refresh. Which Snowflake feature addresses these requirements most efficiently?

A) Deep copy production tables into new schemas using CTAS and regularly refresh using ETL jobs.  
B) Use zero-copy cloning to create transient tables for pre-production validation, refreshing as needed.  
C) Move production data to a separate Snowflake account and reload for each environment.  
D) Grant all users access to the production database for testing.

**Correct Answer:** B

---

Question 42. An e-commerce business wants to ensure that its development and test environments are as close as possible to production, but without incurring unnecessary long-term storage costs. Which Snowflake table type should the architect use when cloning production data for these environments?

A) Permanent tables  
B) Transient tables  
C) Temporary tables  
D) External tables  
E) Materialized views  

**Correct Answer:** B

---

Question 43. An e-commerce company frequently receives product inventory files in different formats (CSV, JSON) from multiple vendors in their Azure Blob storage. They want to automate the process of making these files queryable in Snowflake with minimal manual intervention. Which strategy is most effective for the architect to implement?

A) Manually create a new external table for each file and format as they arrive.  
B) Configure multiple external stages and file formats, then use Snowflake's external table feature to query new files automatically as they appear in the external storage.  
C) Convert all files to a single format before uploading and use a single external table definition.  
D) Use Snowpipe to ingest all data into a raw staging table and query only the internal tables.

**Correct Answer:** B

---

Question 44. A financial analytics team has noticed that queries run on their Snowflake warehouse are frequently queued during certain periods, impacting report delivery timelines. As the Snowflake Architect, which data source would you use to identify the specific days and warehouses where enabling multi-cluster would provide the most benefit?

A) SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY  
B) SNOWFLAKE.ACCOUNT_USAGE.LOGIN_HISTORY  
C) SNOWFLAKE.ACCOUNT_USAGE.METERING_HISTORY  
D) SNOWFLAKE.INFORMATION_SCHEMA.OBJECTS  

**Correct Answer:** A

---

Question 45. An e-commerce company wants to improve performance for its busiest data science workloads. The architect needs to recommend scaling strategies based on real warehouse activity. Which metric should be analyzed to determine if multi-cluster warehouses are needed?

A) Number of distinct users logged in  
B) Average queued time for queries per warehouse per day  
C) Number of tables in the database  
D) Total amount of data stored in stages  




Question 46. A Snowflake Architect is tasked with optimizing cost and performance for several virtual warehouses. Management asks how to justify multi-cluster configuration for only a subset of warehouses and days. What is the best approach?

A) Enable multi-cluster on all warehouses regardless of usage  
B) Analyze query history to find periods of high concurrency and queuing, then target those warehouses and days  
C) Move all workloads to a single large warehouse  
D) Increase the auto-suspend timeout on warehouses to avoid queuing  

**Correct Answer:** B

---

Question 47. A pharmaceutical company needs to share sensitive research data with a partner who operates in a different cloud provider and region. What is the recommended Snowflake feature to enable secure, private sharing between the company’s AWS us-west-2 account and the partner’s Azure East US 2 account?

A) Use public data sharing via Snowflake Marketplace  
B) Use private data sharing with a reader account in the same cloud region  
C) Use cross-cloud private data sharing via a direct share to the customer’s Azure account  
D) Export data to S3 and have the customer ingest it manually  

**Correct Answer:** C

---

Question 48. During a cross-cloud private data sharing setup, which key step must the data provider complete to ensure the consumer in Azure East US 2 can access the shared data from their AWS us-west-2 Snowflake account?

A) Create a share and invite the consumer’s account locator in the Azure region  
B) Grant SELECT privileges directly to the consumer’s user  
C) Move the provider’s Snowflake account to Azure  
D) Use Snowpipe to stream the data to Azure Blob Storage  

**Correct Answer:** A

---

Question 49. An architect is asked to outline the correct sequence of actions for enabling cross-cloud private data sharing between two Snowflake accounts. Which sequence should they recommend?

A) Create a share → Grant table privileges → Invite consumer using their account locator → Consumer creates a database from the share  
B) Export data to CSV → Upload to Azure Blob Storage → Consumer loads data into Snowflake  
C) Grant privileges to the consumer’s Snowflake user → Consumer queries the provider’s tables  
D) Create a reader account in the provider’s region → Consumer accesses shared data via the reader account  

**Correct Answer:** A

---

Question 50. Company B maintains their sales data in on-premises databases, while company A’s Snowflake account is hosted in AWS us-east-1. What is the most efficient way for the architect to consolidate company B’s sales data into company A’s Snowflake environment for ongoing integration?

A) Use Snowflake’s data sharing feature between accounts  
B) Export data from Company B’s database as files, upload to an S3 bucket in us-east-1, and use an external stage with COPY INTO to load data into Snowflake  
C) Connect directly to company B’s database from Snowflake using native connectors  
D) Use a Snowflake reader account in company B’s environment  

**Correct Answer:** B

---

Question 51. Company B has their sales data in their own Snowflake account, but it is hosted in a different region and cloud provider than company A’s Snowflake account in AWS us-east-1. Which Snowflake capability should the architect use to consolidate sales data into company A’s account with minimal data movement and security risk?

A) Snowflake Secure Data Sharing  
B) Manual data export and import via CSV files  
C) Replicate the entire Snowflake account to AWS us-east-1  
D) Use Snowpipe to stream data across regions  

**Correct Answer:** A

---

Question 52. The architect wants to automate the consolidation process so that company B’s latest sales data is regularly ingested into company A’s Snowflake account in AWS us-east-1, with minimal manual effort. Which solution is best suited for this requirement?

A) Use an ETL tool (such as Fivetran or Informatica) to schedule incremental loads from company B’s source system to company A’s Snowflake account  
B) Require company B to email CSV files daily for manual upload  
C) Manually run COPY INTO commands each week  
D) Use a physical data courier to deliver external drives with data  

**Correct Answer:** A

---

Question 53. A multinational company operates separate Snowflake accounts for development and production in different geographic regions. The development team needs to work with production data. Why would copying the data be required in this scenario, rather than using zero-copy cloning?

A) Zero-copy cloning does not support copying data across different accounts or regions.  
B) Zero-copy cloning always creates a physical copy of the data by default.  
C) Zero-copy cloning automatically masks the data for security compliance.  
D) Zero-copy cloning encrypts the data during transfer between accounts.  

**Correct Answer:** A

---

Question 54. An architect is tasked with creating a testing environment that requires only a subset of production data, with sensitive information masked for compliance reasons. Why is zero-copy cloning not suitable for this scenario?

A) Zero-copy cloning is only available for database administrators.  
B) Zero-copy cloning does not allow for data transformation or masking during the cloning process.  
C) Zero-copy cloning automatically anonymizes all data.  
D) Zero-copy cloning is only supported for temporary tables.  

**Correct Answer:** B

---

Question 55. In which situation would copying data be preferred over zero-copy cloning when setting up a persistent test environment that should remain accessible even after the source production data is deleted or altered?

A) Zero-copy clones are independent of the source and persist even after the original table is dropped.  
B) Zero-copy cloning automatically refreshes the test environment with new production data.  
C) Copying data creates a fully independent copy that remains accessible regardless of changes to the source.  
D) Zero-copy cloning supports cross-cloud transfers without additional configuration.  

**Correct Answer:** C

---

Question 56. After cloning a production database in Snowflake to create a test environment, the architect notices that scheduled tasks in the cloned database are not executing. What is the most likely reason for this behavior?

A) Tasks in cloned databases are automatically enabled and should run without intervention.  
B) Cloned tasks are suspended by default to prevent unintended execution in the new environment.  
C) The clone does not include any tasks from the original database.  
D) Tasks require manual re-creation after cloning a database.  

**Correct Answer:** B

---

Question 57. What action must a Snowflake architect take to resume scheduled task execution after cloning a database and its objects?

A) Nothing; tasks will automatically resume once the clone is created.  
B) Use the ALTER TASK <task_name> RESUME command to enable tasks in the cloned database.  
C) Re-create all tasks in the cloned environment from scratch.  
D) Enable automatic scheduling in the Snowflake account settings.  

**Correct Answer:** B

---

Question 58. After implementing Okta SSO with MFA for Snowflake, the Data Analyst team using DBeaver reports frequent credential prompts that disrupt workflow. What is the best way for the architect to reduce these prompts and improve analyst productivity?

A) Switch DBeaver’s authentication method to OAuth and configure Okta as the identity provider for session reuse.  
B) Disable MFA in Okta for all analyst accounts.  
C) Increase the frequency of password resets for analyst users.  
D) Require analysts to use the Snowflake web UI instead of DBeaver.  

**Correct Answer:** A

---

Question 59. The data analysts are experiencing frequent sign-in prompts in DBeaver after SSO/MFA was enabled in Snowflake. Which configuration should the architect recommend to minimize these interruptions without compromising security?

A) Configure DBeaver to use OAuth authentication for Snowflake, allowing session persistence and SSO token reuse.  
B) Remove SSO and MFA requirements from Snowflake.  
C) Ask analysts to save their passwords in a spreadsheet for convenience.  
D) Set up a shared analyst account with no authentication.  

**Correct Answer:** A

---

Question 60. USER_01 has been assigned to build a materialized view in the schema EDW.STG_SCHEMA. Which privilege must the architect grant on the schema to allow USER_01 to create a materialized view?

A) CREATE VIEW  
B) CREATE MATERIALIZED VIEW  
C) OWNERSHIP  
D) USAGE  

**Correct Answer:** B

---

Question 61. After granting USER_01 the CREATE MATERIALIZED VIEW privilege on EDW.STG_SCHEMA, USER_01 still cannot create a materialized view referencing tables in the schema. What additional privilege must be granted to USER_01 to resolve this issue?

A) DELETE on the referenced tables  
B) OWNERSHIP on the referenced tables  
C) SELECT on the referenced tables  
D) INSERT on the referenced tables  

**Correct Answer:** C

---

Question 62. An architect wants to ensure USER_01 can create and query materialized views in EDW.STG_SCHEMA but does not want to grant excessive permissions. Which combination of privileges should be granted to satisfy both requirements securely?

A) USAGE on the schema and CREATE MATERIALIZED VIEW on the schema  
B) CREATE MATERIALIZED VIEW on the schema and OWNERSHIP on all tables  
C) USAGE on the schema and SELECT on all tables in the schema  
D) USAGE on the schema, CREATE MATERIALIZED VIEW on the schema, and SELECT on the referenced tables  

**Correct Answer:** D

---

Question 63. A retail analytics team complains about slow query performance when accessing external tables in Snowflake that reference data in an AWS S3 data lake. What is the most effective way for the architect to improve performance for frequently accessed queries?

A) Create materialized views on the external tables  
B) Increase the size of the Snowflake virtual warehouse  
C) Move the external data to Snowflake internal tables  
D) Enable automatic clustering on the external tables  

**Correct Answer:** C

---

Question 64. Users report slow performance when querying external tables that reference thousands of small files in cloud storage. What action should the architect take to optimize query speed?

A) Partition the files by commonly queried columns  
B) Combine smaller files into larger files to reduce metadata overhead  
C) Use uncompressed CSV format for all files  
D) Increase the number of external tables  

**Correct Answer:** B

---

Question 65. The architect observes that queries on external tables are not efficiently scanning only necessary data. Which optimization can help Snowflake skip scanning unnecessary files and improve query performance?

A) Use partitioned file structures in cloud storage and write queries using partition columns in WHERE clauses  
B) Set the file format to JSON for better compatibility  
C) Disable caching for external tables  
D) Drop and recreate the external tables daily  

**Correct Answer:** A

---

Question 66. The Operations team must manually download failed files from an internal named stage for error analysis after an ingestion failure. Which method should the Architect recommend to automate and reduce operational overhead for file retrieval?

A) Instruct the team to manually copy files using the Snowflake web UI  
B) Use the Snowflake GET command to programmatically download files from the internal stage  
C) Request files from the stage by emailing the Snowflake administrator  
D) Manually transfer files using FTP from the Snowflake stage  

**Correct Answer:** B

---

Question 67. When designing a recovery solution for file ingestion failures, which approach allows the Operations team to most efficiently and consistently retrieve failed files for inspection from an internal stage?

A) Automate file download using the GET command within a script or scheduled task  
B) Assign more staff to manually handle downloads during peak failure times  
C) Require users to access the stage via external cloud storage tools  
D) Use manual downloads from the Snowflake UI for each failed file  

**Correct Answer:** A

---

Question 68. A healthcare company operating on Snowflake Business Critical edition wants to securely share data with a partner using an Enterprise edition account. Which statement accurately describes this capability?

A) Data sharing is only supported between accounts on the same edition  
B) Secure data sharing is supported between Business Critical and Enterprise edition accounts, subject to compliance and security controls  
C) Data sharing is not possible outside the Business Critical edition  
D) Data sharing requires both accounts to be upgraded to Enterprise edition  

**Correct Answer:** B

---

Question 69. A Business Critical edition provider shares data with an Enterprise edition consumer. What important limitation should the architect consider?

A) The consumer cannot access any shared data  
B) The consumer may not have access to certain Business Critical-only features, such as Tri-Secret Secure  
C) The provider must downgrade their edition before sharing  
D) Data sharing is only possible in the same region  

**Correct Answer:** B

---

Question 70. A Business Critical data provider wants to share sensitive data with an Enterprise edition consumer but has enabled account-level restriction (override restriction) for sharing. What effect does this have?

A) The override restriction prevents the provider from sharing data with accounts on lower editions unless explicitly permitted  
B) The Enterprise consumer can always access the shared data, regardless of restrictions  
C) The override restriction only applies to metadata, not actual data  
D) Override restrictions are ignored during secure data sharing  

**Correct Answer:** A

---

Question 71. An Architect notices that the COMPILATION_TIME of a query in QUERY_HISTORY is significantly higher than its EXECUTION_TIME. What is the most likely reason for this observation?

A) The query executed against a very large dataset  
B) The query involves complex logic, joins, or subqueries that require extensive optimization during compilation  
C) The query was run during peak hours with heavy warehouse load  
D) The query was suspended during execution by the administrator  

**Correct Answer:** B

---

Question 72. When examining QUERY_HISTORY, an Architect sees several queries with higher COMPILATION_TIME than EXECUTION_TIME. Which scenario could result in quick execution but lengthy compilation?

A) The queries reference simple tables with no indexes  
B) The queries are accessing tables with very few rows  
C) The queries have complex structures but ultimately filter down to a small result set  
D) The queries use only SELECT * FROM table statements  

**Correct Answer:** C

---

Question 73. What is a possible operational cause for consistently high COMPILATION_TIME compared to EXECUTION_TIME in a Snowflake environment?

A) Frequent changes to schema objects, such as table structures or view definitions, preventing cache reuse during query compilation  
B) The virtual warehouse is undersized and cannot process data quickly  
C) Users are running queries in the Snowflake web UI instead of a BI tool  
D) Data is stored in an unsupported file format  

**Correct Answer:** A

---

Question 74. An analyst is required to use MFA when connecting to Snowflake via SnowSQL. If the password is `SNOWFLAKE` and the current MFA token is `123456`, what should the analyst enter at the password prompt when using `--mfa-passcode-in-password`?

A) SNOWFLAKE:123456  
B) SNOWFLAKE123456  
C) 123456SNOWFLAKE  
D) SNOWFLAKE 123456  

**Correct Answer:** B

---

Question 75. During a security audit, the architect notices that users are connecting to Snowflake using SnowSQL and MFA. What is the correct method for including the MFA token when prompted for a password with the `--mfa-passcode-in-password` flag?

A) Enter the MFA token only  
B) Enter the password, then the MFA token, separated by a space  
C) Concatenate the password and MFA token with no spaces or separators  
D) Use a comma to separate the password and MFA token  

**Correct Answer:** C

---

Question 76. Which of the following is a security best practice when using the `--mfa-passcode-in-password` option in SnowSQL for MFA-enabled accounts?

A) Always separate the password and token by a colon  
B) Enter the password first, then the MFA token, without any spaces or special characters  
C) Share the combined password and token with colleagues to simplify login  
D) Store the combined password and token in a plaintext file for convenience  

**Correct Answer:** B

---

Question 77. During a CI/CD deployment, an architect attempts to clone a production table into the development environment, but the operation fails. Upon investigation, the architect finds that the table’s data retention time in production was set to a very low value. Which of the following best explains why the cloning operation failed?

A) The low retention time caused some historical data to be unavailable for cloning.  
B) The development environment does not support cloning operations.  
C) The table was encrypted in production.  
D) The table’s schema was incompatible with development.  

**Correct Answer:** A

---

Question 78. In a CI/CD workflow, an architect is unable to clone a table from production to development. After checking permissions, the architect notices that users in the development environment do not have the necessary privileges to access the production table. What step should the architect take to resolve this issue?

A) Increase the table’s data retention period in production.  
B) Grant the appropriate privileges on the production table to the development environment’s users.  
C) Change the table’s storage format.  
D) Deactivate CI/CD automation.  

**Correct Answer:** B

---

Question 79. While cloning a table from production to development as part of a CI/CD process, an architect encounters a failure. The architect discovers that the production table uses features not supported in the development environment, such as certain clustering keys. What is the best solution to enable successful cloning?

A) Remove the unsupported features from the production table before cloning.  
B) Increase the development environment’s compute resources.  
C) Set a longer retention time on the production table.  
D) Use a different CI/CD tool.  

**Correct Answer:** A

---

Question 80. A company needs to share its product catalog (stored in PRODUCT_CATEGORY and PRODUCT_DETAILS tables) with a partner who is not a Snowflake customer and uses Amazon S3 for cloud storage. The company wants to ensure only the partner has access and manages data securely and cost-effectively. Which Snowflake feature should the architect recommend for this scenario?

A) Use Snowflake Data Sharing to create a secure share for the partner.  
B) Export the data to Parquet format and use Snowflake’s External Functions to upload directly to the partner’s S3 bucket.  
C) Use Snowflake’s COPY INTO command to export the data to Amazon S3 and control access via S3 permissions.  
D) Grant the partner direct read access to the Snowflake tables using a Snowflake Reader account.  

**Correct Answer:** C

---

Question 81. The company must ensure that only the intended partner can access the exported product catalog data on Amazon S3, and that access is auditable. Which of the following actions should the architect implement as part of the solution?

A) Set up S3 bucket policies restricting access to the partner’s AWS account.  
B) Share the S3 bucket credentials with the partner via email.  
C) Make the S3 bucket public for easy access.  
D) Use Snowflake’s secure views to mask sensitive data before export.  

**Correct Answer:** A

---

Question 82. In order to minimize costs and automate the data sharing process between Snowflake and the partner’s Amazon S3, which solution should the architect implement?

A) Schedule a Snowflake task to periodically run a COPY INTO statement, exporting only updated records to S3.  
B) Manually download query results and upload them to the S3 bucket.  
C) Use Snowflake Streams to replicate data directly to the partner’s AWS account.  
D) Enable continuous data sharing using Snowflake’s secure data exchange marketplace.  

**Correct Answer:** A

---

Question 83. A data engineer logs in to Snowflake and starts a session without specifying a warehouse in their connection string. The account has a default warehouse set, and the engineer’s user profile also has a default warehouse. Which warehouse will Snowflake assign to the session by default?

A) The warehouse specified in the connection string  
B) The default warehouse set for the user profile  
C) The warehouse last used in the previous session  
D) The account-level default warehouse  

**Correct Answer:** B

---

Question 84. During a troubleshooting scenario, an architect is asked why a particular session used a specific warehouse even though the user did not specify one explicitly and their user profile does not have a default warehouse. Which warehouse will be used for the session?

A) The warehouse specified in the user's previous query  
B) No warehouse will be assigned and queries will fail  
C) The default warehouse set at the account level  
D) The warehouse with the highest compute resources  

**Correct Answer:** C

---

Question 85. A BI application connects to Snowflake and specifies a warehouse in its connection string, even though the user’s profile and account both have default warehouses set. Which warehouse will be active for the session?

A) The warehouse specified in the connection string  
B) The default warehouse for the account  
C) The default warehouse for the user profile  
D) The warehouse last assigned by the administrator  

**Correct Answer:** A

---

Question 86. A multinational company is migrating its analytics platform to Snowflake. The architect wants to ensure that only specific users can access sensitive financial data, while other team members have broader access to sales data. Which feature of Role-Based Access Control (RBAC) helps the architect achieve this selective access?

A) RBAC allows roles to be assigned to users and privileges to be granted to roles, enabling fine-grained access control.  
B) RBAC automatically grants all users full access to all data.  
C) RBAC enforces access management only at the account level.  
D) RBAC requires manual permission assignment for every individual user and object.  

**Correct Answer:** A

---

Question 87. A Snowflake Architect is designing an environment where data scientists should be able to query but not modify production tables. Which characteristic of RBAC would best support this requirement?

A) RBAC allows for row-level security policies.  
B) RBAC enables the assignment of read-only roles to users, restricting their ability to modify data.  
C) RBAC prevents users from accessing any data unless they are administrators.  
D) RBAC supports automatic privilege escalation for queries.  

**Correct Answer:** B

---

Question 88. A project manager asks the Snowflake Architect how permissions can be efficiently managed across hundreds of users and objects. What is a key advantage of RBAC for this scenario?

A) RBAC enables centralized privilege management by granting permissions to roles rather than to individual users.  
B) RBAC requires privileges to be granted directly to each user for every object.  
C) RBAC only supports static privilege assignments.  
D) RBAC restricts privilege management to database administrators only.  

**Correct Answer:** A

---

Question 89. An architect is reviewing the security configuration of a Snowflake account and finds that users are sometimes assigned multiple roles. What is a result of this RBAC characteristic?

A) Users can switch between roles in a session to access different sets of privileges, based on their current role.  
B) Users can only use privileges from their default role.  
C) Users lose access to all data if multiple roles are assigned.  
D) Users are forced to use all roles simultaneously.  

**Correct Answer:** A

---

Question 90. During an audit, the compliance team asks how Snowflake ensures that only authorized users can perform sensitive operations like creating or dropping tables. Which RBAC characteristic helps address this concern?

A) RBAC restricts sensitive operations by only allowing roles with specific privileges to execute them.  
B) RBAC allows all users to perform any operation by default.  
C) RBAC requires external IAM integration for all operations.  
D) RBAC does not support object-level privileges.  

**Correct Answer:** A

---

Question 91. A retail company has created a materialized view to accelerate frequent sales summary queries. After deployment, the analytics team notices that some queries matching the materialized view’s definition are not using the materialized view for execution. What could explain this behavior?

A) Snowflake only rewrites queries to use a materialized view if the base tables have not changed since the last refresh.  
B) Snowflake does not guarantee that every matching query will be dynamically rewritten to use the materialized view due to factors such as query structure, filters, or join order.  
C) The materialized view is automatically used for all queries that match its definition, regardless of other conditions.  
D) The materialized view must be manually referenced in every query to be utilized.  

**Correct Answer:** B

---

Question 92. An architect optimizes a dashboard by creating materialized views for the most common queries. However, users observe that performance improvements are inconsistent and sometimes queries do not benefit from the materialized views. Which scenario best explains why Snowflake might not rewrite certain queries to use a materialized view?

A) The queries contain additional columns not present in the materialized view.  
B) The queries are executed by users without sufficient privileges.  
C) Snowflake only rewrites queries to use materialized views if the query matches the definition and other rewrite conditions are satisfied.  
D) The virtual warehouse size is too small.  

**Correct Answer:** C

---

Question 93. After creating a materialized view, a Snowflake Architect runs a query that is structurally identical to the materialized view’s definition. Surprisingly, the query does not use the materialized view. Which of the following is a valid reason for this outcome?

A) The materialized view is still refreshing and not yet available for query rewriting.  
B) Snowflake always rewrites the query to use the materialized view if the definitions match.  
C) The query must include a USE MATERIALIZED VIEW clause.  
D) Only queries run by account administrators can use materialized views.  

**Correct Answer:** A

---

Question 94. A financial analyst is running a series of SQL statements to update customer balances in Snowflake. The analyst wants to ensure that all statements either complete successfully together or none are applied if an error occurs. Which characteristic of transactions in Snowflake enables this business requirement?

A) Transactions in Snowflake automatically commit each individual statement.  
B) Transactions in Snowflake support atomicity, allowing multiple statements to be committed or rolled back as a single unit.  
C) Transactions in Snowflake are limited to read-only operations.  
D) Transactions require manual log file management for rollback.  

**Correct Answer:** B

---

Question 95. A retail company’s ETL process inserts and updates data in several tables during nightly loads. The architect wants to ensure that, in case of a failure, no partial changes are made to the database. Which transaction behavior in Snowflake helps achieve this?

A) Snowflake automatically saves all changes, even if an error occurs.  
B) Snowflake supports the use of explicit COMMIT and ROLLBACK statements to control transaction boundaries and undo partial changes.  
C) Snowflake only supports implicit commits with no rollback capability.  
D) Transactions in Snowflake are only available for SELECT queries.  

**Correct Answer:** B

---

Question 96. A development team needs to coordinate updates to multiple related tables to ensure data consistency. During a deployment, they encounter a deadlock when two sessions try to update the same set of rows. What does this reveal about Snowflake’s transaction management?

A) Snowflake does not support concurrent transactions.  
B) Snowflake uses optimistic concurrency control, and deadlocks are possible when transactions contend for the same data.  
C) Snowflake automatically serializes all transactions to prevent conflicts.  
D) Transactions in Snowflake do not support updates to multiple tables.  

**Correct Answer:** B

Question 97. A logistics company is experiencing delays when collecting telemetry data from its fleet. The architect proposes Kafka. Which feature of Kafka is most relevant to solving this business problem?

A) Kafka’s ability to batch data for weekly reports  
B) Kafka’s support for distributed, fault-tolerant streaming  
C) Kafka’s built-in machine learning models  
D) Kafka’s data visualization capabilities  
E) Kafka’s user authentication system  

**Correct Answer:** B

---

Question 98. A media company wants to capture, process, and distribute live event data to multiple analytics systems simultaneously. Why would an architect recommend Kafka?

A) Kafka integrates with Excel for manual analysis  
B) Kafka allows simultaneous, scalable data distribution to multiple consumers  
C) Kafka replaces relational databases for historical storage  
D) Kafka performs automatic data masking  
E) Kafka is a cloud-only service  

**Correct Answer:** B

---

Question 99. A media company stores large volumes of JSON log files from various sources and needs to efficiently ingest and analyze this data in Snowflake. What is the MOST efficient technique to support these semi-structured workloads?

A) Load data using the COPY INTO command with the VARIANT column type  
B) Convert JSON files to CSV before loading  
C) Use Snowflake’s secure views to analyze the data  
D) Store files in external tables only  
E) Use clustering keys to organize the data  

**Correct Answer:** A

---

Question 100. An e-commerce business regularly receives semi-structured data in Avro and Parquet formats from its partners. The architect must enable scalable analytics on this data in Snowflake. Which ingestion approach is best suited to this use case?

A) Use Snowflake’s automatic clustering feature  
B) Load Avro and Parquet files into a VARIANT column using COPY INTO  
C) Convert all files to XML before loading  
D) Apply masking policies to every file  
E) Upload files manually using the web UI  

**Correct Answer:** B

---

Question 101. A logistics firm wants to analyze real-time sensor data sent as semi-structured JSON to their Snowflake data lake. Which technique will best enable fast ingestion and flexible querying of this data?

A) Use Snowpipe to stream JSON files into Snowflake with VARIANT columns  
B) Insert each record manually via a worksheet  
C) Convert the JSON to flat tables before loading  
D) Schedule batch jobs with the COPY INTO command for CSV files  
E) Export the data to a third-party BI tool before ingesting  

**Correct Answer:** A

---

Question 102. A media company ingests large volumes of semi-structured data from multiple sources into their Snowflake data lake. The data structures often vary and evolve over time. What is the primary advantage of using a schema-on-read approach in this environment?

A) It enforces strict column definitions before data ingestion  
B) It allows flexible querying and interpretation of data structure at query time  
C) It requires converting all incoming data into flat tables  
D) It limits the data types that can be ingested  
E) It automatically indexes all columns  

**Correct Answer:** B

---

Question 103. An architect is considering how to handle frequent changes in JSON and Avro files from external partners. Which statement best describes the schema-on-read technique in Snowflake?

A) Schema-on-read means the data’s structure is defined before ingestion  
B) Schema-on-read requires users to manually update table definitions for each change  
C) Schema-on-read enables Snowflake to infer the structure of semi-structured data when queries are run  
D) Schema-on-read forces users to convert data to CSV format  
E) Schema-on-read disables time travel features  

**Correct Answer:** C

---

Question 104. A healthcare organization needs to analyze patient device logs that arrive in varying formats and structures. How does schema-on-read help the architect deliver a solution in Snowflake that is both scalable and adaptive?

A) It requires all logs to be standardized before loading  
B) It allows storing and querying diverse log formats in a VARIANT column without prior schema definition  
C) It restricts loading to only structured tables  
D) It automatically transforms all logs into a fixed schema  
E) It blocks ingestion of files with unexpected fields  

**Correct Answer:** B

---

Question 105. A retail data provider shares five tables with a partner using Snowflake’s data sharing feature. The consumer account’s role has been granted the imported privileges privilege. What does this enable the consumer role to do?

A) Create new tables in the provider account  
B) Grant privileges on shared objects to other roles within the consumer account  
C) Modify the definition of shared tables  
D) Delete shared tables from the provider account  
E) Access provider’s account usage views  

**Correct Answer:** B

---

Question 106. After granting the imported privileges privilege to a role in the consumer account, which scenario demonstrates how this privilege is typically used in business workflows?

A) The consumer role can update records in the shared tables  
B) The consumer role can grant SELECT access on shared tables to additional users in their organization  
C) The consumer role can change the structure of the shared tables  
D) The consumer role can revoke access from the provider account  
E) The consumer role can load new data into the provider’s tables  

**Correct Answer:** B

---

Question 107. A financial institution, acting as a Snowflake data consumer, receives shared tables from a provider account and the role has imported privileges. Which limitation still applies to the consumer’s access to these shared tables?

A) The consumer cannot grant privileges to other roles  
B) The consumer cannot create views on shared tables  
C) The consumer cannot modify the data or schema in shared tables  
D) The consumer can delete shared tables from the provider account  
E) The consumer can move shared tables to another database  

**Correct Answer:** C

---

Question 108. A SaaS provider wants to offer its analytics platform to multiple clients using a single Snowflake account, while keeping each client’s data isolated and secure. Which architectural concept does this scenario illustrate?

A) Single-tenant architecture  
B) Multi-tenant architecture  
C) Hybrid deployment  
D) On-premises installation  
E) Peer-to-peer sharing  

**Correct Answer:** B

---

Question 109. In a multi-tenant Snowflake environment, what is a key consideration for an architect when designing data models?

A) Combining all tenants’ data in one table without distinction  
B) Ensuring data isolation and security between tenants  
C) Limiting the number of tenants to one per account  
D) Disabling resource monitors for all tenants  
E) Sharing credentials among all tenants  

**Correct Answer:** B

---

Question 110. A software company supports several customers on a shared Snowflake infrastructure. What is an advantage of using a multi-tenant setup for this business model?

A) Each customer must have a separate account  
B) Increased operational efficiency and cost savings through shared resources  
C) Customers can access each other’s data directly  
D) Query performance is always identical for all tenants  
E) No need to manage roles and privileges  

**Correct Answer:** B

---

Question 111. A global retail company is experiencing slow query performance on its large sales transaction table due to frequent range scans on the "sales_date" column. What clustering strategy should the architect recommend to optimize these queries?

A) Cluster the table on the "sales_date" column  
B) Use automatic clustering on unrelated columns  
C) Partition the table by region  
D) Increase the virtual warehouse size  
E) Create materialized views on the table  

**Correct Answer:** A

---

Question 112. An architect is tasked with improving query efficiency for a table with billions of rows. The business frequently filters by "customer_id" and "region". What is an effective clustering strategy?

A) Cluster the table by both "customer_id" and "region" columns  
B) Cluster the table by the primary key only  
C) Do not use clustering for large tables  
D) Create a separate table for each region  
E) Use the COPY INTO command to reload the table daily  

**Correct Answer:** A

---

Question 113. A logistics company notices that their queries on shipment data are becoming slower over time, even after increasing compute resources. The architect suspects poor clustering. Which step should be taken to maintain optimal clustering over time?

A) Enable automatic clustering on the table  
B) Rebuild the table from scratch every week  
C) Drop and recreate all indexes  
D) Disable clustering  
E) Archive old shipments to external storage  

**Correct Answer:** A

---

Question 114. A retail analytics team notices that queries on their large transactions table are slowing down. The architect recommends using SYSTEM$CLUSTERING_INFORMATION. What is the primary purpose of this function in Snowflake?

A) To analyze the clustering depth and effectiveness of a table  
B) To create materialized views for faster queries  
C) To encrypt the table data  
D) To monitor warehouse usage  
E) To archive table partitions  

**Correct Answer:** A

---

Question 115. An architect is troubleshooting query performance issues on a heavily clustered table and wants to measure clustering quality. Which result from SYSTEM$CLUSTERING_INFORMATION would indicate the need for reclustering?

A) High clustering depth and large average values for clustering metrics  
B) Low number of partitions  
C) Frequent auto-suspend events  
D) Large warehouse size  
E) High percentage of NULL values  

**Correct Answer:** A

---

Question 116. A logistics company is using SYSTEM$CLUSTERING_INFORMATION to monitor their shipment data table. What business value does this approach provide?

A) Enables the team to proactively maintain clustering, optimizing query performance  
B) Automatically creates new tables for each month  
C) Ensures data is encrypted in transit  
D) Notifies users of schema changes  
E) Schedules backups of the database  

**Correct Answer:** A

---

Question 117. A retail company stores sales data in AWS S3 and wants to automate ingestion into their Snowflake account hosted on GCP. Which solution can trigger the ingestion process directly from AWS when new files land?

A) Use AWS Lambda to invoke the Snowpipe REST endpoint  
B) Manually upload files using Snowflake’s web interface  
C) Schedule batch jobs in GCP Dataflow  
D) Export files to local disk before loading  
E) Use Google Cloud Functions without integration  

**Correct Answer:** A

---

Question 118. An Architect is designing a cross-cloud ingestion workflow from AWS S3 to Snowflake on GCP. What is the role of the Snowpipe REST endpoint in this architecture?

A) It provides a programmatic interface for triggering file ingestion into Snowflake  
B) It processes files locally on AWS before transfer  
C) It encrypts the files for transit  
D) It generates data masking policies  
E) It manages network routing between AWS and GCP  

**Correct Answer:** A

---

Question 119. A media company wants to automate file ingestion from AWS S3 to Snowflake on GCP whenever new files are uploaded. Which AWS-native component can detect new files and trigger the ingestion?

A) AWS Lambda function  
B) AWS EC2 instance  
C) AWS Glue crawler  
D) AWS Redshift stream  
E) AWS IAM role  

**Correct Answer:** A

---

Question 120. A financial services architect must ensure immediate ingestion of compliance reports from AWS S3 into Snowflake hosted on GCP. Which approach will provide near real-time automation?

A) Use AWS Lambda to call the Snowpipe REST endpoint when new files arrive  
B) Manually monitor the S3 bucket and run COPY INTO in Snowflake  
C) Schedule daily transfer jobs in Google Cloud Composer  
D) Email files to the Snowflake admin for loading  
E) Use a third-party ETL tool with weekly loads  

**Correct Answer:** A

---

Question 121. The company’s architect wants to minimize manual steps and automate file ingestion from AWS S3 to Snowflake on GCP. What is the most scalable solution for this cross-cloud integration?

A) Configure an AWS Lambda function to invoke the Snowpipe REST endpoint for each new file  
B) Require staff to manually upload files through the Snowflake UI  
C) Use Google Cloud Storage Transfer Service for all files  
D) Download files to a local server and push to Snowflake  
E) Schedule a monthly bulk load job in Snowflake  

**Correct Answer:** A

---

Question 122. An architect is evaluating options for ingesting files from AWS S3 into Snowflake on GCP. What is a key benefit of using the Snowpipe REST endpoint in this scenario?

A) Enables automated, event-driven ingestion from external cloud storage  
B) Restricts ingestion to only CSV file formats  
C) Requires manual polling of the S3 bucket  
D) Prevents ingestion from multiple cloud sources  
E) Transfers files only during off-peak hours  

**Correct Answer:** A

---

Question 123. A retail company wants to automate the ingestion of new sales data files from AWS S3 into Snowflake. Which AWS service could be used to detect new files and trigger the Snowpipe REST endpoint for ingestion?

A) AWS Lambda function  
B) Amazon EC2 instance  
C) Amazon Redshift  
D) AWS IAM role  
E) Amazon DynamoDB  

**Correct Answer:** A

---

Question 124. During a cloud architecture review, an architect recommends using Lambda functions to process and route data between different cloud services. What is a key advantage of using Lambda functions in this scenario?

A) Lambda functions enable serverless, event-driven execution without manual server management  
B) Lambda functions require dedicated servers for each event  
C) Lambda functions only support batch processing  
D) Lambda functions must be scheduled manually  
E) Lambda functions encrypt data at rest by default  

**Correct Answer:** A

---

Question 125. A logistics company wants to perform real-time transformations on streaming data before loading it into Snowflake. How can Lambda functions assist in this business workflow?

A) By executing custom code automatically in response to data events  
B) By creating materialized views in Snowflake  
C) By storing raw data in Amazon Glacier  
D) By disabling event notifications  
E) By monitoring warehouse usage in Snowflake  

**Correct Answer:** A

---

Question 126. A data engineer needs to ensure that a Snowpark stored procedure has access to maximum memory and compute resources for a complex data transformation. Which SQL command should the architect use on the `snowpark_opt_wh` warehouse?

A) `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';`  
B) `ALTER PROCEDURE my_proc SET MEMORY = 'MAX';`  
C) `ALTER SCHEMA SET COMPUTE_OPTIMIZED = TRUE;`  
D) `ALTER WAREHOUSE snowpark_opt_wh SUSPEND;`  
E) `ALTER USER SET RESOURCE_MONITOR = 'HIGH';`  

**Correct Answer:** A

---

Question 127. What is Snowpark in the context of Snowflake?

A) A developer framework for building data pipelines and applications using familiar programming languages directly in Snowflake  
B) A built-in tool for resizing warehouses  
C) A visualization dashboard for BI users  
D) A storage management feature for external stages  
E) A Snowflake billing optimization service  

**Correct Answer:** A

---

Question 128. A financial services company wants to run complex machine learning scoring logic directly inside Snowflake. What Snowflake feature should the architect recommend for this requirement?

A) Snowpark, to run custom logic and ML models within Snowflake using Java, Scala, or Python  
B) Secure Data Sharing  
C) Materialized Views  
D) Resource Monitors  
E) File Format objects  

**Correct Answer:** A

---

Question 129. An architect wants to optimize the performance of Snowpark stored procedures that run on the `snowpark_opt_wh` warehouse. Which configuration will best support high concurrency and intensive workloads?

A) Increase both the warehouse size using `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';` and set `MAX_CONCURRENCY_LEVEL` appropriately  
B) Set `MAX_CONCURRENCY_LEVEL = 1` to maximize memory for one query  
C) Use the smallest warehouse size for cost savings  
D) Suspend the warehouse during peak times  
E) Disable automatic clustering  

**Correct Answer:** A

---

Question 130. A retail company uses Snowpark for ETL transformations. What is one major benefit of using Snowpark over traditional SQL-based pipelines?

A) Snowpark allows developers to use familiar languages (Python, Java, Scala) and apply advanced logic not easily expressed in SQL  
B) Snowpark only supports visualization tasks  
C) Snowpark disables time travel for all tables  
D) Snowpark must be run outside Snowflake  
E) Snowpark restricts access to structured data only  

**Correct Answer:** A

---

Question 131. An architect needs to run a resource-intensive Snowpark workload. Which strategy will help ensure the warehouse provides maximum compute and memory resources during execution?

A) Use `ALTER WAREHOUSE snowpark_opt_wh SET WAREHOUSE_SIZE = 'XXLARGE';` before running the workload  
B) Set the warehouse to auto-suspend frequently  
C) Use a very small warehouse to limit resource consumption  
D) Run the workload outside Snowflake  
E) Limit the number of concurrent queries to zero  

**Correct Answer:** A

---

Question 132. An architect notices that queries against an external table referencing large Parquet files on cloud storage are performing slowly. What is one step that can improve query performance?

A) Partition the data files in cloud storage based on common query filters  
B) Increase the size of the virtual warehouse  
C) Run ANALYZE TABLE to gather statistics  
D) Add clustering keys to the external table  
E) Disable automatic file discovery  

**Correct Answer:** A

---

Question 133. A logistics company frequently runs queries that filter by shipment date on their external table referencing cloud storage. Which action would result in faster query responses?

A) Organize the external data files by shipment date folders  
B) Load all data into an internal Snowflake table  
C) Enable time travel for the external table  
D) Use masking policies on date columns  
E) Increase the concurrency level of the warehouse  

**Correct Answer:** A

---

Question 134. An architect wants to minimize scan costs and reduce latency when querying a large external table on S3. What is the best approach?

A) Partition the data files in S3 based on frequently queried columns  
B) Use the smallest possible warehouse size  
C) Create materialized views on the external table  
D) Store all files in a single large folder  
E) Disable external stage caching  

**Correct Answer:** A

---

Question 135. A financial services firm experiences slow queries against an external table referencing JSON files. Which Snowflake feature can help improve performance for point lookup queries?

A) Enable the Search Optimization Service on the external table  
B) Convert all files to CSV format  
C) Set the retention period to zero  
D) Use temporary tables for the data  
E) Grant imported privileges to the consumer role  

**Correct Answer:** A

---

Question 136. A media company wants to optimize query speed on an external table referencing video metadata. What design principle should be applied to the source files?

A) Partition the files in cloud storage according to metadata attributes commonly used for filtering  
B) Compress all files into a single archive  
C) Use only unstructured blobs for storage  
D) Disable schema-on-read  
E) Store all files in random folders  

**Correct Answer:** A

---

Question 137. A retail company is running aggregate queries on an external table and notices delays. What step can the architect take to ensure better query performance?

A) Organize external files by keys often used in query predicates (e.g., region, date)  
B) Use the default file organization with no partitioning  
C) Increase the retention period on the external stage  
D) Turn off file format validation  
E) Use only VARIANT columns in the table definition  

**Correct Answer:** A

---

Question 138. A financial services company wants to ensure only authorized analysts can access sensitive tables in Snowflake. What RBAC feature should the architect leverage to enforce this requirement?

A) Assign SELECT privileges to analysts using roles  
B) Enable time travel for sensitive tables  
C) Set file format restrictions  
D) Use clustering keys for access control  
E) Partition data by department  

**Correct Answer:** A

---

Question 139. During a security audit, an architect is asked how Snowflake RBAC can prevent accidental data changes by junior staff. What is a characteristic of RBAC that supports this control?

A) Roles can restrict access and actions at the object level  
B) RBAC automatically encrypts all data  
C) RBAC disables data sharing features  
D) RBAC forces multi-factor authentication for all users  
E) Roles determine query performance  

**Correct Answer:** A

---

Question 140. A global retailer wants to simplify privilege management across multiple teams in Snowflake. Which RBAC characteristic should the architect use to manage access efficiently?

A) Privileges are granted to roles, not directly to users  
B) All privileges must be granted directly to each user  
C) Every user must have the same role  
D) Roles are only available for administrators  
E) RBAC requires external identity providers  

**Correct Answer:** A

---

Question 141. An architect is designing a multi-department data warehouse in Snowflake. How does RBAC help maintain data security between departments?

A) By assigning department-specific roles and granting privileges only on relevant objects  
B) By requiring all departments to use the same tables  
C) By disabling time travel for all departments  
D) By forcing all users into a single role  
E) By encrypting data at the column level  

**Correct Answer:** A

---

Question 142. A healthcare company wants to allow only doctors to view patient records, while administrators can manage but not view the data. Which RBAC principle enables this separation of duties?

A) Roles can be tailored so that different users have different access and capabilities  
B) Roles must be assigned randomly  
C) RBAC disables external sharing  
D) Privileges must be granted to everyone  
E) RBAC requires role hierarchy to match org chart exactly  

**Correct Answer:** A

---

Question 143. A media organization is onboarding new staff with different responsibilities. How does RBAC in Snowflake simplify provisioning access?

A) Users inherit privileges from assigned roles, so onboarding only requires assigning the correct role  
B) Each privilege must be set for every user individually  
C) New users cannot be added to Snowflake  
D) RBAC only works for database objects  
E) Role assignment must be performed outside of Snowflake  

**Correct Answer:** A

---

Question 144. A financial services architect wants to ensure only schema owners can grant privileges on objects within a specific schema, even as new tables and views are added. What Snowflake feature should be used to enforce this requirement?

A) Managed access schema  
B) Time travel  
C) External stage  
D) Secure view  
E) Data masking policy  

**Correct Answer:** A

---

Question 145. In a multi-team Snowflake environment, how can an architect support future privilege grants on newly created tables so only the schema owner can grant access to other roles?

A) By creating a managed access schema  
B) By enabling automatic clustering  
C) By setting up a resource monitor  
D) By using external tables  
E) By disabling all grants  

**Correct Answer:** A

---

Question 146. A retail company wants to prevent users from directly granting privileges on objects inside a schema to other roles, unless they are the schema owner. Which approach achieves this goal in Snowflake?

A) Use managed access schema for the relevant database objects  
B) Assign imported privileges to all user roles  
C) Store all objects in external stages  
D) Use only transient tables  
E) Grant privileges to the public role  

**Correct Answer:** A

---

Question 147. In Snowflake, what is a managed access schema?

A) A schema where only the schema owner (or roles with the MANAGE GRANTS privilege) can grant privileges on objects within the schema  
B) A schema that automatically encrypts all data  
C) A schema that allows any user to grant privileges on its objects  
D) A schema used only for temporary tables  
E) A schema that enforces clustering on all tables  

**Correct Answer:** A

---

Question 148. A retail company wants to ensure only schema owners can grant access to tables and views in a specific schema, even after new objects are created. Which Snowflake feature supports this requirement?

A) Managed access schema  
B) Secure data sharing  
C) External stage  
D) Materialized views  
E) Data masking policy  

**Correct Answer:** A

---

Question 149. What is a key benefit of using a managed access schema in Snowflake for a multi-team project?

A) Centralized privilege management for all objects in the schema  
B) Automatic creation of tables and views  
C) Schema objects are automatically replicated to other accounts  
D) All users can grant access to any object  
E) Only structured data formats are supported  

**Correct Answer:** A

---

Question 150. A development team plans to use database cloning to create isolated test environments quickly. What is a key consideration to address before cloning the production database?

A) Sensitive data in the source database may need to be masked or obfuscated before cloning  
B) Cloning automatically encrypts all data  
C) Cloning disables object creation in the clone  
D) Cloning increases the retention period of all tables  
E) Cloning removes all user roles from the clone  

**Correct Answer:** A

---

Question 151. An architect uses database cloning to enable parallel development efforts. What limitation should be communicated to developers regarding cloned databases?

A) Changes made to the clone do not affect the source database  
B) All changes in the clone are automatically reflected in the source  
C) Cloned databases cannot be queried  
D) Clones are only available for 24 hours  
E) Cloning deletes all stages and file formats  

**Correct Answer:** A

---

Question 152. A software company wants to use database cloning for rapid prototyping in their dev environment. Which storage consideration is important when managing cloned databases?

A) Cloned databases initially consume little additional storage, but changes made to the clone will increase storage usage  
B) Cloned databases always double the original storage instantly  
C) Storage usage for clones is unrelated to changes made in the clone  
D) Cloning compresses all data to minimize costs  
E) Clones require manual data backups to preserve state  

**Correct Answer:** A

---

Question 153. A project manager wants to ensure compliance requirements are met when using database clones for testing. What should the architect advise?

A) Review and update access controls and masking policies on clones to match compliance requirements  
B) Compliance is automatically inherited from the source database  
C) Clones cannot be assigned any roles  
D) Cloned databases do not support masking policies  
E) Compliance reviews are only necessary for production databases  

**Correct Answer:** A

---

Question 154. An architect is using database cloning to create a development environment from production. What happens to pipes that reference internal stages in the source database?

A) Pipes referring to internal stages are not cloned and must be recreated in the target environment  
B) Pipes are automatically redirected to external stages  
C) All pipes are cloned regardless of stage type  
D) Pipes are converted to tasks during cloning  
E) Pipes are duplicated and enabled in both source and clone  

**Correct Answer:** A

---

Question 155. A retail company clones a database for testing but discovers some automated data ingestion is missing. What is the likely reason if their ingestion uses pipes?

A) Pipes that reference internal stages are not cloned and need to be manually recreated  
B) All pipes are disabled after cloning  
C) Pipes referencing external stages are deleted  
D) Cloning automatically migrates all ingestion logic  
E) Pipes are converted to scheduled jobs in the clone  

**Correct Answer:** A

---

Question 156. During a database cloning process, an architect must ensure data pipelines continue working in the cloned environment. What must be checked and possibly recreated after cloning?

A) Any pipes in the source that refer to internal stages  
B) All masking policies  
C) External tables and file formats  
D) All virtual warehouses  
E) User roles and privileges  

**Correct Answer:** A

---

Question 157. A development team clones a schema to create a test environment in Snowflake. What happens to the privileges on tables and views inside the cloned schema?

A) The clone inherits all granted privileges of tables and views from the source schema  
B) All privileges are removed in the clone  
C) Only SELECT privileges are inherited  
D) Privileges must be manually reassigned to every object in the clone  
E) The clone can only inherit privileges from the database, not child objects  

**Correct Answer:** A

---

Question 158. After cloning a schema, a user notices that the database-level privileges are not present in the clone. What explains this behavior?

A) Database-level privileges are not inherited by the clone; only child object privileges are inherited  
B) All privileges from source are always inherited, including database privileges  
C) The clone automatically receives all privileges from every parent object  
D) Database privileges are converted to schema privileges in the clone  
E) Privileges are never inherited in any cloning operation  

**Correct Answer:** A

---

Question 159. An architect clones a schema for a parallel development stream. What must they consider regarding access control in the cloned schema?

A) Privileges on child objects (tables, views, etc.) are inherited, but any database-level privileges must be granted separately  
B) All privileges must be manually granted on every child object  
C) No privileges are inherited during cloning  
D) The clone receives privileges only on temporary tables  
E) All privileges are inherited, including future grants  

**Correct Answer:** A

---

Question 160. A global financial institution wants the highest level of availability for its mission-critical Snowflake workloads. Which architecture provides the best solution for minimizing disruption during a service event?

A) Deploy Snowflake across multiple cloud regions with automatic failover  
B) Run all workloads in a single region  
C) Use manual backups and restore processes  
D) Limit access to only one virtual warehouse  
E) Use transient tables for all data  

**Correct Answer:** A

---

Question 161. An e-commerce company is designing a disaster recovery strategy for its Snowflake environment. What feature should the architect use to ensure the fastest recovery time and least disruption?

A) Cross-region replication with failover capabilities  
B) Rely solely on time travel for data recovery  
C) Schedule weekly data exports to local servers  
D) Use only the default cloud region  
E) Suspend all warehouses during business hours  

**Correct Answer:** A

---

Question 162. A healthcare provider needs to maximize redundancy for its Snowflake environment, ensuring application processes remain available during regional outages. Which approach best supports this requirement?

A) Configure Snowflake accounts with business continuity enabled across multiple regions  
B) Store all historical data in on-premises servers  
C) Rely exclusively on daily manual backups  
D) Use a single small warehouse for all workloads  
E) Implement row-level security only  

**Correct Answer:** A

---

Question 163. During a Snowflake architecture review, the CTO asks how to guarantee the highest uptime for critical applications, regardless of cost. What is the best solution?

A) Multi-region deployment with automatic failover and replication  
B) Single-region deployment with increased warehouse size  
C) Manual monitoring and intervention for service events  
D) Take periodic snapshots to cloud storage  
E) Use transient tables to speed up recovery  

**Correct Answer:** A

---

Question 164. A financial services company shares sensitive data using secure views in a Snowflake data share. What is the most effective method for the architect to validate that only authorized records are visible to consumers?

A) Log in as a consumer user and query the secure view directly  
B) Review the Snowflake billing history  
C) Run a DESCRIBE TABLE command on the secure view  
D) Check the record count in the provider account  
E) Email consumers to confirm data visibility  

**Correct Answer:** A

---

Question 165. A media company wants to ensure that secure views shared via a data share only expose specific records to consumers. Which approach should the architect use to confirm correct data exposure?

A) Create a test consumer account and perform queries on the shared secure views  
B) Enable automatic clustering on the data share  
C) Modify the secure view definition in the consumer account  
D) Compare query results from materialized views  
E) Review the underlying table privileges  

**Correct Answer:** A

---

Question 166. An architect suspects that a secure view in a data share may be exposing more data than intended. What is a recommended validation step before granting access to production consumers?

A) Simulate consumer queries in a sandbox or test consumer environment  
B) Use the SHOW SHARES command to list all shares  
C) Rely on the default view settings without checks  
D) Grant access to all users and monitor usage  
E) Disable secure views in the share  

**Correct Answer:** A

---

Question 167. A retail company needs to verify that their shared secure views enforce row-level security for each consumer of a Snowflake data share. What is the best validation practice?

A) Query the shared secure views from a consumer account with appropriate role and confirm row-level security  
B) Inspect only the provider account’s access controls  
C) Only check the definition of the secure view  
D) Run metadata queries on the provider database  
E) Use external tools to review view logic  

**Correct Answer:** A

Question 168. A company needs to confirm that only appropriate records are exposed in a data share for each consumer. How can the provider account efficiently validate secure view results for different consumers?

A) By setting SIMULATED_DATA_SHARING_CONSUMER to the consumer’s account identifier in the provider session and querying the secure view  
B) By granting SELECT privileges to every user in the provider account  
C) By exporting the data share and importing into a sandbox  
D) By disabling all secure views in the share  
E) By using masking policies on internal tables only  

**Correct Answer:** A

---

Question 169. An architect wants to review the privileges of a newly created user, user_01, in Snowflake. What does the command `SHOW GRANTS TO USER user_01;` display?

A) All privileges that have been granted to user_01, including roles assigned  
B) All privileges granted by user_01 to other users  
C) Only table-level grants for user_01  
D) Privileges on objects owned by user_01  
E) All database objects created by user_01  

**Correct Answer:** A

---

Question 170. In a troubleshooting session, a Snowflake architect runs the command `SHOW GRANTS ON USER user_01;`. What information does this command provide?

A) All privileges granted on the user object user_01 (e.g., who can manage this user)  
B) A list of all roles assigned to user_01  
C) Warehouse usage statistics for user_01  
D) Secure view access for user_01  
E) Masking policies applied to user_01  

**Correct Answer:** A

---

Question 171. A company manager asks the architect about the difference between `SHOW GRANTS TO USER user_01;` and `SHOW GRANTS ON USER user_01;`. Which statement is correct?

A) The first command lists privileges and roles assigned to user_01; the second lists who has privileges to manage the user_01 object itself  
B) Both commands list the same information  
C) The first command shows login history, and the second shows grants  
D) The first command only works for warehouse objects  
E) The second command lists all databases user_01 can access  

**Correct Answer:** A

---

Question 172. The business team wants to visualize daily data in Tableau, and old data can be discarded. Which Snowflake table type is most appropriate for this use case?

A) Transient table, since it does not retain historical data and is cost-effective for temporary storage  
B) Permanent table, as it retains all historical data  
C) External table, to keep old data accessible  
D) Materialized view, for fast query performance on all historical data  
E) Secure view, to restrict access to old data  

**Correct Answer:** A

---

Question 173. As a Snowflake architect, which strategy best supports the business requirement of discarding yesterday’s data when loading new data for Tableau dashboards?

A) Drop or truncate the table before loading new data each day  
B) Store every day’s data in a new table and keep them indefinitely  
C) Archive old data to an external stage  
D) Use time travel to retain old versions  
E) Enable automatic clustering for the table  

**Correct Answer:** A

---

Question 174. The business team requests assurance that only the latest data is available for Tableau reporting. What Snowflake feature or process should be implemented to meet this requirement?

A) Schedule a daily ETL job that replaces the table contents with fresh data  
B) Partition the table by date and retain all partitions  
C) Set up continuous data loading and never delete old data  
D) Use external tables to access old and new data  
E) Apply row-level security to hide old data  

**Correct Answer:** A

---

Question 175. An architect is tasked with integrating Snowflake with an external application. The team suggests using a REST API. What is a REST API?

A) An interface that allows systems to communicate over HTTP using standardized methods like GET, POST, PUT, and DELETE  
B) A tool for visualizing data in dashboards  
C) A protocol for real-time streaming of video data  
D) An encryption algorithm for securing network traffic  
E) A type of database management system  

**Correct Answer:** A

---

Question 176. A retail company wants to automate the loading of daily sales data into Snowflake from their web application. Which feature of REST APIs makes them suitable for this task?

A) They allow programmatic data exchange between applications over the web  
B) They require manual data entry  
C) They only work for internal network communication  
D) They do not support authentication  
E) They can only be used for file storage  

**Correct Answer:** A

---

Question 177. A Snowflake architect needs to expose business logic to a third-party analytics platform. Which solution would be most appropriate, and why?

A) Build a REST API so the analytics platform can send requests and receive responses over HTTP  
B) Use a spreadsheet and email it manually  
C) Store the logic in a local file on a server  
D) Require the third party to install Snowflake locally  
E) Use SMS messages for communication  

**Correct Answer:** A

---

Question 178. The business requires analysts to load their own data but not share it with other users. What privilege management strategy supports this in Snowflake?

A) Grant object creation privileges to ANALYST_ROLE in managed access schemas, but manage all grants through SYSADMIN  
B) Allow analysts to create and grant privileges on any object  
C) Give all users in the organization access to the analysts’ database  
D) Use external stages for all analyst data loads  
E) Enable time travel for all tables in the database  

**Correct Answer:** A

---

Question 179. A multinational company uses Snowflake on Azure in the Netherlands and receives frequently updated sales data as JSON files in an Amazon S3 bucket located in the AWS Singapore region. The analytics team needs to access this data for daily reporting. The architect is tasked to ensure access to up-to-date data while keeping egress costs low and maintaining low latency. Which approach best meets all the requirements with the least operational overhead?

A) Create a Snowflake external table pointing directly to the S3 bucket and query the data live for each analysis  
B) Schedule regular jobs to replicate data from the S3 bucket to Azure Blob Storage and analyze the replicated data using Snowflake external tables in Azure  
C) Use Snowflake’s materialized views on top of the external table pointing to the S3 bucket to cache the data locally  
D) Move the JSON files manually from S3 to Snowflake stage storage before each analysis  

**Correct Answer:** B

---

Question 180. An architect must design a data ingestion pipeline for a company using Snowflake on Azure in the Netherlands. The team needs frequent access to changing data stored in an AWS S3 bucket in Singapore, and wants to minimize both latency and egress costs with minimal maintenance. Which solution best fulfills these requirements?

A) Query the S3 bucket directly from Snowflake in Azure for every analytics task  
B) Use a transient Snowflake table and periodically load data from the external S3 table  
C) Use Snowflake’s Secure Data Sharing to share data from an AWS-based Snowflake account to Azure  
D) Set up a real-time streaming pipeline to continuously move data from S3 in Singapore to Azure Blob Storage in the Netherlands  

**Correct Answer:** B

---

Question 181. A company’s business analyst team wants to analyze product usage data stored in JSON files in an Amazon S3 bucket in Singapore from their Snowflake environment on Azure Netherlands. The architect is asked to provide a solution that gives timely access to new data, keeps egress costs low, and is simple to operate. Which option should the architect recommend?

A) Set up a scheduled job to replicate only the frequently changing JSON files from S3 Singapore to Azure Blob Storage Netherlands  
B) Use Snowflake’s cross-cloud data sharing to access S3 data without replication  
C) Query the S3 bucket directly via Snowflake’s external table for each analysis  
D) Use a third-party ETL tool to extract, transform, and load the data from S3 to Snowflake every hour  

**Correct Answer:** A

---

Question 182. A global retail company uses Snowflake in Azure Netherlands. Their analytics team requires frequent access to JSON data stored in Amazon S3 in Singapore for near real-time dashboards. Which solution best balances low egress cost, low query latency, and minimal operational overhead?

A) Query the S3 bucket directly from Snowflake in Azure for every dashboard refresh  
B) Use Snowflake materialized views on an external table pointing to the S3 bucket  
C) Periodically copy frequently changing data from S3 Singapore to Azure Blob Storage Netherlands and analyze it via Snowflake external tables  
D) Move the JSON files manually into Snowflake's internal stage before each analysis  
E) Use Snowflake's result cache on queries against the S3 external table  

**Correct Answer:** C

---

Question 183. The architect for a logistics firm wants to optimize query performance on a very large table in Snowflake. Which of the following optimization techniques will incur additional **storage costs** on the account?

A) Using result caching for queries  
B) Defining and maintaining clustering keys on the table  
C) Creating views (not materialized) on top of the table  
D) Increasing compute resources for the virtual warehouse  
E) Using query acceleration service  

**Correct Answer:** B

---

Question 184. A company is using Snowflake in Azure Netherlands and considers enabling the **Search Optimization Service** on some large tables with frequent point lookup queries. What is a key implication of this decision?

A) Queries will always run with zero latency  
B) The service will incur additional storage costs due to persistent search data structures  
C) The service is only available in AWS regions  
D) The service eliminates the need for clustering keys  
E) There is no cost for using the Search Optimization Service  

**Correct Answer:** B

---

Question 185. A media firm uses Snowflake in Azure Netherlands to analyze streaming data stored in JSON format in AWS S3 Singapore. They want to **minimize egress costs** and **maintain low latency** for business-critical dashboards. Which approach is most cost-effective?

A) Use Snowflake's automatic clustering to organize the S3 external table  
B) Create a materialized view on the external table directly against S3  
C) Copy data from S3 to a transient table in Snowflake, and query the transient table  
D) Query S3 directly for each dashboard refresh  
E) Use the Search Optimization Service on the external table  

**Correct Answer:** C

---

Question 186. Which of the following Snowflake features **does NOT directly incur storage costs** when enabled or used?

A) Time Travel  
B) Search Optimization Service  
C) Materialized Views  
D) Result Caching  
E) Automatic Clustering  

**Correct Answer:** D

---

Question 187. A logistics company stores sensor data in an AWS S3 bucket, and new files arrive every minute. They want their Snowflake environment to automatically ingest these files for near real-time analytics with minimal manual intervention. Which Snowflake feature should the architect recommend to enable seamless auto-ingestion triggered by AWS cloud messaging services?

A) Bulk COPY INTO command  
B) Streams and Tasks  
C) Snowpipe  
D) Materialized Views  

**Correct Answer:** C

---

Question 188. A financial institution needs to process transaction logs in near real-time as soon as they are uploaded to Azure Blob Storage. Which Snowflake ingestion method can use Azure Event Grid to automatically trigger loading of new files into Snowflake?

A) Snowpipe  
B) Manual file upload to internal stage  
C) Data Exchange  
D) External tables with a scheduled refresh  

**Correct Answer:** A

---

Question 189. A company wants to minimize latency for data analytics by having new files in Google Cloud Storage automatically ingested into Snowflake as soon as they arrive. Which combination of Snowflake feature and cloud provider service should the architect choose?

A) Snowpipe with Google Pub/Sub  
B) Bulk COPY INTO with Google Cloud Functions  
C) Streams and Tasks with Google Sheets  
D) Materialized Views with Google Dataflow  

**Correct Answer:** A

---

Question 190. A retail company stores customer interaction logs as JSON in a VARIANT column within Snowflake. Initially, queries filtering by specific fields in the JSON performed well, but recently, the analytics team reports significant slowdowns in their dashboards. As the Snowflake architect, which factor is most likely contributing to the poor query performance?

A) The VARIANT column was indexed incorrectly  
B) The number of micro-partitions has decreased  
C) The VARIANT column’s structure has become more complex and less uniform over time  
D) Query caching has been disabled  

**Correct Answer:** C

---

Question 191. An e-commerce business uses a Snowflake table with a VARIANT column to store product metadata in JSON format. After a recent data migration, users notice that queries retrieving specific attributes from the JSON are much slower than before. Which architectural solution would BEST address this performance issue?

A) Increase the compute warehouse size  
B) Extract frequently queried JSON attributes into dedicated columns and use clustering keys  
C) Enable Time Travel for the table  
D) Compress the VARIANT column using a stronger algorithm  

**Correct Answer:** B

---

Question 192. A financial services firm has a reporting dashboard that runs complex queries on a Snowflake table where transaction events are stored as JSON in a VARIANT column. Performance was acceptable, but after several weeks, reports became sluggish. What is a likely root cause an architect should investigate first?

A) The warehouse has reached its maximum storage quota  
B) The VARIANT column now contains highly heterogeneous JSON documents  
C) The data retention period was reduced  
D) The table was converted from transient to permanent  

**Correct Answer:** B

---

Question 193. An online travel agency stores booking details as JSON in a VARIANT column. After a recent increase in business, analysts notice that queries to retrieve specific booking information are much slower. As the Snowflake architect, which scenario is most likely causing the slowdown?

A) The VARIANT column contains increasingly diverse JSON structures  
B) The Snowflake account’s credit balance is low  
C) The queries are executed during Snowflake maintenance windows  
D) The network bandwidth between Snowflake and external sources has decreased  

**Correct Answer:** A

---

Question 194. A healthcare provider stores patient records as JSON documents in a VARIANT column. Recently, reporting queries are taking much longer to run. Which action should the architect take to improve query performance?

A) Increase the retention period on the table  
B) Create dedicated columns for frequently accessed attributes from the JSON  
C) Grant more privileges to analysts running the queries  
D) Set the table to transient to reduce storage costs  

**Correct Answer:** B

---

Question 195. A logistics company uses a VARIANT column to store shipment event data as JSON. Performance of analytics queries has dropped significantly in the past month. Which underlying issue should the Snowflake architect investigate first?

A) JSON documents in the VARIANT column have become larger and more nested  
B) The account has enabled data masking policies  
C) The compute warehouse uses a different region  
D) The table was cloned to a different database  

**Correct Answer:** A

---

Question 196. A financial institution is planning to segregate workloads in Snowflake. Some data requires strict regulatory compliance (PCI DSS), while other business units do not require this level of security. They need to share select datasets between these environments. What is the MOST cost-effective architectural approach?

A) Store all data in a single PCI DSS-compliant account  
B) Use separate Snowflake accounts for PCI DSS and non-PCI DSS workloads, sharing data securely where needed  
C) Store all data in the same database with row-level policies  
D) Store PCI DSS and non-PCI DSS data in separate schemas within one account  

**Correct Answer:** B

---

Question 197. A retail company wishes to minimize Snowflake costs while maintaining compliance. They have marketing data not subject to compliance and transactional data requiring PCI DSS certification. Which design strategy is MOST cost-effective?

A) Provision all data in a PCI DSS-certified account  
B) Use multiple Snowflake accounts, assigning PCI DSS only where needed  
C) Store all data in encrypted tables  
D) Separate data by schema within a single account  

**Correct Answer:** B

---

Question 198. A healthcare provider needs to share patient data (subject to HIPAA) and research data (not subject to compliance) across departments. The goal is to optimize cost and compliance in Snowflake. Which approach should the architect recommend?

A) Use one Snowflake account for all data, enabling highest compliance settings for the entire account  
B) Implement multiple-account strategy, applying compliance only to the account holding patient data and sharing with non-compliant accounts as needed  
C) Store all data in a single account using table masking  
D) Use a single account with distinct databases for each data type  

**Correct Answer:** B

---

Question 199. A media company uses Snowpipe to ingest streaming data into Snowflake. The architecture team wants to maintain a detailed log of every data load for audit purposes. Which approach using the Snowpipe REST API BEST fulfills this requirement?

A) Use the API to trigger data loads and rely on Snowflake’s native INFORMATION_SCHEMA views  
B) Configure the REST API to post load history events to a custom external logging service after each successful API call  
C) Use the REST API’s response payloads to capture load history metadata and write it to an internal logging table  
D) Query the data files directly after each load to reconstruct the load history  

**Correct Answer:** C

---

Question 200. A logistics company wishes to track failed and successful Snowpipe loads for compliance reporting. As the Snowflake architect, which method leverages the Snowpipe REST API to create a reliable load history log?

A) Periodically query the REST API’s load history endpoint and store the results in a separate database table  
B) Enable file-level auditing on the external stage used by Snowpipe  
C) Use the REST API to retrieve load events and then push relevant details (timestamps, status, file names) to an enterprise logging system  
D) Parse the Cloud Storage access logs for Snowpipe activity  

**Correct Answer:** C

---

Question 201. An insurance company must maintain a history of all files loaded via Snowpipe for regulatory review. What is the MOST effective way for the architect to use the Snowpipe REST API to keep an accurate log?

A) Configure the REST API to write directly to a Snowflake table after each load  
B) Extract load history from the REST API’s response whenever a load is triggered and aggregate the data in a log table  
C) Query the data warehouse for recently loaded files  
D) Rely on stage metadata in the cloud storage provider  

**Correct Answer:** B

---

Question 202. Your company ingests large volumes of data daily into Snowflake using Snowpipe. The compliance team requests a detailed log of all data loads, including file names, load times, and statuses, for auditing purposes. As a Snowflake architect, how should you use the Snowpipe REST API to meet this requirement?

A) Schedule a daily query on the Snowflake metadata tables and export results to a CSV file  
B) Use the Snowpipe REST API’s insertReport function to capture and store load history details in a dedicated logging table  
C) Enable Snowflake’s automatic load notification emails for each ingestion  
D) Configure a cloud storage lifecycle rule to archive ingested files  

**Correct Answer:** B

---

Question 203. A financial client wants to monitor failed file loads for their Snowpipe pipelines in real time and maintain a history for troubleshooting. Which approach best leverages Snowpipe REST API features to achieve this?

A) Use insertReport to record load events and statuses in a custom audit table  
B) Query the INFORMATION_SCHEMA.LOAD_HISTORY table every hour  
C) Subscribe to Snowflake’s system alert emails for each failed load  
D) Store all ingested files in a separate folder for manual review  

**Correct Answer:** A

---

Question 204. During a data migration project, your team must track which files were loaded by Snowpipe and when, and ensure this log is available for business users to query. What is an effective solution using Snowpipe REST API?

A) Configure Snowpipe to send notifications to a Slack channel  
B) Utilize the insertReport function to push load event details into a reporting database  
C) Manually record load events in an Excel spreadsheet  
D) Rely on the cloud provider’s storage logs for file access history  

**Correct Answer:** B

---

Question 205. An organization needs to maintain an immutable record of all Snowpipe data loads for regulatory reporting. Which Snowpipe REST API capability is most appropriate for creating such a log?

A) insertReport  
B) updateLog  
C) deleteReport  
D) queryLoadHistory  

**Correct Answer:** A

---

Question 206. A retail company wants to analyze patterns in their nightly data ingestion runs using Snowpipe, such as peak load times and error frequencies. What would be the best practice using Snowpipe REST API?

A) Build a dashboard using data collected by insertReport on load events  
B) Review the Snowpipe configuration files manually  
C) Use the REST API to trigger Snowpipe loads, but not log any history  
D) Query the file system for timestamps on uploaded files  

**Correct Answer:** A

---

Question 207. As part of a disaster recovery plan, your team must be able to reconstruct historical data loads in Snowflake in case of system failure. Which Snowpipe REST API feature can support this requirement?

A) insertReport can provide detailed logs of all past data loads for reconstruction  
B) The REST API can automatically reload all previously ingested files  
C) Snowpipe REST API’s deleteReport function archives old load logs  
D) System tables in Snowflake are updated automatically with every data load  

**Correct Answer:** A

---

Question 208. Your organization’s client application supports several authentication methods, including Okta, username/password, and key pair authentication. As the Snowflake architect, which authentication method should you recommend as the top priority for connecting to Snowflake, according to best practice?

A) Username and password  
B) Okta SSO  
C) OAuth token  
D) Key pair authentication  

**Correct Answer:** B

---

Question 209. A company wants to ensure secure, seamless user access to Snowflake for their client application, which supports Okta, basic authentication, and external OAuth. What is the recommended order of priority for authentication methods to maximize both user experience and security?

A) Basic authentication → Okta → External OAuth  
B) Okta → External OAuth → Basic authentication  
C) External OAuth → Basic authentication → Okta  
D) Key pair authentication → Okta → Basic authentication  

**Correct Answer:** B

---

Question 210. In a scenario where your client application integrates with Okta and also allows direct username/password authentication, what is the best practice for authentication priority when connecting to Snowflake?

A) Always use username/password for simplicity  
B) Use Okta as the primary method and fall back to username/password only if necessary  
C) Alternate between methods on each connection attempt  
D) Use OAuth as the primary method  

**Correct Answer:** B

---

Question 211. A retail company uses Snowflake tasks for daily ETL processing. The data engineering manager asks you, as the Snowflake architect, to provide a report detailing the last week’s task runs, including statuses and error messages. Which query should you use to retrieve this information?

A) SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY()) WHERE scheduled_time >= DATEADD('day', -7, CURRENT_DATE);  
B) SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE table_name = 'TASK_HISTORY';  
C) SELECT * FROM INFORMATION_SCHEMA.TASKS WHERE status = 'FAILED';  
D) SELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY()) WHERE query_type = 'TASK';  

**Correct Answer:** A

---

Question 212. A Snowflake architect needs to analyze the execution history of a specific task to troubleshoot intermittent failures reported by the operations team. What is the most effective way to obtain detailed execution logs for that task?

A) Use SELECT * FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY()) WHERE NAME = '<task_name>';  
B) Check the cloud provider’s storage logs for access patterns  
C) Query INFORMATION_SCHEMA.TASKS for the last status update  
D) Use SELECT * FROM TABLE(INFORMATION_SCHEMA.QUERY_HISTORY()) WHERE query_text LIKE '%TASK%';  

**Correct Answer:** A

---

Question 213. A Snowflake architect at ACCOUNTA needs to share the MARKET_DB database with a business partner whose Snowflake account, PARTNERB, is hosted in Azure East US 2. What is the first step required to enable cross-cloud, cross-region data sharing?

A) Create a database clone in PARTNERB’s account  
B) Set up a Snowflake Reader Account in PARTNERB  
C) Create a share in ACCOUNTA and configure a listing in Snowflake Marketplace or a Direct Share for cross-cloud sharing  
D) Enable VPC peering between ACCOUNTA and PARTNERB  

**Correct Answer:** C

---

Question 214. After ACCOUNTA publishes MARKET_DB as a share on a cross-cloud listing, what must PARTNERB do in their Azure East US 2 account to access the shared data?

A) Create a database from the share in their account using the Snowflake UI or SQL commands  
B) Request a physical backup of MARKET_DB from ACCOUNTA  
C) Set up external stages pointing to ACCOUNTA’s S3 bucket  
D) Use a VPN connection to access ACCOUNTA’s Snowflake account directly  

**Correct Answer:** A

---

Question 215. To successfully consume shared data from MARKET_DB, PARTNERB must meet certain prerequisites in their Snowflake account. Which of the following is required?

A) PARTNERB must have privileges to create databases from shares  
B) PARTNERB must migrate their Snowflake account to AWS us-east-1  
C) PARTNERB must have the same warehouse size as ACCOUNTA  
D) PARTNERB must use the same database name as ACCOUNTA  

**Correct Answer:** A

---

Question 216. A company is building a custom integration to automate file ingestion into Snowflake using the insertFiles API of Snowpipe. The team wants to ingest several thousand files in a single API call for efficiency. What is a limitation they should be aware of?

A) The insertFiles API does not support ingesting files from GCP storage  
B) There is a limit on the maximum number of files that can be included in a single insertFiles API call  
C) The insertFiles API cannot trigger Snowpipe automatically  
D) File sizes must be under 1 MB for each file in the request  

**Correct Answer:** B

---

Question 217. During a testing phase, your development team attempts to use the insertFiles API to ingest files, but some files are not loaded successfully. Upon investigation, you discover that the API does not provide immediate ingestion status feedback for each file. What is another limitation of the insertFiles API?

A) It only supports JSON files  
B) The API does not provide synchronous status feedback; ingestion status must be checked separately  
C) The API automatically retries failed ingestions  
D) It only works with files in the same region as the Snowflake account  

**Correct Answer:** B

---

Question 218. As a Snowflake architect, you are asked to design a system that uses Snowpipe’s insertFiles API to load files from multiple cloud providers into a single database. Which limitation should be considered when planning this architecture?

A) The insertFiles API supports only files stored in the same cloud provider as the Snowflake account  
B) The API encrypts files before ingestion  
C) The insertFiles API requires files to have a .csv extension  
D) The API can ingest files from any cloud provider without restriction  

**Correct Answer:** A

---

Question 219. As a Snowflake architect, you are designing a system to automate file ingestion using Snowpipe’s insertFiles API. The ETL team wants to speed up processing by submitting 5,000 files in a single API request. What will be the result, and what best practice should you follow?

A) The insertFiles API will process all 5,000 files successfully  
B) The insertFiles API will return an error; requests must be limited to 1,000 files per call  
C) The API will queue the extra files and process them later  
D) The API will automatically split the request into batches of 1,000 files each  

**Correct Answer:** B

---

Question 220. A Snowflake architect increases the warehouse size from L to XL to improve the performance of a long-running join query, but the query time remains unchanged. Upon further investigation, what data-related issue could be causing this lack of improvement?

A) Data skew in the join key column is causing one compute node to process much more data than others  
B) The query is using too many subqueries  
C) The XL warehouse is under-provisioned for the dataset  
D) The result set is not being cached  

**Correct Answer:** A

---

Question 221. In a large join between two tables, you notice that one particular value in the join column appears far more frequently than others. What is the likely impact of this scenario on Snowflake’s query performance?

A) The query will utilize all compute nodes evenly  
B) One compute node will be overloaded, resulting in slow performance due to data skew  
C) The query will automatically retry failed nodes  
D) The warehouse size will automatically adjust to handle the load  

**Correct Answer:** B

---

Question 222. A retail analytics team is experiencing inconsistent performance with their nightly joins in Snowflake, despite scaling up the virtual warehouse. What strategy should the architect consider to address performance issues caused by data skew?

A) Redistribute the data or rewrite the join logic to minimize skew in the join key  
B) Increase the warehouse size further  
C) Use manual clustering for all tables  
D) Disable result caching  

**Correct Answer:** A

---

Question 223. A Snowflake architect attempts to clone a schema at a timestamp when a different schema instance exists with the same name. What steps should the architect take to restore and clone the original schema as of the desired timestamp?

A) Rename the current schema to free up the name, then perform an UNDROP to restore the previous version and run the CLONE statement  
B) Increase the warehouse size and retry the clone  
C) Drop the database and recreate it  
D) Request support to retrieve the schema from backups  

**Correct Answer:** A

---

Question 224. An architect receives an error when trying to clone a schema as of a timestamp before the current instance was created. The error states: "Time travel data is not available for schema STAGING. The requested time is either beyond the allowed time travel period or before the object creation time." What is the most likely cause?

A) The requested timestamp is before the current schema instance was created  
B) The schema has never existed  
C) The clone command syntax is incorrect  
D) The user's role lacks sufficient privileges  

**Correct Answer:** A

---

Question 225. After renaming the current schema, an architect uses the UNDROP SCHEMA command to recover a previous version of a schema dropped a week ago. What must the architect do next to access the historical data as of a specific timestamp?

A) Clone the undropped schema using the AT (TIMESTAMP => ...) clause  
B) Drop the undropped schema again  
C) Increase the Time Travel retention period  
D) Restore the underlying database  

**Correct Answer:** A

---

Question 226. A data engineer writes a stored procedure in Snowflake and wants to capture the username of the person executing it for audit logging. Which context function should they use?

A) `CURRENT_USER`  
B) `CURRENT_ROLE`  
C) `CURRENT_DATABASE`  
D) `CURRENT_SESSION`  

**Correct Answer:** A

---

Question 227. A Snowflake architect needs to create a script that dynamically adapts to the current working schema and warehouse for session troubleshooting. Which two context functions should be included in the script?

A) `CURRENT_DATABASE` and `CURRENT_REGION`  
B) `CURRENT_WAREHOUSE` and `CURRENT_SCHEMA`  
C) `CURRENT_USER` and `CURRENT_ACCOUNT`  
D) `CURRENT_ROLE` and `CURRENT_SESSION`  

**Correct Answer:** B

---

Question 228. A Snowflake architect is investigating slow filter queries on a large table. Which parameter, when found to be **significantly greater than 1**, signals that the table is not well-clustered?

A) Clustering Ratio  
B) Clustering Depth  
C) Row Count  
D) Partition Size  

**Correct Answer:** A

---

Question 229. During a performance review, an architect observes that the **average clustering depth** of a table is much higher than expected. What does this imply about the table’s clustering?

A) The table is well-clustered and queries will be efficient  
B) The table is not well-clustered, resulting in less efficient queries  
C) The table has too few micro-partitions  
D) The table is over-indexed  

**Correct Answer:** B

---

Question 230. When analyzing micro-partition statistics, which parameter should an architect examine to confirm that filter queries are scanning more micro-partitions than necessary due to poor clustering?

A) Clustering Depth  
B) Micro-partition Count  
C) Clustering Key  
D) Table Size  

**Correct Answer:** A

Question 231. A business analyst reports that queries filtering by a certain column are slow, even after clustering. What parameter can indicate that the clustering is ineffective for that column?

A) High clustering depth for the column  
B) Low row count in the table  
C) High number of columns  
D) Table retention period  

**Correct Answer:** A

---

Question 232. The data engineering team runs a clustering information query and notices that many micro-partitions have a clustering depth of greater than 5. What action should be considered?

A) Re-cluster the table to reduce clustering depth  
B) Increase the table’s retention period  
C) Add more columns to the clustering key  
D) Reduce the warehouse size  

**Correct Answer:** A

---

Question 233. After performing a clustering operation, an architect finds that the clustering depth has only slightly decreased. What does this suggest about the table or the clustering key?

A) The clustering key may not be optimal for the table’s data distribution  
B) The warehouse size is too small  
C) The table is too large to be clustered  
D) The micro-partitions are too small  

**Correct Answer:** A

---

Question 234. A Snowflake architect is reviewing clustering metadata for a table and sees that the clustering depth for most micro-partitions is 1, but a few have depths of 15 or higher. What does this suggest about data distribution?

A) Most micro-partitions are well-clustered, but some have significant data overlap and poor clustering  
B) All micro-partitions are equally well-clustered  
C) The clustering key is optimal for the entire table  
D) The table does not need re-clustering  

**Correct Answer:** A

---

Question 235. An engineer wants to monitor changes in clustering efficiency after a scheduled reclustering job. Which parameter should they track over time to evaluate improvements?

A) Average clustering depth  
B) Total row count  
C) Number of columns in the table  
D) Table retention period  

**Correct Answer:** A

---

Question 236. A data scientist notices that queries with filters on the clustering key are still slow, even after reclustering. What clustering depth value would most likely explain this issue?

A) Clustering depth values remain high across many micro-partitions  
B) Clustering depth is consistently 1 across all micro-partitions  
C) The table’s row count has decreased  
D) The table has only one micro-partition  

**Correct Answer:** A

---

Question 237. A Snowflake architect is configuring a `COPY INTO <location>` command to unload data into an Amazon S3 bucket. Which of the following options is a valid configuration for specifying the file format?

A) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'NONE')  
B) FILE_FORMAT = (TYPE = 'PDF')  
C) FILE_FORMAT = (TYPE = 'DOCX')  
D) FILE_FORMAT = (TYPE = 'HTML')  

**Correct Answer:** A

---

Question 238. When unloading data from a Snowflake table to a cloud storage location using `COPY INTO <location>`, which of these access configurations is valid for authenticating to an Azure Blob Storage location?

A) STORAGE_INTEGRATION  
B) PASSWORD  
C) OAUTH_TOKEN  
D) API_KEY  

**Correct Answer:** A

---

Question 239. A data engineer wants to partition unloaded files by a column value using the `COPY INTO <location>` command. Which of the following statements correctly implements this configuration?

A) PARTITION BY (column_name)  
B) SPLIT BY column_name  
C) GROUP BY column_name  
D) CLUSTER BY column_name  

**Correct Answer:** A

---

Question 240. An architect wants to unload data from Snowflake into an S3 bucket in compressed CSV files. Which of the following configurations is valid for specifying file format and compression in the `COPY INTO <location>` command?

A) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'GZIP')  
B) FILE_FORMAT = (TYPE = 'CSV', COMPRESSION = 'PDF')  
C) FILE_FORMAT = (TYPE = 'CSV', ENCRYPTION = 'NONE')  
D) FILE_FORMAT = (TYPE = 'PARQUET', COMPRESSION = 'GZIP')  

**Correct Answer:** A

---

Question 241. A data engineer needs to ensure unloaded files are encrypted using a customer-managed key when using `COPY INTO <location>` with Azure Blob Storage. Which option should be included in the command?

A) ENCRYPTION = (TYPE = 'AZURE_CSE', MASTER_KEY = '<key_value>')  
B) ENCRYPTION = (TYPE = 'NONE')  
C) ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  
D) COMPRESSION = 'ENCRYPTED'  

**Correct Answer:** A

---

Question 242. When unloading data into Amazon S3, which of the following file formats support both compression and encryption options in the `COPY INTO <location>` command?

A) CSV  
B) PARQUET  
C) JSON  
D) All of the above  

**Correct Answer:** D

---

Question 243. The analytics team wants to unload data in JSON format, compressed and encrypted, into an external stage. Which configuration is valid for the file format and options in the `COPY INTO <location>` command?

A) FILE_FORMAT = (TYPE = 'JSON', COMPRESSION = 'GZIP'), ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  
B) FILE_FORMAT = (TYPE = 'XML', COMPRESSION = 'GZIP'), ENCRYPTION = (TYPE = 'NONE')  
C) FILE_FORMAT = (TYPE = 'JSON', ENCRYPTION = 'NONE')  
D) FILE_FORMAT = (TYPE = 'JSON', COMPRESSION = 'PDF'), ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')  

**Correct Answer:** A

---

Question 244. A Snowflake architect is overseeing a finance department’s analytics workload. The multi-cluster warehouse is set to the economy scaling policy. When will Snowflake start a new cluster for this warehouse?

A) When queued queries cannot be processed quickly enough by the current cluster  
B) As soon as any user submits a query  
C) When the warehouse has been idle for 10 minutes  
D) Whenever a new user connects to the warehouse  

**Correct Answer:** A

---

Question 245. A logistics company wants to minimize costs during off-peak hours but still ensure timely query processing. They use the economy scaling policy for their multi-cluster warehouse. What is the main factor that triggers Snowflake to start a new cluster?

A) The number and urgency of queued queries  
B) The total number of users connected  
C) The amount of data stored in the warehouse  
D) The frequency of warehouse restarts  

**Correct Answer:** A

---

Question 246. In the economy scaling policy, when will Snowflake NOT start a new cluster?

A) When all queries can be processed quickly by the existing cluster  
B) When the cluster’s CPU usage exceeds 90%  
C) When query queue length is more than ten  
D) When there is a maintenance window scheduled  

**Correct Answer:** A

---

Question 247. During a high-traffic marketing campaign, the analytics team experiences query delays even with a multi-cluster warehouse set to the economy scaling policy. What should the architect investigate as a possible cause?

A) The queued queries are still being processed efficiently by the existing cluster, so no new cluster is started  
B) The warehouse is set to standard scaling policy  
C) The stage storage is full  
D) The number of clusters allowed is set to maximum  

**Correct Answer:** A

---

Question 248. A Snowflake administrator is asked why the multi-cluster warehouse didn’t start a new cluster during a spike in user activity. Which explanation is correct regarding the economy scaling policy?

A) New clusters are only started if queued queries cannot be processed quickly enough by the current cluster  
B) New clusters are started for every new user session  
C) New clusters are started every time the warehouse is resized  
D) New clusters are started at a fixed interval regardless of workload  

**Correct Answer:** A

---

Question 249. A Snowflake architect is asked why a multi-cluster warehouse with the economy scaling policy did not start a new cluster during a peak period. The architect notes that although there were queued queries, the system estimated that the additional cluster would not be busy for at least six minutes. What does this behavior demonstrate about Snowflake’s economy scaling policy?

A) New clusters are only started if the system estimates the additional cluster will be busy for at least six minutes  
B) New clusters are started for every query in the queue, regardless of estimated workload  
C) Clusters are started based solely on the number of connected users  
D) Clusters are automatically started every 15 minutes during peak hours  

**Correct Answer:** A

---

Question 250. A Snowflake architect is tasked with designing a data loading solution for a retail company’s daily sales data. Which of the following considerations should be taken into account when choosing the data loading method? (Choose three.)

A) The volume and frequency of incoming data  
B) The format and structure of source data files  
C) The database user’s preferred SQL editor  
D) The need for real-time or near-real-time availability of loaded data  
E) The color scheme of the Snowflake UI  

**Correct Answers:** A, B, D

---

Question 251. A logistics firm needs to load nightly batch data from multiple remote sources into Snowflake. Which three factors are most important for the architect to evaluate when selecting a loading approach?

A) Network connectivity and bandwidth between source systems and Snowflake  
B) Whether the source files are encrypted or compressed  
C) The time zone settings of the loading script  
D) Availability of automation and error handling features  
E) The vendor of the source database  

**Correct Answers:** A, B, D

---

Question 252. A finance department wants to migrate legacy data into Snowflake from various file formats. Which of the following considerations should influence the architect’s choice of loading method? (Choose three.)

A) Support for different file formats (e.g., CSV, JSON, Parquet)  
B) Ability to handle schema evolution or changes in source data  
C) Whether data transformation is required during the load  
D) The personal preference of the data analyst  
E) The marketing department’s opinion on the loading tool  

**Correct Answers:** A, B, C

---

Question 253. A healthcare company is evaluating different ELT tools to load patient records into Snowflake. Which three considerations should the architect prioritize when selecting an ELT method? (Choose three.)

A) Support for incremental data loading and change data capture  
B) Ability to transform data during or after the load process  
C) Integration with existing data sources and enterprise security policies  
D) The color scheme of the ELT tool’s interface  
E) The company’s preferred operating system  

**Correct Answers:** A, B, C

---

Question 254. An architect is planning a data pipeline for a retail chain, which requires automated nightly loads into Snowflake with complex transformations. What should the architect consider when choosing an ELT approach? (Choose three.)

A) Automation and scheduling capabilities  
B) Scalability to handle varying data volumes  
C) Flexibility to apply business logic as part of the transformation  
D) Whether the tool supports exporting to Excel  
E) The number of employees in the IT department  

**Correct Answers:** A, B, C

---

Question 255. A financial services firm is migrating legacy batch ETL jobs to a modern ELT process in Snowflake. What are the three most important factors to evaluate when designing the new data loading solution?

A) Compatibility with Snowflake’s native ingestion and transformation features  
B) Monitoring and error recovery capabilities  
C) Ability to orchestrate and chain multiple transformation steps  
D) The desktop wallpaper of the data engineer  
E) The preferred browser of end users  

**Correct Answers:** A, B, C

---

Question 256. A data architect wants to use Snowflake’s Search Optimization Service to speed up searches on semi-structured data stored in a VARIANT column. What limitation should they be aware of?

A) The Search Optimization Service does not support search optimization on semi-structured data types such as VARIANT, OBJECT, or ARRAY  
B) The service will automatically create materialized views for semi-structured columns  
C) All data types are supported equally for search optimization  
D) The service only works for data in external stages  

**Correct Answer:** A

---

Question 257. A financial analyst needs to optimize queries that use `LIKE '%pattern%'` on a text column. Can the Search Optimization Service improve performance for this type of query?

A) No, the Search Optimization Service does not optimize search performance for pattern matching queries using wildcards at the start of the pattern  
B) Yes, it fully supports all pattern matching queries  
C) Only for columns with numeric data types  
D) Only for clustered tables  

**Correct Answer:** A

---

Question 258. A Snowflake architect plans to use the Search Optimization Service to speed up searches on a small lookup table. What limitation might make this an inefficient choice?

A) The overhead and cost of the Search Optimization Service may outweigh the performance benefits for small tables  
B) The service cannot be used on any table smaller than 10GB  
C) The service is required for all tables in Snowflake  
D) The service requires manual refresh after each data load  

**Correct Answer:** A

---

Question 259. An architect tries to use the Search Optimization Service to accelerate searches on a table that is frequently updated throughout the day. What limitation should they consider regarding freshness of query results?

A) Search Optimization Service may have lag in indexing updates, so queries might not reflect the most recent changes immediately  
B) The service guarantees real-time indexing for all updates  
C) The service cannot be used on tables with frequent updates  
D) The service can only be refreshed once per day  

**Correct Answer:** A

---

Question 260. A retail company wants to apply the Search Optimization Service to a multi-cluster warehouse. What is a limitation they should understand?

A) The Search Optimization Service is applied at the table level, not at the warehouse level  
B) The service is only available for single-cluster warehouses  
C) It can only be used for external tables  
D) It is available only for tables with less than 1 million rows  

**Correct Answer:** A

---

Question 261. A Snowflake architect needs to assign SELECT privileges on a table named `SALES` in the `REPORTING` schema of the `FINANCE` database to a user. What must the architect do before granting the SELECT privilege on the table?

A) First grant USAGE privilege on the `FINANCE` database and `REPORTING` schema to the user  
B) Grant CREATE TABLE privilege on the schema  
C) Grant OWNERSHIP privilege on the database  
D) Grant MONITOR privilege on the table  

**Correct Answer:** A

---

Question 262. A financial services company uses Okta as its identity provider, but also supports username/password and key pair authentication for their client applications connecting to Snowflake. What is the best practice for authentication method priority?

A) Use federated authentication through Okta as the primary method, with key pair authentication as a fallback, and username/password as the last resort  
B) Always use username/password, then Okta, and finally key pair  
C) Use key pair authentication first for all users  
D) Use passwordless authentication only  

**Correct Answer:** A

---

Question 263. A Snowflake architect is designing a secure client application integration. The application supports Okta, key pair authentication, and username/password. How should the architect prioritize these authentication methods for best security and manageability?

A) Prioritize federated authentication (Okta), then key pair authentication, followed by username/password if others are unavailable  
B) Prioritize username/password, then Okta, and never allow key pair  
C) Use key pair authentication only  
D) Allow users to pick any method randomly  

**Correct Answer:** A

---

Question 264. A global enterprise is integrating several client applications with Snowflake, all of which support OAuth, external browser, Okta native authentication, key pair authentication, and password. According to best practice, which method should be given highest priority for application authentication?

A) Key Pair Authentication  
B) Password  
C) OAuth (Snowflake OAuth or External OAuth)  
D) Okta native authentication  
E) External browser  

**Correct Answer:** C

---

Question 265. A Snowflake architect is designing an authentication strategy for a multi-region application with multiple supported methods. What is the recommended order of priority for authentication methods according to Snowflake best practices?

A) Okta native authentication, External browser, Password, Key Pair Authentication, OAuth  
B) OAuth, External browser, Okta native authentication, Key Pair Authentication, Password  
C) Password, External browser, OAuth, Okta native authentication, Key Pair Authentication  
D) Key Pair Authentication, Password, Okta native authentication, OAuth, External browser  
E) External browser, OAuth, Okta native authentication, Password, Key Pair Authentication  

**Correct Answer:** B

---

Question 266. A Snowflake architect is implementing a monitoring solution that calls the Snowpipe `loadHistoryScan` endpoint multiple times per hour. What is the best practice to follow regarding the frequency of these calls?

A) Limit the frequency of calls to avoid exceeding rate limits and incurring unnecessary costs  
B) Call the endpoint as often as possible for real-time updates  
C) Only call the endpoint once per day  
D) Use multiple concurrent requests to maximize throughput  

**Correct Answer:** A

---

Question 267. A data engineer wants to retrieve Snowpipe load history for a large external stage. What is the recommended best practice when using the `loadHistoryScan` endpoint to ensure efficient and reliable data retrieval?

A) Specify a narrow time window for each scan to reduce response size and improve performance  
B) Request the entire history from the start of the stage’s existence in a single call  
C) Disable authentication for faster access  
D) Always use wildcard patterns for file selection  

**Correct Answer:** A

---

Question 268. A Snowflake architect is designing a monitoring solution for Snowpipe data ingestion. What is a recommended polling strategy when calling the `loadHistoryScan` endpoint to ensure robust, gap-free ingestion monitoring?

A) Read the last 10 minutes of history every 8 minutes to provide overlapping coverage  
B) Poll the endpoint every hour to reduce system load  
C) Only read the exact time window since the last poll to avoid duplicate data  
D) Read the entire history of the stage every time the endpoint is called  

**Correct Answer:** A

---

Question 269. A DevOps team sets `DATA_RETENTION_TIME_IN_DAYS = 7` at the database level for staging tables, but finds that some tables are only recoverable for 1 day. What is the most likely cause?

A) The `DATA_RETENTION_TIME_IN_DAYS` parameter was set to 1 at the table level, overriding the database setting  
B) The tables are in a different database  
C) The user does not have the RECOVER privilege  
D) The schema is set to transient  

**Correct Answer:** A

---

Question 270. Despite configuring a 7-day data retention at the database level, certain staging tables cannot be recovered after 1 day. Which scenario explains this behavior?

A) The staging tables have a lower retention period configured individually  
B) The database has not been refreshed  
C) Time Travel has been disabled for the account  
D) The warehouse is suspended  

**Correct Answer:** A

---

Question 271. The DevOps team discovers that some tables in the staging schema are unrecoverable after 1 day, even though the database parameter for data retention is set to 7 days. What could be causing this issue?

A) Table-level `DATA_RETENTION_TIME_IN_DAYS` settings override the database-level default if explicitly set  
B) The staging tables are not being updated  
C) The tables are permanent and do not support retention  
D) Only transient tables support data retention  

**Correct Answer:** A

---

Question 272. Company A wants to share data with Company B, but the data is located in two separate databases within Company A’s Snowflake account. What is the best approach to enable sharing of this data?

A) Create one or more secure views and combine them into a single share  
B) Create separate shares for each database, as a share can only reference objects from one database  
C) Grant direct table access to Company B’s account  
D) Export the data from both databases and upload it to Company B’s stage  

**Correct Answer:** B

---

Question 273. A Snowflake architect at Company A needs to share tables from two different databases with Company B, whose account is in the same region. What should the architect do?

A) Create two separate shares, one for each database, and provide access to Company B for both shares  
B) Copy all tables into a single database, then create a single share  
C) Use direct database grants to Company B  
D) Use external tables for sharing  

**Correct Answer:** A

---

Question 274. Company A’s data engineering team is trying to share data from two databases with Company B. Which statement reflects Snowflake’s best practice and technical limitation for data sharing?

A) Since a share can only reference objects from one database, Company A must create a separate share for each database  
B) Both databases can be included in a single share if they have the same owner  
C) A share can include objects from any number of databases in the account  
D) Data sharing is only possible if both companies use the same warehouse  

**Correct Answer:** A

---

Question 275. A Data Architect receives the error "Number of columns in file (15) does not match that of the corresponding table (14)" while loading CSV data with the COPY INTO statement. What is the best approach to resolve the error and ensure all fields are loaded into the table?

A) Alter the target table to add an additional column so the number of table columns matches the file  
B) Ignore the error and rerun the load  
C) Drop one column from the CSV file  
D) Use the ON_ERROR='SKIP_FILE' option to skip problematic files  

**Correct Answer:** A

---

Question 276. While attempting to load a CSV file with 15 columns into a table with 14 columns, a Data Architect encounters a column mismatch error. What should they do to successfully load all fields from the file?

A) Add a new column to the target table and reload the file  
B) Change the file format to JSON  
C) Set the FILE_FORMAT parameter to automatically drop the extra column  
D) Reduce the number of columns in the CSV file to 14  

**Correct Answer:** A

---

Question 277. A Data Architect is loading CSV data into Snowflake using COPY INTO and gets an error due to more columns in the file than in the target table. What is the recommended solution if every field in the CSV is needed for analysis?

A) Modify the table structure to include an additional column, then reload  
B) Use a WHERE clause in the COPY INTO statement  
C) Set the ON_ERROR parameter to 'CONTINUE'  
D) Ignore the extra column during loading  

**Correct Answer:** A

---

Question 278. A Snowflake architect wants to enable event-based automated data loading from an Amazon S3 bucket using Snowpipe. When is the INTEGRATION parameter required?

A) When configuring an external stage to use cloud messaging for automated Snowpipe ingestion  
B) When loading data manually using the Snowpipe REST API  
C) When using an internal Snowflake stage  
D) When loading data from a local file system  

**Correct Answer:** A

---

Question 279. A data engineer is setting up Snowpipe to automatically ingest files from Azure Blob Storage as soon as new files arrive. What must be included in the external stage definition to allow Snowpipe to receive event notifications from Azure?

A) The INTEGRATION parameter referencing a storage integration object  
B) The FILE_FORMAT parameter set to JSON  
C) A direct reference to the storage account credentials  
D) The ON_ERROR parameter set to CONTINUE  

**Correct Answer:** A

---

Question 280. Which scenario requires specifying the INTEGRATION parameter for Snowpipe?

A) When using an external stage with cloud event notifications for automatic loading  
B) When using COPY INTO to load data on demand  
C) When querying data from a permanent table  
D) When listing the files in an internal stage  

**Correct Answer:** A

---

Question 281. A data engineer uses a stream to track changes in a table but notices the stream has become stale after the table was truncated. What is the reason for this?

A) Truncating the source table invalidates the stream’s change tracking and causes it to become stale  
B) The table was not queried for a week  
C) The stream was dropped  
D) The table was renamed  

**Correct Answer:** A

---

Question 282. A Snowflake architect sets a table’s `DATA_RETENTION_TIME_IN_DAYS` to 2. After 5 days of not consuming the stream, it is marked as stale. Why did this occur?

A) The stream was not consumed within the table’s data retention window  
B) A new column was added to the table  
C) The stream’s name was changed  
D) The stream was used to track a view instead of a table  

**Correct Answer:** A

---

Question 283. During a schema migration, a table is replaced using `CREATE OR REPLACE TABLE`. What impact does this have on any existing streams on the table?

A) The stream becomes stale because the underlying table was replaced  
B) The stream automatically updates to follow the new table  
C) The stream continues tracking changes without interruption  
D) The stream is converted to a materialized view  

**Correct Answer:** A

---

Question 284. A financial services company discovers that the `Data` table in its Snowflake environment contains corrupted data due to a faulty ETL job. What is the most efficient command to recover the table to its state 5 minutes ago while preserving the original table for investigation?

A) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
C) DROP TABLE Data;  
D) SELECT * FROM Data WHERE date > current_timestamp() - interval '5 minutes';  

**Correct Answer:** B

---

Question 285. A healthcare organization needs to restore a table named `Data` to its state as of 5 minutes ago after a user mistakenly updated all rows. Which command should the architect use to overwrite the current table with its previous state?

A) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
B) UPDATE Data SET ... ;  
C) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
D) ALTER TABLE Data SET DATA_RETENTION_TIME_IN_DAYS = 1;  

**Correct Answer:** C

---

Question 286. A retail company wants to compare the current corrupted data in the `Data` table with its state 5 minutes ago. What is the best approach to achieve this using Snowflake features?

A) Use CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60); to create a copy for analysis  
B) Use DROP TABLE Data; and reload from backup  
C) Use SELECT * FROM Data; only  
D) Use GRANT SELECT ON Data TO ANALYST;  

**Correct Answer:** A

---

Question 287. During a system audit, a logistics company identifies data corruption in the `Data` table. The analyst is tasked with restoring the table to its exact state 5 minutes prior using Snowflake’s Time Travel feature. Which command should be executed?

A) SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
C) DELETE FROM Data WHERE timestamp < current_timestamp() - interval '5 minutes';  
D) CREATE TABLE Data AS CLONE Data AT (OFFSET => -5*60);  

**Correct Answer:** B

---

Question 288. A manufacturing company’s data engineering team wants to investigate a table corruption incident without impacting ongoing operations. What is the safest way to obtain a snapshot of the `Data` table as it was 5 minutes ago?

A) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
B) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data;  
C) SELECT * FROM Data WHERE timestamp > current_timestamp() - interval '5 minutes';  
D) TRUNCATE TABLE Data;  

**Correct Answer:** A

---

Question 289. A retail company wants to export a 5 GB table from Snowflake to CSV as quickly as possible for downstream analytics. What is the most performant method the architect should use?

A) Use the COPY INTO command and set a large SPLIT_SIZE to maximize parallelism  
B) Use SELECT * FROM table and download the results via the web interface  
C) Use COPY INTO with compression disabled  
D) Use INSERT INTO to copy data to a stage and export manually  

**Correct Answer:** A

---

Question 290. A Snowflake Architect is configuring an extract job to unload 5 GB of sales data as CSV. To take advantage of parallel operations and maximize performance, what should they do regarding the MAX_FILE_SIZE parameter?

A) Set MAX_FILE_SIZE to a very large value to minimize the number of output files  
B) Leave MAX_FILE_SIZE at its default value to allow Snowflake to split the output into multiple 16 MB files for parallel writing  
C) Set MAX_FILE_SIZE to 1 MB for smaller files  
D) Set MAX_FILE_SIZE to 5 GB to create a single output file  

**Correct Answer:** B

---

Question 291. A data architect at a logistics company needs to regularly unload large tables to CSV for reporting. What Snowflake feature allows the export process to be distributed and completed faster?

A) Snowflake automatically splits output into multiple files and writes them in parallel when using COPY INTO  
B) Data must be unloaded using a single thread for consistency  
C) Export jobs require manual file segmentation after unloading  
D) All output files must be merged after unloading to improve performance  

**Correct Answer:** A

---

Question 292. A Snowflake Architect is creating a read-only role for employees in the human resources department. Which set of permissions ensures users with this role can only view data in `hr_db` and cannot modify it?

A) GRANT USAGE ON DATABASE hr_db; GRANT USAGE ON SCHEMA hr_db.public; GRANT SELECT ON ALL TABLES IN SCHEMA hr_db.public;  
B) GRANT OWNERSHIP ON DATABASE hr_db;  
C) GRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA hr_db.public;  
D) GRANT ALL PRIVILEGES ON DATABASE hr_db;  

**Correct Answer:** A

---

Question 293. A company wants to prevent certain employees in the human resources department from making changes to the `hr_db` database, ensuring they can only view employee data. Which permissions should the architect grant to the HR read-only role?

A) USAGE and SELECT privileges on the relevant database, schema, and tables  
B) INSERT and UPDATE privileges on hr_db  
C) USAGE and DELETE privileges on all tables in hr_db  
D) OWNERSHIP privilege on hr_db  

**Correct Answer:** A

---

Question 294. An Architect must allow HR staff to run queries against employee data in `hr_db` while restricting them from editing or deleting records. Which combination of privileges should be granted to the HR read-only role?

A) USAGE on hr_db and its schema, plus SELECT on all tables  
B) CREATE TABLE and SELECT on hr_db  
C) USAGE on hr_db and INSERT on all tables  
D) USAGE on hr_db and UPDATE on all tables  

**Correct Answer:** A

---

Question 295. A fintech company wants to connect its AWS VPC directly to Snowflake without routing traffic over the public internet. Which security feature must be enabled to accomplish this?

A) AWS PrivateLink  
B) SSL encryption  
C) Snowflake Network Policy  
D) SAML authentication  

**Correct Answer:** A

---

Question 296. An architect is tasked with ensuring secure, private connectivity between an AWS VPC and Snowflake, preventing exposure of data to the public internet. What is the required security feature?

A) Configure AWS PrivateLink for Snowflake  
B) Set up a VPN tunnel between VPC and Snowflake  
C) Restrict traffic using AWS Security Groups  
D) Enable IP whitelisting in Snowflake  

**Correct Answer:** A

---

Question 297. A healthcare company must comply with strict data privacy regulations and needs to connect its AWS-hosted analytics environment directly to Snowflake. Which feature should be used to meet this requirement?

A) AWS PrivateLink  
B) Multi-factor authentication  
C) Snowflake role-based access control  
D) AWS IAM roles  

**Correct Answer:** A
Question 297. A department is running slow aggregation queries on a massive table, primarily filtering and grouping by a small subset of columns. The table is not currently clustered. What is the most optimal solution to improve query performance?

A) Define a cluster key on the columns used for filtering and grouping  
B) Increase the warehouse size for all queries  
C) Create a materialized view on the table  
D) Partition the table using manual sharding  

**Correct Answer:** A

---

Question 298. An analytics team is experiencing long runtimes for queries on a huge table because the data isn’t organized according to the columns used in search predicates. What should the architect do to optimize query performance for this use case?

A) Create a cluster key on the columns most frequently used in query filters  
B) Grant more permissions to the users  
C) Use row-level security policies  
D) Move the table to a different schema  

**Correct Answer:** A

---

Question 299. After noticing that queries are slow due to lack of clustering on relevant columns, what is Snowflake’s recommended solution to optimize performance for aggregation queries on a large table?

A) Implement clustering on the columns most often used in WHERE and GROUP BY clauses  
B) Drop and recreate the table  
C) Only run queries during off-peak hours  
D) Disable automatic clustering  

**Correct Answer:** A

---

Question 300. A data engineer needs to diagnose why a scheduled Snowflake task failed overnight. Which function should they use to retrieve information about past task executions, including status and errors?

A) TASK_HISTORY  
B) SYSTEM$TASK_STATUS  
C) INFORMATION_SCHEMA.QUERY_HISTORY  
D) SHOW TASKS  

**Correct Answer:** A

---

Question 301. A Snowflake Architect wants to analyze trends and durations of task runs for a specific data pipeline over the last week. Which approach allows them to obtain this information?

A) Query the TASK_HISTORY table function for the specific task and time period  
B) Use the SHOW TABLES command  
C) Check the WAREHOUSE_HISTORY function  
D) Review the stage history using STAGE_HISTORY  

**Correct Answer:** A

---

Question 302. A business analyst is tasked with auditing how frequently a certain Snowflake task has run and whether any executions failed recently. What is the most appropriate method to obtain this information?

A) Use the TASK_HISTORY function to list executions and statuses  
B) Query the INFORMATION_SCHEMA.TABLES view  
C) Check the stage for loaded files  
D) Use the COPY_HISTORY function  

**Correct Answer:** A

---

Question 303. A Snowflake architect creates a database with `DATA_RETENTION_TIME_IN_DAYS=30` and a schema within it with `DATA_RETENTION_TIME_IN_DAYS=50`. If the database is dropped, how long is the data in schema S1 available using Time Travel?

A) 50 days  
B) 30 days  
C) 1 day  
D) 0 days  

**Correct Answer:** B

---

Question 304. After dropping a database with a retention period of 30 days, but a contained schema has a retention period of 50 days, for how long can objects in the dropped schema be accessed via Time Travel?

A) The retention period set at the database level, 30 days  
B) The retention period set at the schema level, 50 days  
C) Until the end of the current day  
D) Indefinitely  

**Correct Answer:** A

---

Question 305. If a database with `DATA_RETENTION_TIME_IN_DAYS=30` and a schema with `DATA_RETENTION_TIME_IN_DAYS=50` is dropped, what is the maximum period that Time Travel can be used to recover data from the schema?

A) 7 days  
B) 30 days  
C) 50 days  
D) 24 hours  

**Correct Answer:** B

---

Question 306. A Snowflake Architect wants to simplify the management of credentials for several external stages pointing to different S3 buckets. What is the best way to accomplish this?

A) Create a storage integration and reference it in each stage  
B) Store credentials in each stage definition individually  
C) Use a VPN to connect to each bucket  
D) Use a user-defined function to manage access  

**Correct Answer:** A

---

Question 307. A company needs to securely automate Snowpipe data ingestion from multiple Azure Blob Storage containers. What key feature does a storage integration enable to support this business requirement?

A) Centralized credential management and support for multiple external stages  
B) Data masking for sensitive columns  
C) Automatic data compression during ingestion  
D) Creating clustered tables automatically  

**Correct Answer:** A

---

Question 308. An architect needs to ensure that only authorized Snowflake users can access specific files in Google Cloud Storage, and wants to easily audit such access. What is an appropriate use of a storage integration?

A) Use storage integration to set up least-privilege access and enable auditing  
B) Grant direct file access to all Snowflake users  
C) Store access keys in a local file  
D) Use row-level security on all tables  

**Correct Answer:** A

---

Question 309. A business wants to reuse a single set of cloud storage credentials across several stages for ease of management and compliance. Which Snowflake feature allows this?

A) Storage integration  
B) Materialized view  
C) Virtual warehouse  
D) Network policy  

**Correct Answer:** A

---

Question 310. A Snowflake Architect is evaluating options to improve query performance on a large table. The architect is considering either clustering the table or enabling search optimization. What is the primary difference between these approaches in terms of costs?

A) Clustering incurs ongoing maintenance costs as Snowflake reorganizes data, while search optimization incurs a fixed upfront cost only  
B) Clustering incurs ongoing maintenance costs, whereas search optimization incurs recurring storage costs for search access paths  
C) Both clustering and search optimization incur only one-time costs when enabled  
D) Clustering is free, while search optimization is a paid feature  

**Correct Answer:** B

---

Question 311. A financial institution wants to enable SSO for its Snowflake users using its corporate identity provider. Which of the following vendors is natively supported by Snowflake for federated authentication?

A) Okta  
B) Auth0  
C) OneLogin  
D) JumpCloud  

**Correct Answer:** A

---

Question 312. A Snowflake Architect is tasked with setting up federated authentication using SAML 2.0. Which identity provider can be configured natively in Snowflake for this purpose?

A) Microsoft Azure Active Directory  
B) Duo Security  
C) Centrify  
D) IBM Tivoli  

**Correct Answer:** A

---

Question 313. A healthcare company wants to use Ping Identity to provide SSO access to Snowflake for its employees. Is Ping Identity natively supported by Snowflake for federated authentication?

A) No, only Google Workspace is supported  
B) Yes, Ping Identity is natively supported  
C) No, only Okta and Azure AD are supported  
D) Yes, but only with OAuth2  

**Correct Answer:** B

---

Question 314. A company with multiple regional offices needs to ingest data into Snowflake from cloud storage locations in each region. What Snowflake feature should be used to securely connect these external sources?

A) Storage integration  
B) Materialized view  
C) Row access policy  
D) Data masking  

**Correct Answer:** A

---

Question 315. An architect is asked to set up a solution that allows data ingestion from many geographically dispersed sources into Snowflake, while minimizing credential sprawl. What is the recommended approach?

A) Create external stages in each region and reference a storage integration for secure access  
B) Grant direct access to the Snowflake account for all users in each region  
C) Use a virtual warehouse for each region  
D) Configure network policies for each region  

**Correct Answer:** A

---

Question 316. A global enterprise needs to automate data loading from different cloud storage locations representing various company sites into Snowflake. Which configuration enables this type of data ingestion?

A) Use storage integrations to securely connect external stages from each site  
B) Create a single table with region metadata  
C) Use masking policies for each region  
D) Grant ownership privileges to all site managers  

**Correct Answer:** A

---

Question 317. A data engineering team needs to load large datasets from Spark to Snowflake with minimal impact on their Spark cluster’s resources. Which Snowflake Spark connector transfer mode should they use?

A) External transfer mode  
B) Internal transfer mode  
C) Direct JDBC mode  
D) Manual CSV export  

**Correct Answer:** A

---

Question 318. While using the Snowflake Spark connector, a team wants all data transferred between Spark and Snowflake to happen over JDBC with no use of cloud storage. Which transfer mode accomplishes this?

A) Internal transfer mode  
B) External transfer mode  
C) Snowpipe mode  
D) Bulk copy mode  

**Correct Answer:** A

---

Question 319. A company is ingesting terabytes of data daily from Spark into Snowflake and needs to optimize for speed and scalability. What is a key feature of the external transfer mode in the Snowflake Spark connector?

A) Data is staged in cloud storage before being loaded to Snowflake  
B) Data is transferred directly via JDBC  
C) No intermediate storage is used  
D) Only small datasets are supported  

**Correct Answer:** A

---

Question 320. A Snowflake Architect is configuring data integration between Spark and Snowflake. What is the main difference between internal and external transfer modes in the Snowflake Spark connector?

A) Internal mode uses JDBC for direct data transfer, while external mode stages data in cloud storage for bulk loading  
B) Both modes always use cloud storage  
C) Internal mode uses Snowpipe for loading data  
D) External mode does not support parallel data loads  

**Correct Answer:** A

---

Question 321. A Snowflake architect notices that the database `shared_database` has been created in a consumer account via secure data sharing. Which operation can be performed on this database?

A) Query tables and views in the shared database  
B) Create new schemas in the shared database  
C) Drop objects from the shared database  
D) Grant privileges on the shared database to other accounts  

**Correct Answer:** A

---

Question 322. A user in a consumer account wants to modify the structure of a shared database named `shared_database` they received from a provider. Which of the following operations is permitted?

A) Query data in tables and views  
B) Create tables in the shared database  
C) Alter table structures in the shared database  
D) Delete schemas from the shared database  

**Correct Answer:** A

---

Question 323. An analyst is trying to insert data into a table within the `shared_database` that was shared with their Snowflake account. What is the result of this operation?

A) The operation will fail, as DML is not permitted on shared databases  
B) The data will be inserted successfully  
C) The analyst can alter the table structure before inserting  
D) The operation will succeed only if the analyst has the OWNERSHIP privilege  

**Correct Answer:** A

---

Question 324. An Architect has created an external stage in Snowflake pointing to an Amazon S3 bucket. Which feature requires setting up an AWS SNS topic to enable auto-refresh of staged files?

A) Snowpipe  
B) Manual COPY INTO command  
C) Table clustering  
D) External table creation  

**Correct Answer:** A

---

Question 325. A Snowflake Architect is configuring continuous, automated data loading from an S3 bucket. What role does an SNS topic play in this setup?

A) It sends notifications to Snowflake when new files are added to the bucket, triggering auto-refresh and data ingestion via Snowpipe  
B) It encrypts files before loading  
C) It manages user permissions for the S3 bucket  
D) It provides metadata about the bucket’s contents  

**Correct Answer:** A

---

Question 326. What is an AWS SNS topic in the context of Snowflake data ingestion from S3?

A) A messaging channel used to notify Snowflake when new files are available for automated loading  
B) A type of S3 bucket policy  
C) A user role for accessing the S3 bucket  
D) An encryption key for securing S3 files  

**Correct Answer:** A

---

Question 327. An architect is setting up a Snowpipe for automated data ingestion using the `CREATE PIPE ... AS COPY INTO` command. Which of the following copy options can be specified in this command?

A) FILE_FORMAT  
B) ON_ERROR  
C) VALIDATION_MODE  
D) ALL OF THE ABOVE  

**Correct Answer:** D

---

Question 328. A data engineer needs to configure a pipe in Snowflake to handle errors gracefully during file ingestion. Which COPY INTO option is supported in the `CREATE PIPE ... AS COPY INTO` statement to achieve this?

A) ON_ERROR  
B) PURGE  
C) COPY_GRANTS  
D) FORCE  

**Correct Answer:** A

---

Question 329. A company wants to perform data validation before loading files with Snowpipe. Which copy option is supported by the `CREATE PIPE ... AS COPY INTO` command to facilitate this requirement?

A) VALIDATION_MODE  
B) PATTERN  
C) ENCRYPTION  
D) TRUNCATECOLUMNS  

**Correct Answer:** A

---

Question 330. A Data Architect is setting up an API integration for external functions in Snowflake. What should be verified regarding network connectivity?

A) The integration’s allowed network policies enable Snowflake to access the remote API endpoint  
B) The API endpoint is hosted in the same cloud region as Snowflake  
C) The API endpoint uses HTTP only  
D) The API endpoint is accessible only from private IPs  

**Correct Answer:** A

---

Question 331. When configuring a Snowflake API integration for external functions, which security consideration is most important?

A) The remote API endpoint requires TLS (HTTPS) for secure communication  
B) The API endpoint supports multiple languages  
C) The API endpoint has no authentication  
D) The API integration uses hardcoded credentials  

**Correct Answer:** A

---

Question 332. A data architect needs to ensure that external functions in Snowflake can scale with high volumes of requests. What should be considered?

A) The remote service can handle concurrent requests and is highly available  
B) The remote service is hosted on a desktop machine  
C) The remote API endpoint is rate-limited for individual users  
D) The API integration is only used for batch jobs  

**Correct Answer:** A

---

Question 333. Before creating an external function using an API integration, what is an important configuration aspect to check in Snowflake?

A) The API integration has been granted the appropriate privileges and is active in the correct Snowflake account  
B) The API integration is named after the database  
C) The API integration uses a free API endpoint  
D) The integration only supports GET requests  

**Correct Answer:** A

---

Question 334. A Data Architect is preparing to create an API integration for external functions. Which role consideration is most important before starting the configuration?

A) The API integration must be created using the ACCOUNTADMIN or a role with the CREATE INTEGRATION privilege  
B) Any user can create an API integration without specific privileges  
C) Only the PUBLIC role can be used for API integrations  
D) The API integration does not require any role-based permissions  

**Correct Answer:** A

---

Question 335. When configuring an API integration for external functions, who should have the privilege to create or manage the integration object?

A) Only users with the necessary privileges, such as SECURITYADMIN or ACCOUNTADMIN  
B) All users in the organization  
C) Only users with the PUBLIC role  
D) Any user, regardless of privilege  

**Correct Answer:** A

---

Question 336. A Data Architect wants to restrict who can use an external function backed by an API integration. What should be considered regarding roles?

A) Grant usage privileges on the API integration and external function only to specific roles  
B) Always allow the PUBLIC role to use all integrations  
C) Do not assign usage privileges to any role  
D) Assign privileges to individual users only, not roles  

**Correct Answer:** A

---

Question 337. Before creating an external function, what is an important step for a Data Architect to take regarding Snowflake roles and privileges?

A) Ensure that the necessary roles have been granted the appropriate privileges to create, manage, and use the API integration and external functions  
B) Ignore role assignments when configuring integrations  
C) Assign privileges only after the function has been used  
D) Use a deprecated role for integration configuration  

**Correct Answer:** A

---

Question 338. A data warehouse contains a large fact table with billions of rows. Analysts frequently run queries filtering on a specific date column. When should the architect consider adding a clustering key to this table?

A) When queries often filter on the same column(s) and table size is large  
B) When the table is very small and rarely queried  
C) When only SELECT * queries are run  
D) When table is used only for staging data  

**Correct Answer:** A

---

Question 339. A business intelligence team is experiencing slow performance when querying a large table on specific columns. What scenario suggests adding a clustering key?

A) When repeated queries filter or join on the same column(s)  
B) When the table has fewer than 1,000 rows  
C) When the table is dropped frequently  
D) When queries only aggregate all rows  

**Correct Answer:** A

---

Question 340. A Snowflake Architect is designing a table expected to grow rapidly over time. The table will be queried using filters on a customer_id column. When should a clustering key be considered?

A) When query filters or sorting are frequently applied to specific columns  
B) When the table is write-only  
C) When there is no need for performance optimization  
D) When the table is used for temporary storage only  

**Correct Answer:** A

---

Question 341. A company receives thousands of very small JSON files from IoT devices every hour and stores them in cloud storage. What is the most cost-effective way to load this data into Snowflake?

A) Use Snowpipe to incrementally ingest files as they arrive  
B) Manually run COPY INTO commands for each file  
C) Load files individually using the web interface  
D) Use a high-capacity virtual warehouse for batch loading  

**Correct Answer:** A

---

Question 342. An architect is designing a solution to ingest 1,000 small JSON files per hour from cloud storage into Snowflake. Which approach minimizes cost and operational overhead?

A) Configure Snowpipe for automated, continuous ingestion  
B) Schedule frequent batch jobs using large warehouses  
C) Use the PUT command for each file  
D) Load data via the Snowflake UI  

**Correct Answer:** A

---

Question 343. A data engineer is tasked with moving numerous tiny JSON files from cloud storage into a Snowflake table. Which ingestion method is most cost-effective for this workload?

A) Snowpipe with auto-ingest from cloud storage  
B) Large-scale batch COPY INTO jobs  
C) Manual uploads with the Snowflake web UI  
D) Real-time ingestion using external functions  

**Correct Answer:** A

---

Question 344. A data engineer receives a successful response after submitting files to the `insertFiles` Snowpipe REST endpoint. What does this indicate?

A) The files have been accepted and queued for ingestion by Snowflake  
B) The files have already been loaded into the target table  
C) The files have been deleted from the stage  
D) The endpoint returned a summary of the loaded data  

**Correct Answer:** A

---

Question 345. After calling the `insertFiles` endpoint for Snowpipe, an architect sees a success message. What does this confirm?

A) Snowflake has received the files and will attempt to load them  
B) The files are immediately available in the destination table  
C) The files have failed to queue for ingestion  
D) The files were rejected due to format errors  

**Correct Answer:** A

---

Question 346. After enforcing network policies to restrict access to Private Link IP ranges in a Snowflake Azure account, users receive an error stating their IP is not allowed when logging in via SSO. What should the Architect check first to resolve this?

A) Ensure the SSO identity provider (ADFS) metadata service is also allowed by the network policy  
B) Disable SSO and use password authentication only  
C) Increase the allowed IP range to include public internet addresses  
D) Switch to OAuth authentication  

**Correct Answer:** A

---

Question 347. An Architect has confirmed that Private Link connectivity is working for direct logins, but SSO logins fail due to IP restrictions. What is a recommended next step?

A) Add the IP address ranges used by the SSO identity provider’s services to the Snowflake network policy  
B) Remove all network policies  
C) Add a new virtual warehouse  
D) Change DNS settings to point to public endpoints  

**Correct Answer:** A

---

Question 348. A company’s Snowflake account in Azure is set up with SAML SSO and Private Link. Users can log in with username/password, but receive “IP not allowed” errors with SSO. Which action should the Architect consider?

A) Update the network policy to include the IP addresses used by the SSO/SCIM provider and Azure AD Connect services  
B) Delete the network policy  
C) Use a different SSO provider  
D) Change the Private Link configuration to public access  

**Correct Answer:** A

---

Question 349. A company uses the Snowflake Connector for Kafka to ingest data into a Snowflake table. What will happen if a file generated by the connector cannot be loaded?

A) The connector will retry loading the file until it succeeds or reaches the maximum retry limit  
B) The file will be deleted and the data lost  
C) The connector will ignore the file and move on  
D) A manual intervention is required for each failed file  

**Correct Answer:** A

---

Question 350. While streaming data using the Snowflake Connector for Kafka, a file fails to load into Snowflake due to schema mismatch. What is the connector’s default behavior?

A) The connector retries the load operation according to its configuration  
B) The connector drops the failed file immediately  
C) The connector moves the file to a backup location  
D) No error is logged for failed loads  

**Correct Answer:** A

---

Question 351. An architect is troubleshooting ingestion issues with the Snowflake Connector for Kafka. If a file cannot be loaded into the Snowflake stage, what happens next?

A) The connector retries the operation, and if retries fail, it logs the error for further investigation  
B) The connector automatically deletes the file  
C) The connector marks the Kafka topic as completed  
D) The connector sends an alert to all users  

**Correct Answer:** A

---

Question 352. A data engineer is designing a real-time streaming solution for ingesting large volumes of data from various sources. Which technology is best described as a distributed event streaming platform for building real-time data pipelines and applications?

A) Kafka  
B) MySQL  
C) Hadoop  
D) Tableau  

**Correct Answer:** A

---

Question 353. A company wants to implement a scalable system that can publish and subscribe to streams of records in real time for analytics. Which of the following technologies fulfills this role?

A) Kafka  
B) Excel  
C) FTP  
D) SFTP  

**Correct Answer:** A

---

Question 354. A SYSADMIN creates several database objects and transfers OWNERSHIP to a custom role. The custom role is not in the SYSADMIN or SECURITYADMIN role hierarchy. What is the impact on SYSADMIN’s ability to manage those objects?

A) SYSADMIN loses the ability to manage the objects via ownership  
B) SYSADMIN can still drop and alter the objects  
C) SYSADMIN can grant itself access at any time  
D) SYSADMIN retains ownership regardless of hierarchy  

**Correct Answer:** A

---

Question 355. A custom role is granted OWNERSHIP of database objects, but is not assigned to SYSADMIN or SECURITYADMIN. What happens if SYSADMIN tries to modify privileges on these objects?

A) SYSADMIN will be unable to modify privileges or perform privileged operations  
B) SYSADMIN can always update privileges on any object  
C) SYSADMIN can forcibly take ownership at any time  
D) SYSADMIN can drop the objects without ownership  

**Correct Answer:** A

---

Question 356. A Snowflake Architect wants to understand the effect of transferring object ownership to a custom role outside the SYSADMIN/SECURITYADMIN hierarchy. What is the main consequence for those administrative roles?

A) They lose ownership and cannot manage the objects unless the custom role is added to their hierarchy  
B) They retain full management capabilities  
C) They can grant themselves privileges on the objects  
D) They can view but not modify the objects  

**Correct Answer:** A

---

Question 357. A data engineer uses a stream to track changes in a table but notices the stream has become stale after the table was truncated. What is the reason for this?

A) Truncating the source table invalidates the stream’s change tracking and causes it to become stale  
B) The table was not queried for a week  
C) The stream was dropped  
D) The table was renamed  

**Correct Answer:** A

---

Question 358. A financial services company discovers that the Data table in its Snowflake environment contains corrupted data due to a faulty ETL job. What is the most efficient command to recover the table to its state 5 minutes ago while preserving the original table for investigation?

A) CREATE OR REPLACE TABLE Data AS SELECT * FROM Data AT (OFFSET => -5*60);  
B) CREATE TABLE Data_clone CLONE Data AT (OFFSET => -5*60);  
C) DROP TABLE Data;  
D) SELECT * FROM Data WHERE date > current_timestamp() - interval '5 minutes';  

**Correct Answer:** B

---

Question 359. A data architect is building a monitoring solution for a high-volume Snowpipe implementation. The business wants to minimize API calls and ensure scalable tracking of file loading status. Which endpoint should be used, and what is the main benefit?

A) loadHistoryScan, because it allows for real-time data manipulation  
B) insertReport, because it provides efficient, scalable tracking of file load status with fewer API calls  
C) insertFiles, because it automatically triggers file ingestion  
D) insertHistory, because it gives historical data for all loads  

**Correct Answer:** B

---

Question 360. A company’s ETL solution checks whether files have been loaded into Snowflake using Snowpipe. The architect is concerned about API rate limits and performance. What is the recommended approach to optimize system load?

A) Periodically calling loadHistoryScan for each individual file  
B) Using insertFiles to batch file uploads  
C) Using insertReport, because it reduces the number of API calls required for load monitoring  
D) Implementing a custom notification system outside Snowflake  

**Correct Answer:** C

---

Question 361. During a Snowflake implementation, a business wants to optimize their Snowpipe monitoring strategy for cost and performance. What is a key benefit of using the insertReport endpoint over loadHistoryScan?

A) loadHistoryScan is required for external table refresh  
B) loadHistoryScan allows direct data manipulation in the stage  
C) insertReport provides batch status for multiple files in a single call, improving efficiency  
D) insertReport increases operational costs due to excessive API usage  

**Correct Answer:** C

---

Question 362. An Architect needs to grant a group of ORDER_ADMIN users the ability to clean old data in an ORDERS table (deleting all records older than 5 years), without granting any privileges on the table. The group’s manager (ORDER_MANAGER) has full DELETE privileges on the table.  
How can the ORDER_ADMIN role be enabled to perform this data cleanup, without needing the DELETE privilege held by the ORDER_MANAGER role?

A) Grant the ORDER_ADMIN role the DELETE privilege directly on the ORDERS table  
B) Create a stored procedure owned by ORDER_MANAGER that performs the cleanup, and grant EXECUTE on the procedure to ORDER_ADMIN  
C) Assign the ORDER_MANAGER role to ORDER_ADMIN users during the cleanup  
D) Share the table with ORDER_ADMIN using a secure view and allow deletes through the view  

**Correct Answer:** B

---

Question 363. A retailer schedules a daily Snowflake task to run at 2:00 AM local time. After daylight savings time ends, the business notices a shift in execution timing. How does Snowflake handle task scheduling with respect to daylight savings changes?

A) The task automatically adjusts to UTC and ignores local time  
B) The task continues to run at the defined local time, adapting to daylight savings changes  
C) The task skips execution on the day the time changes  
D) The task fails and requires manual rescheduling after time changes  

**Correct Answer:** B

---

Question 364. A global company operates Snowflake tasks scheduled for local time. When daylight savings time begins, what is the expected behavior for these tasks?

A) Tasks are paused until the administrator updates the schedule  
B) Tasks run at the same designated local time, regardless of daylight savings adjustments  
C) Tasks may run twice on the day of the change  
D) Tasks switch to a fixed UTC time and no longer represent local time  

**Correct Answer:** B

---

Question 365. An architect is asked to ensure Snowflake tasks always run at 1:30 AM local time, even during daylight savings transitions. What does Snowflake do when daylight savings time shifts?

A) The task execution time becomes inconsistent and may drift  
B) The task runs at the same local time, automatically adjusting for daylight savings changes  
C) The task executes based on server time, not user-configured time  
D) The task needs to be manually adjusted each time daylight savings starts or ends  

**Correct Answer:** B

---

Question 366. A retail company schedules a daily Snowflake task to run at 3:00 AM local time. After daylight savings ends, the business notices a change in execution time. How does Snowflake handle this shift in local time for scheduled tasks?

A) The task keeps running at the same UTC time, ignoring local time changes  
B) The task automatically adjusts and runs at the scheduled local time, accounting for daylight savings changes  
C) The task fails and requires manual rescheduling after the time change  
D) The task skips execution on the day of the change  

**Correct Answer:** B

---

Question 367. A multinational organization operates Snowflake tasks scheduled for local time. When daylight savings begins, what is the expected behavior for these tasks?

A) Tasks switch to a fixed UTC schedule and no longer represent local time  
B) Tasks are paused until the administrator updates the schedule  
C) Tasks continue to run at the same designated local time, adapting automatically for daylight savings  
D) Tasks may run twice on the day of the change  

**Correct Answer:** C

---

Question 368. An architect is asked to ensure Snowflake tasks always run at 1:30 AM local time, even during daylight savings transitions. What will Snowflake do when daylight savings time shifts?

A) The task execution time becomes inconsistent and may drift  
B) The task executes based on server time, not the user-configured local time  
C) The task needs to be manually adjusted each time daylight savings starts or ends  
D) The task runs at the same local time, automatically adjusting for daylight savings changes  

**Correct Answer:** D

---

Question 369. A media company wants to continuously ingest customer review data into Snowflake, perform sentiment analysis using Amazon Comprehend, de-identify the data, and share it publicly for advertising companies on different cloud providers. Which design best minimizes operational complexity and infrastructure maintenance?

A) Build a custom ETL pipeline using self-managed EC2 instances and schedule data loads  
B) Use Snowpipe for continuous ingestion, external functions to call Amazon Comprehend, and secure views for public data sharing  
C) Use third-party ETL tools hosted on-premises for ingestion and transformation  
D) Set up a dedicated Kubernetes cluster to manage ingestion, transformation, and sharing  

**Correct Answer:** B

---

Question 370. A Snowflake architect must design a solution for ingesting data triggered by object storage event notifications, with minimal platform management and easy integration with external sentiment analysis. Which approach should be recommended?

A) Build and manage a serverless Lambda pipeline with custom code for data movement  
B) Leverage Snowpipe for event-driven ingestion, Snowflake external functions for Amazon Comprehend, and secure data sharing for cross-cloud access  
C) Use cron jobs to batch load and process reviews  
D) Create manual upload processes for each advertising client  

**Correct Answer:** B

---

Question 371. To ensure efficiency and scalability as new customer reviews arrive, while minimizing development and upgrade effort, what Snowflake feature should be used for ingesting data from object storage?

A) Manually run ETL scripts periodically  
B) Snowpipe, which supports continuous and automated data ingestion triggered by event notifications  
C) Streamlit app for direct data upload  
D) ODBC connection with polling  

**Correct Answer:** B

---

Question 372. How many files can the COPY INTO operation load as the maximum when providing a discrete list of files?

A) 500  
B) 1000  
C) 2000  
D) 250  

**Correct Answer:** B

---

Question 373. When data is transferred from a Snowflake primary account to another target account using database replication, which account is billed for the data transfer and compute charges?

A) Both the primary and target accounts equally  
B) The target account  
C) The primary account  
D) Neither account; charges are covered by Snowflake  

**Correct Answer:** C

---

Question 374. A Data Architect is importing customer data in JSON format from an external stage into a Snowflake table with a VARIANT column. Occasionally, malformed JSON causes parsing errors, leading to failed imports. What function should the Architect use in the COPY INTO command to set the column to NULL when a parsing error occurs?

A) TRY_TO_DATE  
B) TRY_PARSE_JSON  
C) TO_VARIANT  
D) PARSE_JSON  

**Correct Answer:** B

Question 375. During a Snowflake migration project, an Architect needs to ensure that any VARIANT column in a target table is set to NULL if the source JSON data is malformed during a bulk COPY INTO operation. Which function provides this capability?

A) TO_JSON  
B) TO_OBJECT  
C) TRY_PARSE_JSON  
D) TO_VARIANT  

**Correct Answer:** C

---

Question 376. A media company’s Data Architect wants to avoid import failures caused by malformed JSON when loading review data from cloud storage. The solution should automatically set the VARIANT column to NULL for any rows with parsing errors. Which function should be used in the data pipeline?

A) PARSE_JSON  
B) TO_VARIANT  
C) TO_OBJECT  
D) TRY_PARSE_JSON  

**Correct Answer:** D

---

Question 377. A Data Architect is importing JSON data from an external stage into a table with a VARIANT column using the COPY INTO command. During testing, the Architect discovers that the import sometimes fails, with parsing errors, due to malformed JSON values. The Architect decides to set the VARIANT column to NULL when a parsing error is encountered.

A) TO_VARIANT  
B) TRY_PARSE_JSON  
C) TO_OBJECT  
D) PARSE_JSON  

**Correct Answer:** B

---

Question 378. Which pipes are cloned when cloning a database or schema?

A) Only pipes with active data streams  
B) Pipes whose stage references are also cloned in the same operation  
C) All pipes, regardless of stage references  
D) No pipes are cloned during database or schema cloning  

**Correct Answer:** B

---

Question 379. WA company needs to have the following features available in its Snowflake account:

1. Support for Multi-Factor Authentication (MFA)  
2. A minimum of 2 months of Time Travel availability  
3. Database replication in between different regions  
4. Native support for JDBC and ODBC  
5. Customer-managed encryption keys using Tri-Secret Secure  
6. Support for Payment Card Industry Data Security Standards (PCI DSS)  

In order to provide all the listed services, what is the MINIMUM Snowflake edition that should be selected during account creation?

A) Standard Edition  
B) Enterprise Edition  
C) Business Critical Edition  
D) Premier Edition  

**Correct Answer:** C

---

Question 380. A retail company uses Snowflake to load sales transaction files daily. The architect discovers that after a file is loaded and its metadata has expired, the same file needs to be reloaded for a reconciliation process. Which method should be used to reload the file?

A) Change the file name to a new one and reload  
B) Copy the file to a different location or rename it, then execute the COPY command again  
C) Re-enable metadata tracking for the file  
D) Use the REFRESH keyword in the COPY command  

**Correct Answer:** B

---

Question 381. A data engineering team needs to reload a file into Snowflake, but the file’s load metadata has expired. What is a practical way to ensure the file can be loaded again?

A) Use the OVERWRITE option in the COPY command  
B) Rename or move the file in the stage so Snowflake treats it as a new file  
C) Run COPY INTO with FORCE=true  
D) Restore the metadata from backup  

**Correct Answer:** B

---

Question 382. During a data audit, an architect is asked to reload a previously loaded file, but the metadata that tracks the load has expired. What action should be taken to successfully reload the file into Snowflake?

A) Set the file retention policy to unlimited  
B) Rename the file or move it to a different location in the stage before reloading  
C) Use the RECOVER FILE command  
D) Request manual metadata reset from Snowflake support  

**Correct Answer:** B

---

Question 383. A manufacturing company’s Data Engineering team needs to provide raw data for the Data Science team and secure, curated data for the Sales team, while also supporting reporting and visualization for Finance and Vendor Management. Which data modeling approach best supports all these requirements?

A) Store all data in a single, flat table accessible to every team  
B) Implement a multi-layered architecture with raw, curated, and presentation layers  
C) Use only secure views for all users, regardless of their needs  
D) Build a star schema with only aggregated data  

**Correct Answer:** B

---

Question 384. The Sales team at a manufacturing company requires access to engineered and protected data for monetization, while the Data Science team needs raw data for model development. How should the Data Engineering team structure their Snowflake data models to support both use cases?

A) Only provide access to aggregated reporting tables  
B) Use a multi-zone architecture, separating raw, transformed, and shared data zones  
C) Grant full access to all tables for all teams  
D) Build one large fact table with all attributes exposed  

**Correct Answer:** B

---

Question 385. A Snowflake architect is tasked with supporting diverse analytics requirements, including detailed reporting, raw data exploration, and secure data sharing for monetization. What approach should be used to organize the data in Snowflake?

A) Create a flat wide table for all reporting and analytics  
B) Use a layered modeling approach, including staging, core, and presentation layers  
C) Only store data in the stage and allow users to query directly  
D) Partition data by team and restrict cross-team sharing  

**Correct Answer:** B

---

Question 386. A Snowflake Architect notices that the destination table has significantly more data than expected after setting up a COPY INTO command to continuously load data from an external stage. What is a likely reason for this?

A) The COPY INTO command is ignoring duplicate files  
B) The same files are being loaded multiple times because file load metadata is not being tracked correctly  
C) The data in the external stage is changing format frequently  
D) The COPY INTO command is only partially loading each file  

**Correct Answer:** B

---

Question 387. After configuring a continuous data load from an external stage with COPY INTO, an Architect realizes that the target table size is unexpectedly large. What could be causing this issue?

A) The COPY INTO command is skipping some files  
B) The external stage is storing compressed files  
C) The same files are being reloaded repeatedly due to expired or missing file load metadata  
D) The destination table has incorrect clustering  

**Correct Answer:** C

---

Question 388. A Snowflake Architect implements a COPY INTO command for continuous loading, but the table accumulates far more rows than anticipated. What is a possible root cause?

A) The COPY INTO command is filtering out too much data  
B) The external stage has files with duplicate names but different contents  
C) Files are repeatedly loaded because the metadata tracking which files have been loaded has expired or is not functioning properly  
D) The table is set to auto-scale  

**Correct Answer:** C

---

Question 389. A data engineering team is organizing files into logical paths in a Snowflake stage to improve data management and query efficiency. What additional parameter does Snowflake recommend adding to the COPY INTO command to optimize file selection?

A) OVERWRITE  
B) PATTERN  
C) FILE_FORMAT  
D) FORCE  

**Correct Answer:** B

---

Question 390. A Snowflake Architect is tasked with optimizing the ingestion of files that are organized into logical directories within a stage. Which parameter should be included in the COPY INTO command to target specific files based on naming conventions?

A) VALIDATE  
B) PATTERN  
C) ON_ERROR  
D) SIZE_LIMIT  

**Correct Answer:** B

---

Question 391. A retail company wants to efficiently load only sales data files from a large stage containing various types of data. When organizing files into logical paths, which parameter is most helpful for specifying which files to include during the load process?

A) HEADER  
B) PATTERN  
C) MAX_FILE_SIZE  
D) PARSE_JSON  

**Correct Answer:** B

---

Question 392. A Snowflake architect is designing a solution that uses external tables for data sourced from cloud storage. The architect needs to implement row-level security for different business units. Which statement correctly describes how row access policies can be applied to external tables?

A) Row access policies cannot be applied to external tables  
B) Row access policies can be applied directly to external tables to control access based on user attributes  
C) External tables require column masking policies instead of row access policies  
D) Row access policies are only available for views, not tables  

**Correct Answer:** B

---

Question 393. A Data Engineering team has created several external tables to support cross-departmental analytics. They want to restrict access to specific rows based on user roles. What must the architect know about applying row access policies to external tables?

A) Row access policies automatically apply to all referenced objects  
B) Row access policies can be applied directly to external tables, enabling granular access control  
C) Row access policies must be applied at the stage level, not table level  
D) External tables must be converted to regular tables before applying row access policies  

**Correct Answer:** B

---

Question 394. A financial institution is leveraging external tables in Snowflake to ingest large amounts of transactional data. How can row access policies be used to manage sensitive data exposure in these tables?

A) Row access policies are not compatible with external tables  
B) Row access policies can be applied to external tables, allowing dynamic filtering of rows based on user context  
C) Row access policies can only be used on internal Snowflake tables  
D) Row access policies require manual enforcement outside of Snowflake  

**Correct Answer:** B

---

Question 395. A Snowflake Architect wants to use external functions to invoke a third-party REST API for data enrichment. What is one limitation they should be aware of when designing this solution?

A) External functions can only call internal Snowflake services  
B) External functions have latency and timeout limits when interacting with remote endpoints  
C) External functions automatically retry failed requests indefinitely  
D) External functions support unlimited concurrent executions  

**Correct Answer:** B

---

Question 396. A Data Engineering team is considering using external functions for data processing. What is a constraint related to permissions and network access that they must keep in mind?

A) External functions can be executed without any network configuration  
B) External functions require an integration object that manages network connectivity and security  
C) External functions are only available for users with ACCOUNTADMIN privileges  
D) External functions are exempt from Snowflake’s role-based access control  

**Correct Answer:** B

---

Question 397. A financial services company plans to use external functions to call APIs hosted outside Snowflake. Which of the following is a technical limitation they must consider?

A) External functions can return any data type, including complex nested JSON  
B) External functions cannot return large result sets due to payload size restrictions  
C) External functions can only be triggered via manual user commands  
D) External functions can access on-premises resources directly without a secure tunnel  

**Correct Answer:** B

---

Question 398. Which of the following is a limitation of Snowflake external functions?

A) They can be used to write both functions and stored procedures  
B) They can only be used to write functions, not stored procedures  
C) They can directly access on-premises databases  
D) They can execute in parallel without any restrictions  

**Correct Answer:** B

---

Question 399. Person1 is currently using the SECURITYADMIN role in Snowflake. After creating a new role named DBA_ROLE to manage warehouses, what command should Person1 execute to switch the worksheet context to DBA_ROLE?

A) ALTER ROLE DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) SET ROLE DBA_ROLE;  
D) SWITCH ROLE DBA_ROLE;  

**Correct Answer:** B

---

Question 400. A Snowflake administrator creates a role called DBA_ROLE for warehouse management. What is the correct SQL command Person1 should use to activate DBA_ROLE for their current session?

A) GRANT ROLE DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) ACTIVATE ROLE DBA_ROLE;  
D) CHOOSE ROLE DBA_ROLE;  

**Correct Answer:** B

---

Question 401. After creating a new custom role in Snowflake, which command allows a user to change their worksheet context to the newly created role?

A) SELECT DBA_ROLE;  
B) USE ROLE DBA_ROLE;  
C) SWITCH TO DBA_ROLE;  
D) EXEC DBA_ROLE;  

**Correct Answer:** B

---

Question 402. Which command can we use to convert JSON NULL values to SQL NULL values?

A) TO_VARIANT  
B) NULLIF  
C) TRY_PARSE_JSON  
D) TO_JSON  

**Correct Answer:** B

---

Question 403. Which ALTER commands will impact a column's availability in Time Travel?

A) ALTER TABLE ... RENAME COLUMN  
B) ALTER TABLE ... DROP COLUMN  
C) ALTER TABLE ... MODIFY COLUMN COMMENT  
D) ALTER TABLE ... SET DEFAULT  

**Correct Answer:** B

---

Question 404. A streaming data engineering team has set up the Snowflake Kafka Connector and subscribed it to multiple Kafka topics. However, the topics have not yet been mapped to any Snowflake tables. What behavior should the team expect from the Kafka Connector?

A) The Kafka Connector will automatically create tables for each topic  
B) The Kafka Connector will not ingest data from the Kafka topics until they are mapped to Snowflake tables  
C) The Kafka Connector will send an error message to the Kafka cluster  
D) The Kafka Connector will load data into a default Snowflake table  

**Correct Answer:** B

---

Question 405. An organization’s Kafka Connector is subscribed to several Kafka topics, but those topics have not been mapped to destination tables in Snowflake. What will happen to the data streamed to those topics?

A) The data will be ingested into Snowflake using inferred schemas  
B) The data will not be loaded into Snowflake until the topics are mapped to tables  
C) The connector will store the data locally until mapping is provided  
D) The connector will merge all data into a single staging table  

**Correct Answer:** B

---

Question 406. A Snowflake Architect configures the Kafka Connector for real-time ingestion but omits mapping between Kafka topics and Snowflake tables. What is the connector’s response to incoming messages on these topics?

A) The connector creates temporary tables for unmapped topics  
B) The connector ignores incoming messages from topics that are not mapped to Snowflake tables  
C) The connector logs a warning and continues running  
D) The connector attempts to auto-map topics to tables using topic names  

**Correct Answer:** B

---

Question 407. Which system functions does Snowflake provide to monitor clustering information within a table? (Choose two.)

A) SYSTEM$CLUSTERING_DEPTH  
B) SYSTEM$CLUSTERING_INFORMATION  
C) SYSTEM$CLUSTERING_RATIO  
D) SYSTEM$CLUSTERING_METADATA  

**Correct Answers:** B, C

---

Question 408. After how many days does the load activity of the COPY INTO command and Snowpipe in the Information Schema expire?

A) 1 day  
B) 14 days  
C) 7 days  
D) 30 days  

**Correct Answer:** B

---

Question 409. What two requirements are necessary for the remote service to be called by the Snowflake external function? (Choose two.)

A) The remote service must be hosted on-premises  
B) The remote service must be accessible via a network integration  
C) The remote service must accept and respond to HTTPS requests  
D) The remote service must use the Snowflake-provided API gateway  

**Correct Answers:** B, C

---

Question 410. A Data Engineer is setting up Snowpipe for auto-ingest of event logs from Amazon S3. The Snowflake Architect wants to ensure the minimum required privileges are granted to the Snowpipe user. Which privileges must be assigned for the user to execute Snowpipe?

A) OWNERSHIP on the stage and table  
B) USAGE on the stage and INSERT on the target table  
C) SELECT on the target table and USAGE on the stage  
D) MONITOR on the Snowpipe and INSERT on the target table  

**Correct Answer:** B

---

Question 411. An organization is configuring Snowpipe for automated data ingestion from S3. What are the minimum object privileges the Snowpipe user must have to successfully execute Snowpipe?

A) CREATE PIPE on the database  
B) USAGE on the external stage and INSERT on the destination table  
C) USAGE on the database and SELECT on the table  
D) OWNERSHIP on the pipeline and MONITOR on the stage  

**Correct Answer:** B

---

Question 412. In a near real-time ingestion scenario using Snowpipe, a Data Engineer is configuring access control. Which combination of privileges is the minimum required to load data with Snowpipe?

A) USAGE on the schema and INSERT on the database  
B) USAGE on the stage and INSERT on the target table  
C) SELECT on the stage and OWNERSHIP on the table  
D) MONITOR on the Snowpipe and USAGE on the stage  

**Correct Answer:** B

---

Question 413. A global healthcare organization is migrating sensitive patient data to Snowflake. The company must comply with strong legal isolation requirements due to regional privacy laws, but also wants to support multiple tenants in a single Snowflake account for cost efficiency. Which tenancy strategy should the Snowflake Architect recommend if RBAC is an acceptable method for isolation?

A. Use a single schema for all tenants and control access with masking policies  
B. Create a separate schema and dedicated role for each tenant, using RBAC to restrict access  
C. Deploy separate Snowflake accounts for each tenant  
D. Store all tenant data in a single table and filter with row access policies  

**Answer:** B

---

Question 414. A financial services provider is onboarding multiple clients (tenants) to a Snowflake-powered analytics platform. Each client’s data must be strictly isolated for legal compliance, but the company wishes to minimize operational overhead. Which solution best balances strong legal isolation with efficient multi-tenancy, assuming RBAC is sufficient?

A. Use a separate Snowflake account for each client  
B. Use a single account with one schema for all clients and apply data masking  
C. Use a single account, with separate schemas and unique roles for each client, enforced by RBAC  
D. Use one centralized role with row access policies across all schemas  

**Answer:** C

---

Question 415. A SaaS provider is building a multi-tenant application on Snowflake. Legal regulations require that no tenant can access another tenant’s data. The architect wants to keep costs low and management simple, and RBAC is confirmed to meet regulatory needs. What is the best design pattern to satisfy these requirements?

A. Deploy a dedicated Snowflake account for each tenant  
B. Use a single Snowflake account with a dedicated schema and role for each tenant  
C. Store all tenant data in a single schema and use masking policies  
D. Use a single account with all tenant data in one table and control access via views  

**Answer:** B

---

Question 416. A retail company receives transaction files from stores every 10 seconds into an external stage on Snowflake. The files are between 500 K and 3 MB. Store managers need to see the most recent sales data immediately on their dashboards. What is the best solution with the least coding effort?

A. Develop a custom ETL pipeline in Python to load each file as it arrives  
B. Create an external table on the stage and allow dashboards to query it directly  
C. Use Snowpipe to automatically ingest the files into an internal table  
D. Schedule a batch job to load files every hour  

**Answer:** B, C

---

Question 417. A logistics company’s proprietary system drops shipment files every 10 seconds to an S3 bucket, which is configured as an external stage in Snowflake. Dashboards must reflect new shipments immediately. What two Snowflake features will provide the needed data accessibility with minimal development?

A. Manual bulk loading using COPY INTO statements  
B. Snowpipe for continuous, automated ingestion  
C. External tables for direct querying of staged data  
D. Stream and task for micro-batch processing every 5 minutes  

**Answer:** B, C

---

Question 418. A financial institution receives small data files in an external stage every 10 seconds from a proprietary system. Compliance dashboards require instant access to the newest data. As a Snowflake Architect, which approaches allow for rapid data availability with the least coding? Choose 2

A. Build a custom lambda function to ingest files  
B. Set up an external table on the stage for direct queries  
C. Configure Snowpipe for near real-time ingestion  
D. Periodically aggregate the data with a scheduled task  

**Answer:** B, C

---

Question 419. A Snowflake Architect notices that a query takes 30 minutes to run, with 24 minutes spent on compilation. The business requires faster turnaround, but increasing the virtual warehouse size is not an option. What can the Architect do to improve query performance?

A. Increase the size of the virtual warehouse  
B. Refactor the query to reduce complexity and enable better pruning  
C. Break the query into smaller, modular subqueries or use intermediate tables  
D. Run the query more frequently to leverage result caching  

**Answer:** B

---

Question 420. A data analytics team is frustrated because their long-running queries spend most of the time in the compilation phase, not execution. The team cannot upgrade to a larger warehouse due to cost constraints. Which strategy should the Snowflake Architect recommend to reduce compilation times?

A. Add more compute clusters to the warehouse  
B. Simplify the query by reducing the number of joins and subqueries  
C. Increase the cache retention period  
D. Use clustering keys on the underlying tables  

**Answer:** B

---

Question 421. A retail company’s reporting query spends 24 of its 30 minutes runtime in compilation. The Architect is asked to reduce query time but must keep the warehouse size unchanged. What is the most effective action?

A. Use a larger virtual warehouse for the query  
B. Rewrite the query to simplify nested logic and break up large statements  
C. Add more partitions to the tables  
D. Schedule the query during off-peak hours  

**Answer:** B

---

Question 422. An e-commerce company uses Snowpipe for continuous ingestion from an external stage. After several days, the Data Architect needs to change the external stage definition to point to a new S3 bucket. What is the recommended approach?

A. Delete the pipe and create a new one with the updated stage definition  
B. Suspend the pipe, modify the external stage definition, and then resume the pipe  
C. Modify the external stage directly and notify downstream users of the change  
D. Drop the stage, recreate it with the new definition, and ensure the pipe references the updated stage  

**Answer:** B

---

Question 423. A healthcare analytics team relies on Snowpipe for real-time file ingestion from an external stage. The team needs to update the external stage to use a different cloud storage location. What steps should the Data Architect follow to ensure minimal disruption?

A. Leave the pipe running while modifying the stage  
B. Suspend the pipe, update the external stage definition, then resume the pipe  
C. Create a new pipe for the new stage and deprecate the old one  
D. Run a manual COPY INTO command after updating the stage  

**Answer:** B

---

Question 424. A financial services firm has configured Snowpipe for daily ingestion from a referenced external stage. The Data Architect needs to update the pipe definition due to changes in the external stage. What is the best practice to follow?

A. Remove all files from the stage before making changes  
B. Suspend the pipe, update the stage, resume the pipe, and validate ingestion  
C. Make changes to the stage while the pipe is active  
D. Recreate the pipe from scratch after stage changes  

**Answer:** B

---

Question 425. A financial services Architect needs to retain quarter-end financial results for the previous six years in Snowflake. Which feature should the Architect use to accomplish this requirement?

A. Configure Time Travel for all financial tables  
B. Create zero-copy clones of the tables at each quarter-end and retain them  
C. Rely on the Fail-safe feature to access historical data  
D. Set up automatic data retention policies for six years  

**Answer:** B

---

Question 426. A user needs to change object parameters in Snowflake. Which roles allow a user to make these changes? (Choose 2)

A. SYSADMIN  
B. PUBLIC  
C. ACCOUNTADMIN  
D. SECURITYADMIN  

**Answers:**  
A. SYSADMIN (Correct)  
C. ACCOUNTADMIN (Correct)

---

Question 427. A retail analytics team has created two views on their sales data in the same schema: one is a Secure View, and the other is a standard View. When running queries against both, they notice that the query profiler displays different information for each. What best explains this behavior?

A. The Secure View automatically filters sensitive data columns  
B. The Secure View does not expose underlying query details in the profiler for security reasons  
C. The standard View is using cached results  
D. There is a difference in the data contained in each view  

**Answer:** B

---

Question 428. A financial institution has two views on transaction data: one is a Secure View, and the other is a regular View. Upon analysis, the architect observes that the query profiler for the Secure View discloses less information about the underlying query execution compared to the standard View. Why does this occur?

A. Secure Views are always materialized, standard Views are not  
B. Secure Views mask query details in the profiler for enhanced security  
C. Standard Views require more permissions to access profiler details  
D. Secure Views display performance metrics in a separate dashboard  

**Answer:** B

---

Question 429. A healthcare architect notices that when users query both a Secure View and a standard View with identical data, the query profiler shows less detail for one of the views. What is the reason for this difference?

A. Secure Views are faster and skip profiler tracking  
B. Secure Views intentionally limit profiler information to protect logic and sensitive data  
C. Standard Views are limited by user roles  
D. Secure Views run queries in a different warehouse  

**Answer:** B

---

Question 430. A Snowflake Architect is reviewing the output of `SYSTEM$CLUSTERING_INFORMATION` for a large table and notices the metric `average_overlaps`. What does `average_overlaps` refer to?

A. The average number of clusters in the table  
B. The average number of times a database user has queried the table  
C. The average number of micro-partitions that contain overlapping values for the clustering key columns  
D. The average number of rows in each micro-partition  

**Answer:** C

---

Question 431. A Snowflake Architect is designing a large table and wants to optimize cluster key selection. Which steps are recommended best practices for prioritizing cluster keys in Snowflake? (Choose two.)

A. Select columns that are frequently used in filtering and join conditions  
B. Choose columns with high cardinality and minimal null values  
C. Pick columns that are rarely queried to minimize overhead  
D. Use columns with complex data types such as VARIANT or OBJECT  

**Answers:**  
B. Choose columns with high cardinality and minimal null values (Correct)  
A. Select columns that are frequently used in filtering and join conditions (Correct)

---

Question 432. A Snowflake Architect accidentally drops a data share. Which command can be used to restore the dropped share?

A. ALTER SHARE ... UNDROP  
B. RECOVER SHARE ...  
C. UNDROP SHARE ...  
D. RESTORE SHARE ...  

**Answer:** C

---

Question 433. A company’s daily Snowflake workload consists of a huge number of concurrent queries triggered between 9pm and 11pm. At the individual level, these queries are smaller statements that get completed within a short time period.

What configuration can the company’s Architect implement to enhance the performance of this workload? (Choose two.)

A. Increase the size of the virtual warehouse to the largest available  
B. Enable multi-cluster warehouses with auto-scaling to handle concurrency  
C. Use a multi-cluster warehouse and set the minimum cluster count greater than one  

---

Question 434. Before declaring the disaster recovery solution operational, what should the architect do to validate business continuity?

A) Promote the failover group in the secondary account and test data access and integrity  
B) Archive the database in cold storage  
C) Set up daily backups using external tables  
D) Increase the replication frequency to every minute  

**Correct Answer:** A

---

Question 435. An analytics team migrated a legacy reporting system to Snowflake. Their queries now execute directly against large fact tables rather than precomputed summary tables, and they want to speed up performance and reduce compute costs without changing the query SQL or the reporting tool connections. Which Snowflake feature is best suited to address this requirement?

A) Clustering keys  
B) Materialized Views  
C) Query Result Caching  
D) Data Masking  

**Answer: B) Materialized Views**

---

Question 436. The analytics team wants to reduce compute costs and improve query speed for their migrated reporting system in Snowflake, but they do not want to alter the queries or require users to connect to different tables. Which feature can transparently accelerate repeated queries with identical results without any changes on the client side?

A) Zero-Copy Cloning  
B) Query Result Caching  
C) External Tables  
D) Time Travel  

**Answer: B) Query Result Caching**

---

Question 437. After migrating to Snowflake, the analytics team notices that some queries against large fact tables could be further optimized. They are not allowed to modify the original SQL or change the reporting tool’s data sources. Which approach should the architect recommend to optimize query performance in this scenario?

A) Repartitioning the data  
B) Creating materialized views  
C) Increasing the warehouse size  
D) Optimizing clustering keys  

**Answer: D) Optimizing clustering keys**

---

Question 438. A global retailer wants to analyze daily sales reports stored as CSV files in their cloud storage (AWS S3). They need to query these files directly in Snowflake, enriching the data with the source file name and row number for auditing purposes. What is the most architecturally sound solution to enable this capability?

A) Load the files into an internal Snowflake table using the COPY INTO command and add custom columns for file name and row number during ETL.  
B) Create an external stage pointing to the S3 bucket, and then define an external table in Snowflake that includes metadata columns for filename and file row number.  
C) Use Snowpipe to continuously load the data into a permanent table and include file metadata in a separate mapping table.  
D) Manually parse files outside Snowflake, append metadata, and upload them to a Snowflake internal stage.

**Answer:** B) Create an external stage pointing to the S3 bucket, and then define an external table in Snowflake that includes metadata columns for filename and file row number.

---

Question 439. A financial services company needs to ensure that sensitive customer data stored in external files is not exposed to unauthorized users when queried via Snowflake external tables. As a Snowflake Architect, which approach should you recommend to satisfy both security and business requirements?

A) Grant SELECT privileges on the external table to all users so they can access the data as needed.  
B) Use row access policies in Snowflake to restrict data visibility based on user attributes, combined with secure external stages.  
C) Allow unrestricted access to the cloud storage bucket and enforce security only at the Snowflake role level.  
D) Copy the data into transient tables and delete them after use to limit exposure.

**Answer:** B) Use row access policies in Snowflake to restrict data visibility based on user attributes, combined with secure external stages.
